function id,signature with docstring,function body
1,"    def get_model_agent(self):
        """"""
        Return model agent.
        """"""",        return self.get_agents()[1]
2,"    def _warn_unsafe_extraction_path(path):
        """"""
        If the default extraction path is overridden and set to an insecure
        location, such as /tmp, it opens up an opportunity for an attacker to
        replace an extracted file with an unauthorized payload. Warn the user
        if a known insecure location is used.

        See Distribute #375 for more details.
        """"""","        if os.name == 'nt' and not path.startswith(os.environ['windir']):
            # On Windows, permissions are generally restrictive by default
            #  and temp directories are not writable by other users, so
            #  bypass the warning.
            return
        mode = os.stat(path).st_mode
        if mode & stat.S_IWOTH or mode & stat.S_IWGRP:
            msg = (""%s is writable by group/others and vulnerable to attack ""
                ""when ""
                ""used with get_resource_filename. Consider a more secure ""
                ""location (set with .set_extraction_path or the ""
                ""PYTHON_EGG_CACHE environment variable)."" % path)
            warnings.warn(msg, UserWarning)"
3,"    def _compute_dependencies(self):
        """"""Recompute this distribution's dependencies.""""""","        from _markerlib import compile as compile_marker
        dm = self.__dep_map = {None: []}

        reqs = []
        # Including any condition expressions
        for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:
            distvers, mark = self._preparse_requirement(req)
            parsed = next(parse_requirements(distvers))
            parsed.marker_fn = compile_marker(mark)
            reqs.append(parsed)

        def reqs_for_extra(extra):
            for req in reqs:
                if req.marker_fn(override={'extra':extra}):
                    yield req

        common = frozenset(reqs_for_extra(None))
        dm[None].extend(common)

        for extra in self._parsed_pkg_info.get_all('Provides-Extra') or []:
            extra = safe_extra(extra.strip())
            dm[extra] = list(frozenset(reqs_for_extra(extra)) - common)

        return dm"
4,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Adds command line arguments.
        """"""","        parser.add_argument(
            '--num-words', type=int, default=10, help='Number of unigrams to output.'
        )
        cls.dictionary_class().add_cmdline_args(parser, partial_opt=partial_opt)
        return parser"
5,"def trypull(repo, xs):
    """"""Pull the list of given names xs.

    Return true if pull succeeded for all names. Does not raise.
    """"""","    # Do not attempt to pull the same name twice, or names in the repo.
    repo._autopulled = getattr(repo, ""_autopulled"", set())
    xs = [x for x in xs if x not in repo._autopulled and x not in repo]
    if not xs:
        return False
    repo._autopulled.update(xs)

    # If paths.default is not set. Do not attempt to pull.
    if repo.ui.paths.get(""default"") is None:
        return False

    def sortkey(tup):
        name, func = tup
        return (func._priority, name)

    # Try autopull functions.
    funcs = [
        func
        for _name, func in sorted(_table.items(), key=lambda t: (t[1]._priority, t[0]))
    ]
    if not funcs:
        return

    # Collect all attempts.
    attempts = []
    for x in xs:
        for func in funcs:
            attempt = func(repo, x)
            if attempt:
                assert isinstance(attempt, pullattempt)
                attempts.append(attempt)

    # Merge all pullattempts and execute it.
    if attempts:
        attempt = attempts[0]
        for other in attempts[1:]:
            merged = attempt.trymerge(other)
            if merged:
                attempt = merged
            else:
                attempt.execute(repo)
                attempt = other
        attempt.execute(repo)
        unfi = repo
        return all(x in unfi for x in xs)

    return False"
6,"def export_model(opt: Opt):
    """"""
    Export a model to TorchScript so that inference can be run outside of ParlAI.
    """"""","
    if version.parse(torch.__version__) < version.parse(""1.7.0""):
        raise NotImplementedError(
            ""TorchScript export is only supported for Torch 1.7 and higher!""
        )
    else:
        # Only load TorchScriptGreedySearch now, because this will trigger scripting of
        # associated modules
        from parlai.torchscript.modules import TorchScriptGreedySearch

    overrides = {
        ""model_parallel"": False  # model_parallel is not currently supported when TorchScripting,
    }
    if opt.get(""script_module""):
        script_module_name, script_class_name = opt[""script_module""].split("":"", 1)
        script_module = importlib.import_module(script_module_name)
        script_class = getattr(script_module, script_class_name)
    else:
        script_class = TorchScriptGreedySearch
    if ""override"" not in opt:
        opt[""override""] = {}
    for k, v in overrides.items():
        opt[k] = v
        opt[""override""][k] = v

    # Create the unscripted greedy-search module
    agent = create_agent(opt, requireModelExists=True)
    original_module = script_class(agent)

    # Script the module and save
    instantiated = script_class(agent)
    if not opt[""no_cuda""]:
        instantiated = instantiated.cuda()
    if opt.get(""enable_inference_optimizations""):
        scripted_model = torch.jit.optimize_for_inference(
            torch.jit.script(instantiated.eval())
        )
    else:
        scripted_model = torch.jit.script(instantiated)

    with PathManager.open(opt[""scripted_model_file""], ""wb"") as f:
        torch.jit.save(scripted_model, f)

    # Compare the original module to the scripted module against the test inputs
    if len(opt[""input""]) > 0:
        inputs = opt[""input""].split(""|"")
        print(""\nGenerating given the original unscripted module:"")
        _run_conversation(module=original_module, inputs=inputs)
        print(""\nGenerating given the scripted module:"")
        _run_conversation(module=scripted_model, inputs=inputs)"
7,"    def _enqueue_request(self):
        """"""
        Queue a request for loading to the data loader.
        """"""","        if self.threading:
            self.data_loader.request_load(self.receive_data, self.get_chunk, ())
        else:
            self._process_data(self.get_chunk())"
8,"def debugstatus(ui, repo, **opts):
    """"""common performance issues for status""""""","    if ""treestate"" not in repo.requirements:
        raise error.Abort(""debugstatus only supports treestate currently"")
    if ""eden"" in repo.requirements:
        raise error.Abort(""debugstatus is not supported in edenfs virtual checkouts"")

    dirstate = repo.dirstate
    dmap = dirstate._map
    ui.write(_x(""len(dirstate) = %d\n"") % len(dmap))

    nonnormalset = dmap.nonnormalset
    ui.write(_x(""len(nonnormal) = %d\n"") % len(nonnormalset))

    visitdir = dirstate._ignore.visitdir

    def dirfilter(path):
        return visitdir(path) == ""all""

    nonnormalfiltered = dmap.nonnormalsetfiltered(dirfilter)
    ui.write(_x(""len(filtered nonnormal) = %d\n"") % len(nonnormalfiltered))

    toprint = int(opts.get(""nonnormal"", 0))
    if toprint:
        for path in sorted(nonnormalfiltered)[:toprint]:
            ui.write(_x(""  %s\n"") % path)

    ui.write(_x(""clock = %s\n"") % dirstate.getclock())"
9,"    def commit(
        self,
        text="""",
        user=None,
        date=None,
        match=None,
        force=False,
        editor=False,
        extra=None,
        loginfo=None,
        mutinfo=None,
    ):
        """"""Add a new revision to current repository.

        Revision information is gathered from the working directory,
        match can be used to filter the committed files. If editor is
        supplied, it is called to get a commit message.
        """"""","        if extra is None:
            extra = {}

        if loginfo is None:
            loginfo = {}

        def fail(f, msg):
            raise errormod.Abort(""%s: %s"" % (f, msg))

        if not match:
            match = matchmod.always(self.root, """")

        if not force:
            match.bad = fail

        wlock = lock = tr = None
        try:
            wlock = self.wlock()
            lock = self.lock()  # for recent changelog (see issue4368)

            wctx = self[None]
            merge = len(wctx.parents()) > 1

            if not force and merge and not match.always():
                raise errormod.Abort(
                    _(
                        ""cannot partially commit a merge ""
                        ""(do not specify files or patterns)""
                    )
                )

            status = self.status(match=match, clean=force)

            # make sure all explicit patterns are matched
            if not force:
                self.checkcommitpatterns(wctx, match, status, fail)

            loginfo.update({""checkoutidentifier"": self.dirstate.checkoutidentifier})

            cctx = context.workingcommitctx(
                self, status, text, user, date, extra, loginfo, mutinfo
            )

            # internal config: ui.allowemptycommit
            allowemptycommit = (
                wctx.branch() != wctx.p1().branch()
                or extra.get(""close"")
                or merge
                or cctx.files()
                or self.ui.configbool(""ui"", ""allowemptycommit"")
            )
            if not allowemptycommit:
                return None

            if merge and cctx.deleted():
                raise errormod.Abort(_(""cannot commit merge with missing files""))

            ms = mergemod.mergestate.read(self)
            mergeutil.checkunresolved(ms)

            if editor:
                cctx._text = editor(self, cctx)
            edited = text != cctx._text

            # Save commit message in case this transaction gets rolled back
            # (e.g. by a pretxncommit hook).  Leave the content alone on
            # the assumption that the user will use the same editor again.
            msgfn = self.savecommitmessage(cctx._text)

            p1, p2 = self.dirstate.parents()
            hookp1, hookp2 = hex(p1), (p2 != nullid and hex(p2) or """")
            try:
                self.hook(""precommit"", throw=True, parent1=hookp1, parent2=hookp2)
                tr = self.transaction(""commit"")
                ret = self.commitctx(cctx, True)
            except:  # re-raises
                if edited:
                    self.ui.write(_(""note: commit message saved in %s\n"") % msgfn)
                raise
            # update bookmarks, dirstate and mergestate
            bookmarks.update(self, [p1, p2], ret)
            cctx.markcommitted(ret)
            ms.reset()
            tr.close()

        finally:
            lockmod.release(tr, lock, wlock)

        def commithook(node=hex(ret), parent1=hookp1, parent2=hookp2):
            # hack for command that use a temporary commit (eg: histedit)
            # temporary commit got stripped before hook release
            if self.changelog.hasnode(ret):
                self.hook(""commit"", node=node, parent1=parent1, parent2=parent2)

        self._afterlock(commithook)
        return ret"
10,"def mask_rcnn_fcn_head_v1up4convs(model, blob_in, dim_in, spatial_scale):
    """"""v1up design: 4 * (conv 3x3), convT 2x2.""""""","    return mask_rcnn_fcn_head_v1upXconvs(
        model, blob_in, dim_in, spatial_scale, 4
    )"
11,"def _hashignore(ignore):
    """"""Calculate hash for ignore patterns and filenames

    If this information changes between Mercurial invocations, we can't
    rely on Watchman information anymore and have to re-scan the working
    copy.

    """"""","    sha1 = hashlib.sha1()
    sha1.update(repr(ignore))
    return sha1.hexdigest()"
12,"def getprocstarttime(pid):
    """"""Get the windows timestamp of process start time

    Windows Timestamp in this context is a number of 100-nanosecond
    time units since January 1, 1601 at Greenwich, England.

    See https://msdn.microsoft.com/en-us/library/ms683223(VS.85).aspx""""""","    ph = _kernel32.OpenProcess(_PROCESS_QUERY_LIMITED_INFORMATION, 1, pid)
    if not ph or ph == _INVALID_HANDLE_VALUE:
        raise ctypes.WinError(_kernel32.GetLastError())
    try:
        creationtime = _FILETIME()
        exittime = _FILETIME()
        kerneltime = _FILETIME()
        usertime = _FILETIME()
        success = _kernel32.GetProcessTimes(
            ph,
            ctypes.byref(creationtime),
            ctypes.byref(exittime),
            ctypes.byref(kerneltime),
            ctypes.byref(usertime),
        )
        if not success:
            raise ctypes.WinError(_kernel32.GetLastError())
        ct = (creationtime.dwHighDateTime << 32) + creationtime.dwLowDateTime
        return ct
    finally:
        _kernel32.CloseHandle(ph)"
13,"def _reserved():
    """"""characters that are problematic for filesystems

    * ascii escapes (0..31)
    * ascii hi (126..255)
    * windows specials

    these characters will be escaped by encodefunctions
    """"""","    winreserved = [ord(x) for x in '\\:*?""<>|']
    for x in range(32):
        yield x
    for x in range(126, 256):
        yield x
    for x in winreserved:
        yield x"
14,"def is_new_task_filename(filename):
    """"""
    Check if a given filename counts as a new task.

    Used in tests and test triggers, and only here to avoid redundancy.
    """"""","    return (
        'parlai/tasks' in filename
        and 'README' not in filename
        and 'task_list.py' not in filename
    )"
15,"    def _extract_embedding_outputs(
        self, hooks: Dict[str, Dict[str, OutputRecorder]]
    ) -> Dict[str, torch.Tensor]:
        """"""
        Extract out the encoder and decoder embedding outputs.
        """"""","        assert len(hooks['embeddings'].outputs) == 2
        return {
            'encoder': hooks['embeddings'].outputs[0],
            'decoder': hooks['embeddings'].outputs[1],
        }"
16,"def is_this_circleci():
    """"""
    Return if we are currently running in CircleCI.
    """"""",    return bool(os.environ.get('CIRCLECI'))
17,"    def _encoder_input(
        self, batch: Batch
    ) -> Tuple[
        torch.LongTensor,
        torch.LongTensor,
        torch.LongTensor,
        torch.LongTensor,
        Optional[torch.LongTensor],
    ]:
        """"""
        During generation, we send the label truncation to the model.
        """"""","        return (
            batch.text_vec,
            batch.text_vec.ne(self.NULL_IDX).sum(1),
            batch.query_vec,
            batch.input_turn_cnt_vec,
            batch.text_vec.new(batch.batchsize).fill_(self.label_truncate),
        )"
18,"    def _startTest(self, offset, line):
        """"""Internal call to change state machine. Override startTest().""""""","        self._state.startTest(offset, line)"
19,"    def handle_fl(self):
        """"""Handle a flags line.""""""","        if self.current_state in [""MC"", ""MS"", ""MX""]:
            self.instance.append(self.current_entry)
            self.current_entry = POEntry()
        self.current_entry.flags += self.current_token[3:].split("", "")
        return True"
20,"    def addFailure(self, test, error=None, details=None):
        """"""Report a failure in test test.

        Only one of error and details should be provided: conceptually there
        are two separate methods:
            addFailure(self, test, error)
            addFailure(self, test, details)

        :param error: Standard unittest positional argument form - an
            exc_info tuple.
        :param details: New Testing-in-python drafted API; a dict from string
            to subunit.Content objects.
        """"""","        self._addOutcome(""failure"", test, error=error, details=details)"
21,"    def writable(self) -> bool:
        """"""
        Return a bool indicating whether object was opened for writing.

        If False, write() and truncate() will raise OSError.
        """"""",        return False
22,"    def load_entry_point(self, group, name):
        """"""Return the `name` entry point of `group` or raise ImportError""""""","        ep = self.get_entry_info(group, name)
        if ep is None:
            raise ImportError(""Entry point %r not found"" % ((group, name),))
        return ep.load()"
23,"def build_bootstrap_dataset(dataset_name: str, cfg: CfgNode) -> Sequence[torch.Tensor]:
    """"""
    Build dataset that provides data to bootstrap on

    Args:
        dataset_name (str): Name of the dataset, needs to have associated metadata
            to load the data
        cfg (CfgNode): bootstrapping config
    Returns:
        Sequence[Tensor] - dataset that provides image batches, Tensors of size
            [N, C, H, W] of type float32
    """"""","    logger = logging.getLogger(__name__)
    _add_category_info_to_bootstrapping_metadata(dataset_name, cfg)
    meta = MetadataCatalog.get(dataset_name)
    factory = BootstrapDatasetFactoryCatalog.get(meta.dataset_type)
    dataset = None
    if factory is not None:
        dataset = factory(meta, cfg)
    if dataset is None:
        logger.warning(f""Failed to create dataset {dataset_name} of type {meta.dataset_type}"")
    return dataset"
24,"    def get_dialogue_history(self, obs):
        """"""
        Get dialogue history for an observation.

        :param obs:
            observation

        :return:
            the observation with the dialogue history in the `text` field
        """"""","        if len(self.history) > 0:
            obs.force_set(""text"", ""\n"".join(self.history) + ""\n"" + obs[""text""])
        if ""labels"" in obs:
            self.history.append(random.choice(obs[""labels""]))
        elif ""eval_labels"" in obs:
            self.history.append(random.choice(obs[""eval_labels""]))
        if obs.get(""episode_done"", True):
            # end of this episode, clear the history
            self.history.clear()

        return obs"
25,"    def _send_file_arg(self, filename):
        """"""
        Sends the contents of a file to the server.
        """"""","        with open(filename) as f:
            while True:
                num_bytes = f.readinto(self.buf)
                if not num_bytes:
                    break
                self._send_chunk(self.buf.raw[:num_bytes], CHUNKTYPE_LONGARG)"
26,"    def parley(self):
        """"""
        Parley in all subworlds.

        Usually with ref:`batch_act` and ref:`batch_observe`.
        """"""","        # Collect batch together for each agent, and do update.
        # Assumes DialogPartnerWorld, MultiAgentWorld, or MultiWorlds of them.
        num_agents = len(self.world.get_agents())
        batch_observations = self.batch_observations

        if hasattr(self.world, 'parley_init'):
            for w in self.worlds:
                w.parley_init()

        for agent_idx in range(num_agents):
            # The agent acts.
            batch_act = self.batch_act(agent_idx, batch_observations[agent_idx])
            self.acts[agent_idx] = batch_act
            # We possibly execute this action in the world.
            if hasattr(self.world, 'execute'):
                for w in self.worlds:
                    w.execute(w.agents[agent_idx], batch_act[agent_idx])
            # All agents (might) observe the results.
            for other_index in range(num_agents):
                obs = self.batch_observe(other_index, batch_act, agent_idx)
                if obs is not None:
                    batch_observations[other_index] = obs
        self.update_counters()"
27,"def _exchangesetup():
    """"""Make changes to exchange and bundle2""""""","
    @exchange.b2partsgenerator(commonheadsparttype)
    @perftrace.tracefunc(""commonheads"")
    def commonheadspartgen(pushop, bundler):
        if rebaseparttype not in bundle2.bundle2caps(pushop.remote):
            # Server doesn't support pushrebase, so just fallback to normal push.
            return

        if pushop.ui.configbool(""experimental"", ""infinitepush-scratchpush""):
            # We are doing an infinitepush: it's not a pushrebase.
            return

        bundler.newpart(commonheadsparttype, data=b"""".join(pushop.outgoing.commonheads))

    @bundle2.parthandler(commonheadsparttype)
    def commonheadshandler(op, inpart):
        nodeid = inpart.read(20)
        while len(nodeid) == 20:
            op.records.add(commonheadsparttype, nodeid)
            nodeid = inpart.read(20)
        assert not nodeid  # data should split evenly into blocks of 20 bytes

    def checkremotenames():
        try:
            extensions.find(""remotenames"")
            return True
        except KeyError:
            return False

    @exchange.b2partsgenerator(rebasepackparttype)
    @perftrace.tracefunc(""rebasepackpart"")
    def packpartgen(pushop, bundler):
        # We generate this part manually during pushrebase pushes, so this is a
        # no-op. But it's required because bundle2 expects there to be a generator
        # for every handler.
        pass

    @exchange.b2partsgenerator(rebaseparttype)
    @perftrace.tracefunc(""rebasepart"")
    def rebasepartgen(pushop, bundler):
        onto = pushop.ui.config(experimental, configonto)
        if ""changesets"" in pushop.stepsdone or not onto:
            return

        if (
            rebaseparttype not in bundle2.bundle2caps(pushop.remote)
            and checkremotenames()
        ):
            # Server doesn't support pushrebase, but --to is valid in remotenames as
            # well, so just let it through.
            return

        pushop.stepsdone.add(""changesets"")
        pushop.stepsdone.add(""treepack"")
        if not pushop.outgoing.missing:
            # It's important that this text match the text found in upstream
            # Mercurial, since some tools rely on this string to know if a push
            # succeeded despite not pushing commits.
            pushop.ui.status(_(""no changes found\n""))
            pushop.cgresult = 0
            return

        # Force push means no rebasing, so let's just take the existing parent.
        if pushop.force:
            onto = donotrebasemarker

        rebaseparts = getrebaseparts(pushop.repo, pushop.remote, pushop.outgoing, onto)

        for part in rebaseparts:
            bundler.addpart(part)

        # Tell the server which manifests to load before taking the lock.
        # This helps shorten the duration of the lock, which increases our potential
        # commit rate.
        missing = pushop.outgoing.missing
        roots = pushop.repo.set(""parents(%ln) - %ln"", missing, missing)
        preloadnodes = [hex(r.manifestnode()) for r in roots]
        bundler.addparam(""preloadmanifests"", "","".join(preloadnodes))

        def handlereply(op):
            # server either succeeds or aborts; no code to read
            pushop.cgresult = 1

        return handlereply

    bundle2.capabilities[rebaseparttype] = ()

    @bundle2.parthandler(rebasepackparttype, (""version"", ""cache"", ""category""))
    def packparthandler(op, part):
        repo = op.repo

        versionstr = part.params.get(""version"")
        try:
            version = int(versionstr)
        except ValueError:
            version = 0

        if version < 1 or version > 2:
            raise error.Abort(
                _(""unknown rebasepack bundle2 part version: %s"") % versionstr
            )

        temppackpath = tempfile.mkdtemp()
        op.records.add(""tempdirs"", temppackpath)
        with mutablestores.mutabledatastore(repo, temppackpath) as dpack:
            with mutablestores.mutablehistorystore(repo, temppackpath) as hpack:
                wirepack.receivepack(repo.ui, part, dpack, hpack, version=version)
        op.records.add(""temp%spackdir"" % part.params.get(""category"", """"), temppackpath)
        # TODO: clean up

    @bundle2.parthandler(
        # ""newhead"" is not used, but exists for compatibility.
        rebaseparttype,
        (""onto"", ""newhead"", ""obsmarkerversions"", ""cgversion""),
    )
    def bundle2rebase(op, part):
        """"""unbundle a bundle2 containing a changegroup to rebase""""""

        params = part.params

        bundlefile = None
        bundle = None
        markerdate = util.makedate()
        ui = op.repo.ui

        # Patch ctx._fileinfo so it can look into treemanifests. This covers more
        # code paths (ex. fctx.renamed -> _copied -> ctx.filenode -> ctx._fileinfo
        # -> ""repo.manifestlog[self._changeset.manifest].find(path)"")
        def _fileinfo(orig, self, path):
            try:
                return orig(self, path)
            except LookupError:
                # Try look up again
                mf = _getmanifest(op, self)
                try:
                    return mf.find(path)
                except KeyError:
                    raise error.ManifestLookupError(
                        self._node, path, _(""not found in manifest"")
                    )

        with extensions.wrappedfunction(context.basectx, ""_fileinfo"", _fileinfo):
            ontoparam = params.get(""onto"", donotrebasemarker)
            try:  # guards bundlefile
                cgversion = params.get(""cgversion"", ""01"")
                bundlefile = _makebundlefile(op, part, cgversion)
                bundlepath = ""bundle:%s+%s"" % (op.repo.root, bundlefile)
                bundle = _createbundlerepo(op, bundlepath)

                ontoctx = resolveonto(op.repo, ontoparam)

                prepushrebasehooks(op, params, bundle, bundlefile)

                ui.setconfig(""pushrebase"", pushrebasemarker, True)
                verbose = ontoctx is not None and ui.configbool(""pushrebase"", ""verbose"")
                usestackpush = ontoctx is not None and ui.configbool(
                    ""pushrebase"", ""trystackpush"", True
                )

                def log(msg, force=False):
                    if verbose or force:
                        ui.write_err(msg)
                    ui.log(""pushrebase"", msg)

                if usestackpush:
                    try:
                        pushrequest = stackpush.pushrequest.fromrevset(
                            bundle, ""bundle()""
                        )
                    except StackPushUnsupportedError as ex:
                        # stackpush is unsupported. Fallback to old code path.
                        if verbose:
                            ui.write_err(_(""not using stackpush: %s\n"") % ex)

                        usestackpush = False
                if usestackpush:
                    # This can happen in the following (rare) case:
                    #
                    # Client:         Server:
                    #
                    #  C
                    #  |
                    #  B               B
                    #  |               |
                    #  A               A master
                    #
                    # Client runs ""push -r C --to master"". ""bundle()"" only contains
                    # ""C"". The non-stackpush code path would fast-forward master to
                    # ""C"". The stackpush code path will try rebasing ""C"" to ""A"".
                    # Prevent that. An alternative fix is to pass ""::bundle() % onto""
                    # to pushrequest.fromrevset. But that's more expensive and adds
                    # other complexities.
                    if (
                        ontoctx.node() != pushrequest.stackparentnode
                        and op.repo.changelog.isancestor(
                            ontoctx.node(), pushrequest.stackparentnode
                        )
                    ):
                        if verbose:
                            ui.write_err(
                                _(""not using stackpush: not rebasing backwards\n"")
                            )
                        usestackpush = False

                if usestackpush:
                    # stackpush code path - use ""pushrequest"" instead of ""bundlerepo""

                    # Check conflicts before entering the critical section. This is
                    # optional since there is another check inside the critical
                    # section.
                    log(_(""checking conflicts with %s\n"") % (ontoctx,))

                    pushrequest.check(ontoctx)

                    # Print and log what commits to push.
                    log(
                        getpushmessage(
                            pushrequest.pushcommits,
                            lambda c: ""%s  %s""
                            % (short(c.orignode), c.desc.split(""\n"", 1)[0][:50]),
                        ),
                        force=True,
                    )

                    # Enter the critical section! This triggers a hgsql sync.
                    tr = op.gettransaction()
                    hookargs = dict(tr.hookargs)
                    op.repo.hook(""prechangegroup"", throw=True, **hookargs)

                    # ontoctx could move. Fetch the new one.
                    # Print rebase source and destination.
                    ontoctx = resolveonto(op.repo, ontoparam)
                    log(
                        _(""rebasing stack from %s onto %s\n"")
                        % (short(pushrequest.stackparentnode), ontoctx)
                    )
                    added, replacements = pushrequest.pushonto(
                        ontoctx, getcommitdatefn=common.commitdategenerator(op)
                    )
                else:
                    # Old code path - use a bundlerepo

                    # Create a cache of rename sources while we don't have the lock.
                    renamesrccache = {
                        bundle[r].node(): _getrenamesrcs(op, bundle[r])
                        for r in bundle.revs(""bundle()"")
                    }

                    # Opening the transaction takes the lock, so do it after prepushrebase
                    # and after we've fetched all the cache information we'll need.
                    tr = op.gettransaction()
                    hookargs = dict(tr.hookargs)

                    # Recreate the bundle repo, since taking the lock in gettransaction()
                    # may have caused it to become out of date.
                    # (but grab a copy of the cache first)
                    bundle.close()
                    bundle = _createbundlerepo(op, bundlepath)

                    onto = getontotarget(op, params, bundle)

                    revs, oldonto = _getrevs(op, bundle, onto, renamesrccache)

                    op.repo.hook(""prechangegroup"", throw=True, **hookargs)

                    log(
                        getpushmessage(
                            revs,
                            lambda r: ""%s  %s""
                            % (r, bundle[r].description().split(""\n"", 1)[0][:50]),
                        ),
                        force=True,
                    )

                    # Prepopulate the revlog _cache with the original onto's fulltext. This
                    # means reading the new onto's manifest will likely have a much shorter
                    # delta chain to traverse.
                    log(_(""rebasing onto %s\n"") % (short(onto.node()),))

                    # Perform the rebase + commit to the main repo
                    added, replacements = runrebase(op, revs, oldonto, onto)

                    # revs is modified by runrebase to ensure garbage collection of
                    # manifests, so don't use it from here on.
                    revs = None

                op.repo.pushrebaseaddedchangesets = added
                op.repo.pushrebasereplacements = replacements

                markers = _buildobsolete(replacements, bundle, op.repo, markerdate)
            finally:
                try:
                    if bundlefile:
                        os.unlink(bundlefile)
                except OSError as e:
                    if e.errno != errno.ENOENT:
                        raise
                if bundle:
                    bundle.close()

        # Move public phase forward
        publishing = op.repo.ui.configbool(""phases"", ""publish"")
        if publishing:
            phasesmod.advanceboundary(op.repo, tr, phasesmod.public, [added[-1]])

        addfinalhooks(op, tr, hookargs, added)

        # Send new commits back to the client
        clientobsmarkerversions = [
            int(v) for v in params.get(""obsmarkerversions"", """").split(""\0"") if v
        ]
        _addpushbackparts(
            op, replacements, markers, markerdate, clientobsmarkerversions
        )

        for k in list(replacements.keys()):
            replacements[hex(k)] = hex(replacements[k])
        op.records.add(rebaseparttype, replacements)

        return 1"
28,"def outdebug(ui, message):
    """"""debug regarding output stream (bundling)""""""","    if ui.configbool(""devel"", ""bundle2.debug""):
        ui.debug(""bundle2-output: %s\n"" % message)"
29,"    def async_close(self, **kwargs: Any) -> bool:
        """"""
        `async_close()` must be called at the very end of any script that uses the
        asynchronous `opena` feature. This calls `async_join()` first and then closes
        the thread pool used for the asynchronous operations.

        Returns:
            status (bool): True on success
        """"""","        success = self.async_join(**kwargs)
        for handler in self._async_handlers:
            success = handler._async_close(**kwargs) and success
        self._async_handlers.clear()
        return success"
30,"def cosine_similarity(vec1, vec2) -> float:
    """"""
    Cosine similarity between two scipy sparse row matricies.
    """"""","    numerator = np.dot(vec1, vec2.transpose())[0, 0]
    denominator = np.linalg.norm(vec1.data) * np.linalg.norm(vec2.data)
    return numerator / max(denominator, 1e-8)"
31,"    def _set_text_vec(self, *args, **kwargs):
        """"""
        Add the start and end token to the text.
        """"""","        obs = super()._set_text_vec(*args, **kwargs)

        if 'text_vec' in obs and 'added_start_end' not in obs:
            obs.force_set(
                'text_vec', self._add_start_end_tokens(obs['text_vec'], True, True)
            )
            obs['added_start_end'] = True

        # check truncation after adding start end tokens
        if obs.get('text_vec') is not None:
            truncated_vec = self._check_truncate(
                obs['text_vec'], self.text_truncate, True
            )
            obs.force_set('text_vec', torch.LongTensor(truncated_vec))

        return obs"
32,"def _pushrevs(repo, ui, rev):
    """"""Given configuration and default rev, return the revs to be pushed""""""","    pushrev = ui.config(""remotenames"", ""pushrev"")
    if pushrev == ""!"":
        return []
    elif pushrev:
        return [repo[pushrev].rev()]
    if rev:
        return [repo[rev].rev()]
    return []"
33,"    def _parsefuncdecl(self, decl):
        """"""Parse function declaration and return the name of function in it""""""","        i = decl.find(""("")
        if i >= 0:
            return decl[:i]
        else:
            return decl"
34,"    def iscensored(self, rev):
        """"""Check if a file revision is censored.""""""",        return self.flags(rev) & revlog.REVIDX_ISCENSORED
35,"    def share(self):
        """"""
        Share the scripted module object.
        """"""","        shared = super().share()
        shared['module'] = self.module
        return shared"
36,"    def wlock(self, wait=True):
        """"""Lock the non-store parts of the repository (everything under
        .hg except .hg/store) and return a weak reference to the lock.

        Use this before modifying files in .hg.

        If both 'lock' and 'wlock' must be acquired, ensure you always acquires
        'wlock' first to avoid a dead-lock hazard.""""""","        l = self._wlockref and self._wlockref()
        if l is not None and l.held:
            l.lock()
            return l

        if self.ui.configbool(""devel"", ""debug-lockers""):
            util.debugstacktrace(""acquiring wlock for %s"" % self.root, skip=1, depth=5)

        # We do not need to check for non-waiting lock acquisition.  Such
        # acquisition would not cause dead-lock as they would just fail.
        if wait and (
            self.ui.configbool(""devel"", ""all-warnings"")
            or self.ui.configbool(""devel"", ""check-locks"")
        ):
            if self._currentlock(self._lockref) is not None:
                self.ui.develwarn('""wlock"" acquired after ""lock""')

        # if this is a shared repo and we must also lock the shared wlock
        # or else we can deadlock due to lock ordering issues
        sharedwlock = None
        if self.shared():
            sharedwlock = self._lock(
                self.sharedvfs,
                ""wlock"",
                wait,
                None,
                None,
                _(""shared repository %s"") % self.sharedroot,
            )

        def unlock():
            if sharedwlock:
                sharedwlock.release()
            if self.dirstate.pendingparentchange():
                self.dirstate.invalidate()
            else:
                self.dirstate.write(None)

            self._filecache[""dirstate""].refresh()

        l = self._lock(
            self.localvfs,
            ""wlock"",
            wait,
            unlock,
            self.invalidatedirstate,
            _(""working directory of %s"") % self.origroot,
        )
        self._wlockref = weakref.ref(l)
        return l"
37,"    def __init__(self, log_dir: str, window_size: int = 20, **kwargs):
        """"""
        Args:
            log_dir (str): the directory to save the output events
            window_size (int): the scalars will be median-smoothed by this window size

            kwargs: other arguments passed to `torch.utils.tensorboard.SummaryWriter(...)`
        """"""","        self._window_size = window_size
        from torch.utils.tensorboard import SummaryWriter

        self._writer = SummaryWriter(log_dir, **kwargs)
        self._last_write = -1"
38,"def CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error,
                              io=codecs):
  """"""Reports for missing stl includes.

  This function will output warnings to make sure you are including the headers
  necessary for the stl containers and functions that you use. We only give one
  reason to include a header. For example, if you use both equal_to<> and
  less<> in a .h file, only one (the latter in the file) of these will be
  reported as a reason to include the <functional>.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    include_state: An _IncludeState instance.
    error: The function to call with any errors found.
    io: The IO factory to use to read the header file. Provided for unittest
        injection.
  """"""","  required = {}  # A map of header name to linenumber and the template entity.
                 # Example of required: { '<functional>': (1219, 'less<>') }

  for linenum in xrange(clean_lines.NumLines()):
    line = clean_lines.elided[linenum]
    if not line or line[0] == '#':
      continue

    # String is special -- it is a non-templatized type in STL.
    matched = _RE_PATTERN_STRING.search(line)
    if matched:
      # Don't warn about strings in non-STL namespaces:
      # (We check only the first match per line; good enough.)
      prefix = line[:matched.start()]
      if prefix.endswith('std::') or not prefix.endswith('::'):
        required['<string>'] = (linenum, 'string')

    for pattern, template, header in _re_pattern_algorithm_header:
      if pattern.search(line):
        required[header] = (linenum, template)

    # The following function is just a speed up, no semantics are changed.
    if not '<' in line:  # Reduces the cpu time usage by skipping lines.
      continue

    for pattern, template, header in _re_pattern_templates:
      if pattern.search(line):
        required[header] = (linenum, template)

  # The policy is that if you #include something in foo.h you don't need to
  # include it again in foo.cc. Here, we will look at possible includes.
  # Let's copy the include_state so it is only messed up within this function.
  include_state = include_state.copy()

  # Did we find the header for this file (if any) and successfully load it?
  header_found = False

  # Use the absolute path so that matching works properly.
  abs_filename = FileInfo(filename).FullName()

  # For Emacs's flymake.
  # If cpplint is invoked from Emacs's flymake, a temporary file is generated
  # by flymake and that file name might end with '_flymake.cc'. In that case,
  # restore original file name here so that the corresponding header file can be
  # found.
  # e.g. If the file name is 'foo_flymake.cc', we should search for 'foo.h'
  # instead of 'foo_flymake.h'
  abs_filename = re.sub(r'_flymake\.cc$', '.cc', abs_filename)

  # include_state is modified during iteration, so we iterate over a copy of
  # the keys.
  header_keys = include_state.keys()
  for header in header_keys:
    (same_module, common_path) = FilesBelongToSameModule(abs_filename, header)
    fullpath = common_path + header
    if same_module and UpdateIncludeState(fullpath, include_state, io):
      header_found = True

  # If we can't find the header file for a .cc, assume it's because we don't
  # know where to look. In that case we'll give up as we're not sure they
  # didn't include it in the .h file.
  # TODO(unknown): Do a better job of finding .h files so we are confident that
  # not having the .h file means there isn't one.
  if filename.endswith('.cc') and not header_found:
    return

  # All the lines have been processed, report the errors found.
  for required_header_unstripped in required:
    template = required[required_header_unstripped][1]
    if required_header_unstripped.strip('<>""') not in include_state:
      error(filename, required[required_header_unstripped][0],
            'build/include_what_you_use', 4,
            'Add #include ' + required_header_unstripped + ' for ' + template)"
39,"    def check_encoding(self, f=sys.stdout):
        """"""Verify that this instruction can't be encoded more efficiently""""""",        return 0  # Return zero to indicate we can't save any bytes
40,"def generate_labeler_confusion_matrix(num_labelers: int, psi_mean: np.array,
                                      psi_std: np.array):
    '''
    Function to generate a seperate confusion matrix for each labeler
    Args:
        num_labelers: number of different confusion matrix
        psi_mean: mean value of true positive rate and true negative rate
        psi_std: standard deviation of true positive rate and true negative rate
    Return: a list of 2x2 confusion matrix with length equals to num_labelers
    '''","    num_classes = 2
    psi = np.zeros((num_labelers, num_classes, num_classes), dtype=float)
    for i in range(num_labelers):
        for j in range(0, num_classes):
            # Generate true positive/negative probability between 0.5 and 1.0
            psi[i][j][j] = min(
                max(np.random.normal(psi_mean[j], psi_std[j]), 0.5), 1.0)

            # Fill in false positive/negative value as needed
            for k in range(num_classes):
                if j != k:
                    psi[i][j][k] = 1.0 - psi[i][j][j]
    return psi"
41,"    def test_basic_all_sync(self):
        """"""
        Tests that the responses are being aggregated and the most awful
        (based on the awfulness map) is begin returned
        """"""","        mcr = self.get_mcrouter()

        # set key in three cluster
        self.mc1.set(""key"", ""value"")
        self.mc2.set(""key"", ""value"")
        self.mc3.set(""key"", ""value"")
        self.assertEqual(self.mc1.get(""key""), ""value"")
        self.assertEqual(self.mc2.get(""key""), ""value"")
        self.assertEqual(self.mc3.get(""key""), ""value"")
        self.assertEqual(mcr.get(""key""), ""value"")
        self.assertEqual(mcr.gat(0, ""key""), ""value"")
        self.assertTrue(mcr.gats(0, ""key""))

        # delete will return True on DELETED
        # will return False on NOT_FOUND

        # perform a delete and check the response
        # the aggregated response should be DELETED
        self.assertTrue(mcr.delete(""key""))

        # set key in only one cluster
        self.mc1.set(""key"", ""value"")
        self.assertEqual(self.mc1.get(""key""), ""value"")

        # the aggregated response should be NOT_FOUND
        self.assertFalse(mcr.delete(""key""))"
42,"    def create_one_drive_direct_download(self, one_drive_url: str) -> str:
        """"""
        Converts a short OneDrive URI into a download link that can be used with wget

        Args:
            one_drive_url (str): A OneDrive URI supported by this PathHandler

        Returns:
            result_url (str): A direct download URI for the file
        """"""","        data_b64 = base64.b64encode(bytes(one_drive_url, ""utf-8""))
        data_b64_string = (
            data_b64.decode(""utf-8"").replace(""/"", ""_"").replace(""+"", ""-"").rstrip(""="")
        )
        result_url = (
            f""https://api.onedrive.com/v1.0/shares/u!{data_b64_string}/root/content""
        )
        return result_url"
43,"def vis_mask(img, mask, col, alpha=0.4, show_border=True, border_thick=1):
    """"""Visualizes a single binary mask.""""""","
    img = img.astype(np.float32)
    idx = np.nonzero(mask)

    img[idx[0], idx[1], :] *= 1.0 - alpha
    img[idx[0], idx[1], :] += alpha * col

    if show_border:
        contours = cv2.findContours(
            mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)[-2]
        cv2.drawContours(img, contours, -1, _WHITE, border_thick, cv2.LINE_AA)

    return img.astype(np.uint8)"
44,"    def fast_generation(
        cls,
        doc_indices: List[int],
        n_best_beam_preds_scores: List[
            List[Tuple[torch.LongTensor, torch.Tensor, Optional[Dict]]]
        ],
        doc_log_probs: torch.Tensor,
        n_docs: int,
    ):
        """"""
        Apply RAG Sequence fast decoding, for a single batch item.

        :param doc_indices:
            indices into beam preds scores
        :param n_best_beam_preds_scores:
            bsz-length list of Tuples of predictions and scores
        :param doc_log_probs:
            probabilities for each document
        :param n_docs:
            number of docs per example

        :return sorted_hyps:
            return list of (hyp, score, token metadata) tuples, sorted by their score.
        """"""","        marginalized_hypos: Dict[
            str, Tuple[torch.LongTensor, torch.Tensor, Optional[Dict]]
        ] = {}
        for doc_idx in doc_indices:
            doc_hypos = n_best_beam_preds_scores[doc_idx]
            doc_score = doc_log_probs[doc_idx % n_docs]
            for hypo, hypo_score, token_metadata in doc_hypos:
                score = hypo_score + doc_score
                hypo_tokens = str(hypo.tolist())
                if hypo_tokens in marginalized_hypos:
                    marginalised_hypo = marginalized_hypos[hypo_tokens]
                    marginalised_hypo = (
                        marginalised_hypo[0],
                        torch.log(marginalised_hypo[1].exp() + score.exp()),
                        marginalised_hypo[2],
                    )
                else:
                    marginalized_hypos[hypo_tokens] = (hypo, score, token_metadata)
        sorted_by_score = sorted(marginalized_hypos.values(), key=lambda h: -h[1])
        return sorted_by_score"
45,"    def _knowledge_piece(self):
        """"""
        Determines the pieces of knowledge (selected content) to retrieve.

        This may be the enitre document, selected sentences or document titles.
        """"""",
46,"    def evaluation(self):
        """"""
        returns a section with dataset info about the eval tasks if they exist,
        information about the validation metric if it exists, and create a table with
        the validation metric.
        """"""","        # adding info about the eval tasks
        if self.eval_tasks == self.train_tasks:
            msg = ""For evalution, we used the same training datasets; check the [Datasets Used](#datasets-used) section for more information""
            eval_list = ''
        else:
            msg = f""This model was evaluated on the datasets below (use the `parlai display_data` commands to show data). Visit the {make_link('task (dataset) list', task_site)} for more details about the datasets.\n""
            eval_list = get_dataset_info(self.eval_tasks)
            eval_list = '\n' + '\n'.join(eval_list)
        content = [msg + eval_list]

        # validation metric info: getting metric name and description
        splitted = re.sub(r'_+', ' ', self.valid_metric).split()
        key = splitted[-1]
        if extra_metric_info.get(key):
            mname, description = extra_metric_info[key]
        elif METRICS_DISPLAY_DATA.get(key):
            mname = METRICS_DISPLAY_DATA[key].title
            description = METRICS_DISPLAY_DATA[key].description
        else:
            description, mname = (None, None)

        # adding description for validation metric and re-wording it:
        msg = f""\n\nWe used the metric {metric_format(self.valid_metric)}""
        if len(splitted) == 3 and splitted[0] == 'class' and mname:
            msg += f"", the {mname.lower()} scores for the class {splitted[1]}""
        content.append(msg + ' as the validation metric. ')
        if description:
            description = description[0].lower() + description[1:]
            content[-1] += f""Recall that `{self.valid_metric}` is {description}.""

        # evaluation table
        # getting list of subtasks and making columns
        eval_tasks = self.eval_tasks
        if len(self.eval_tasks) > 1:
            eval_tasks.insert(0, 'All')
        columns = [' '] + [taskname(subtask) for subtask in eval_tasks]
        # only one row: validation
        row = [metric_format(self.valid_metric)]
        for subtask in eval_tasks:
            # creating the key to get metric and formatting
            pre = '' if subtask == 'All' or len(eval_tasks) == 1 else subtask + '/'
            key = pre + self.valid_metric
            fmt = '{:.4f}' if self.valid_metric in not_percent else '{:.2%}'
            row.append(fmt.format(self.eval_results[key]))
        return '\n'.join(content) + '\n\n' + '\n'.join(make_md_table([row], columns))"
47,"    def share(self):
        """"""
        Basic sharing function.
        """"""",        return {'dict': self.dict}
48,"def squared_euclidean_distance_matrix(pts1: torch.Tensor, pts2: torch.Tensor) -> torch.Tensor:
    """"""
    Get squared Euclidean Distance Matrix
    Computes pairwise squared Euclidean distances between points

    Args:
        pts1: Tensor [M x D], M is the number of points, D is feature dimensionality
        pts2: Tensor [N x D], N is the number of points, D is feature dimensionality

    Return:
        Tensor [M, N]: matrix of squared Euclidean distances; at index (m, n)
            it contains || pts1[m] - pts2[n] ||^2
    """"""","    edm = torch.mm(-2 * pts1, pts2.t())
    edm += (pts1 * pts1).sum(1, keepdim=True) + (pts2 * pts2).sum(1, keepdim=True).t()
    return edm.contiguous()"
49,"    def send_fb_message(
        self, receiver_id, message, is_response, quick_replies=None, persona_id=None
    ):
        """"""
        Sends a message directly to messenger.
        """"""","        api_address = f'https://graph.facebook.com/{API_VERSION}/me/messages'
        if quick_replies is not None:
            quick_replies = [create_reply_option(x, x) for x in quick_replies]
        ms = create_text_message(message, quick_replies)
        results = []
        for m in ms:
            if m['text'] == '':
                continue  # Skip blank messages
            payload = {
                ""messaging_type"": 'RESPONSE' if is_response else 'UPDATE',
                ""recipient"": {""id"": receiver_id},
                ""message"": m,
            }
            if persona_id is not None:
                payload['persona_id'] = persona_id
            response = requests.post(api_address, params=self.auth_args, json=payload)
            result = response.json()
            if 'error' in result:
                if result['error']['code'] == 1200:
                    # temporary error please retry
                    response = requests.post(
                        api_address, params=self.auth_args, json=payload
                    )
                    result = response.json()
            log_utils.print_and_log(
                logging.INFO, '""Facebook response from message send: {}""'.format(result)
            )
            results.append(result)
        return results"
50,"    def advance(self):
        """"""Advance the current subtask.""""""",        self._tasks[-1][0] += 1
51,"    def _parsedirstate(content):
        """"""Parse given dirstate metadata file""""""","        f = util.stringio(content)
        p1 = f.read(20) or node.nullid
        p2 = f.read(20) or node.nullid
        header = f.read(len(HEADER))
        if header and header != HEADER:
            raise error.Abort(_(""working directory state appears damaged!""))
        # simple key-value serialization
        metadata = _unpackmetadata(f.read())
        if metadata:
            try:
                # main append-only tree state filename and root offset
                filename = metadata[""filename""]
                rootid = int(metadata[""rootid""])
                # whether to write a new file or not during ""write""
                threshold = int(metadata.get(""threshold"", 0))
            except (KeyError, ValueError):
                raise error.Abort(_(""working directory state appears damaged!""))
        else:
            filename = None
            rootid = 0
            threshold = 0

        return p1, p2, filename, rootid, threshold"
52,"    def _mkdirs(self, path: str, **kwargs: Any) -> None:
        """"""
        Recursive directory creation function. Like mkdir(), but makes all
        intermediate-level directories needed to contain the leaf directory.
        Similar to the native `os.makedirs`.
        Args:
            path (str): A URI supported by this PathHandler
        """"""","        self._check_kwargs(kwargs)

        assert path.endswith(""/""), path

        bucket, s3_path = self._parse_uri(path)
        client = self._get_client(bucket)

        try:
            client.put_object(Bucket=bucket, Key=s3_path)
        except botocore.exceptions.ClientError as e:
            raise OSError(
                f""Error in mkdirs path {path} - "" f""{type(e).__name__}: {e}""
            ) from e"
53,"    def _create_agent(self, task_id, agent_id):
        """"""
        Initialize an agent and return it.

        Called each time an agent is placed into a new task.

        :param task_id:
            string task identifier
        :param agent_id:
            int agent id
        """"""","        return MessengerAgent(
            self.opt, self, task_id, agent_id, self.service_reference_id
        )"
54,"    def git(
        self, *args: str, encoding: str = ""utf-8"", env: Optional[Dict[str, str]] = None
    ) -> str:
        """"""
        Invoke a git command inside the repository.

        All non-keyword arguments are treated as arguments to git.

        A keyword argument of ""env"" can be used to specify a dictionary of
        additional environment variables to be passed to git.  (These will be
        added to the current environment.)

        ""env"" is currently the only valid keyword argument.

        Example usage:

          repo.git('commit', '-m', 'my new commit',
                   env={'GIT_AUTHOR_NAME': 'John Doe'})
        """"""","        cmd = [self.git_bin] + list(args)

        git_env = None
        if env is not None:
            git_env = os.environ.copy()
            git_env.update(env)

        try:
            completed_process = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                cwd=self.path,
                env=git_env,
            )
        except subprocess.CalledProcessError as ex:
            raise GitError(ex) from ex
        # pyre-fixme[22]: The cast is redundant.
        return typing.cast(str, completed_process.stdout.decode(encoding))"
55,"    def get_file_name(self, port):
        """""" We generate a file name for the port """"""","      
        port_file_name = ""%s_%s_%d"" %(self.file_prefix, self.system_manager.cur_user, port )
        return os.path.join(self.working_dir, port_file_name)"
56,"def get_fiber(fid):
    """"""return a fiber for the given id, name, or address, or None""""""","    # check if it's a named fiber
    info = FiberInfo.by_name(fid)
    if info:
        return info.fiber

    # next check if it's an assigned id
    if ""."" in fid:
        try:
            # Just grab the first result and return
            return next(get_fiber_info([fid]))[1].fiber
        except IndexError:
            return None

    # finally assume it's an address to a fiber
    try:
        return (
            gdb.parse_and_eval(fid)
            .cast(gdb.lookup_type(""folly::fibers::Fiber"").pointer())
            .dereference()
        )
    except gdb.error:
        return None"
57,"    def score_memories(self, query_enc: torch.Tensor) -> List[torch.Tensor]:
        """"""
        Score memories.
        """"""","        scores = []
        for i in range(query_enc.size(0)):
            scores.append(
                (
                    query_enc[i : i + 1]
                    @ self.memory_enc_dict[self.active_memory_slots[i]].t()
                ).squeeze(0)
            )
        return scores"
58,"    def finalize_message(self, msg: Dict[str, Any]) -> Message:
        """"""
        Remove the knowledge component of the message.
        """"""","        msg['text'] = msg[NO_KNOWLEDGE_FIELD]
        return Message(msg)"
59,"def collapse(repo, first, commitopts, skipprompt=False):
    """"""collapse the set of revisions from first to the working context one as new one.

    Expected commit options are:
        - message
        - date
        - username
    Commit message is edited in all cases.

    This function works in memory.""""""","    last = repo[None]
    ctxs = list(repo.set(""%n::."", first.node())) + [last]
    if not ctxs:
        return None
    for c in ctxs:
        if not c.mutable():
            raise error.ParseError(
                _(""cannot fold into public change %s"") % node.short(c.node())
            )
    base = first.p1()

    # commit a new version of the old changeset, including the update
    # collect all files which might be affected
    files = set()
    for ctx in ctxs:
        files.update(ctx.files())

    # Recompute copies (avoid recording a -> b -> a)
    copied = copies.pathcopies(base, last)

    # prune files which were reverted by the updates
    files = [f for f in files if not cmdutil.samefile(f, last, base)]
    # commit version of these files as defined by head
    headmf = last.manifest()

    def filectxfn(repo, ctx, path):
        if path in headmf:
            fctx = last[path]
            return context.overlayfilectx(fctx, ctx=ctx, copied=copied.get(path, False))
        return None

    if commitopts.get(""message""):
        message = commitopts[""message""]
    else:
        message = first.description()
    user = commitopts.get(""user"")
    date = commitopts.get(""date"")
    extra = commitopts.get(""extra"")
    mutinfo = commitopts.get(""mutinfo"")

    parents = (first.p1(), first.p2())
    editor = None
    if not skipprompt:
        editor = cmdutil.getcommiteditor(edit=True, editform=""histedit.fold"")

    loginfo = {""predecessors"": "" "".join(c.hex() for c in ctxs), ""mutation"": ""histedit""}

    new = context.memctx(
        repo,
        parents=parents,
        text=message,
        files=files,
        filectxfn=filectxfn,
        user=user,
        date=date,
        extra=extra,
        editor=editor,
        loginfo=loginfo,
        mutinfo=mutinfo,
    )
    return repo.commitctx(new)"
60,"def debuglabelcomplete(ui, repo, *args) -> None:
    """"""backwards compatibility with old bash completion scripts (DEPRECATED)""""""","    debugnamecomplete(ui, repo, *args)"
61,"def locate(ui, repo, *pats, **opts):
    """"""locate files matching specific patterns (DEPRECATED)

    Print files under @Product@ control in the working directory whose
    names match the given patterns.

    By default, this command searches all directories in the working
    directory. To search just the current directory and its
    subdirectories, use ""--include ."".

    If no patterns are given to match, this command prints the names
    of all files under @Product@ control in the working directory.

    If you want to feed the output of this command into the ""xargs""
    command, use the -0 option to both this command and ""xargs"". This
    will avoid the problem of ""xargs"" treating single filenames that
    contain whitespace as multiple filenames.

    See :prog:`help files` for a more versatile command.

    Returns 0 if a match is found, 1 otherwise.
    """"""","    if opts.get(""print0""):
        end = ""\0""
    else:
        end = ""\n""
    rev = scmutil.revsingle(repo, opts.get(""rev""), None).node()

    ret = 1
    ctx = repo[rev]
    m = scmutil.match(ctx, pats, opts, default=""relglob"", badfn=lambda x, y: False)

    ui.pager(""locate"")
    for abs in ctx.matches(m):
        if opts.get(""fullpath""):
            ui.write(repo.wjoin(abs), end)
        else:
            ui.write(((pats and m.rel(abs)) or abs), end)
        ret = 0

    return ret"
62,"    def setenv(self) -> None:
        """"""Clear and update os.environ

        Note that not all variables can make an effect on the running process.
        """"""","        l = self._readlist()
        try:
            newenv = dict(  # ignore below bc pyre doesn't like list to kv conversion
                s.split(""="", 1) for s in l if ""="" in s
            )
        except ValueError:
            raise ValueError(""unexpected value in setenv request"")
        _log(""setenv: %r\n"" % sorted(newenv.keys()))
        encoding.environ.clear()
        encoding.environ.update(newenv)
        # Apply $TZ changes.
        hgtime.tzset()"
63,"def _sort_proposals(proposals, id_field):
    """"""Sort proposals by the specified id field.""""""","    order = np.argsort(proposals[id_field])
    fields_to_sort = ['boxes', id_field, 'scores']
    for k in fields_to_sort:
        proposals[k] = [proposals[k][i] for i in order]"
64,"def FindNextMultiLineCommentStart(lines, lineix):
  """"""Find the beginning marker for a multiline comment.""""""","  while lineix < len(lines):
    if lines[lineix].strip().startswith('/*'):
      # Only return this marker if the comment goes beyond this line
      if lines[lineix].strip().find('*/', 2) < 0:
        return lineix
    lineix += 1
  return len(lines)"
65,"def showrevslist(name, revs, **args) -> _hybrid:
    """"""helper to generate a list of revisions in which a mapped template will
    be evaluated""""""","    args = args
    repo = args[""ctx""].repo()
    return _hybrid(
        None,
        revs,
        lambda x: {name: x, ""ctx"": repo[x], ""revcache"": {}},
        pycompat.identity,
        keytype=int,
        fastlen=revs.fastlen(),
    )"
66,"    def score(self, batch):
        """"""
        Score the batch.
        """"""","        segment_idx = (batch.text_vec * 0).long()
        if self.sep_last_utt:
            batch_len = batch.text_vec.size(1)
            # find where [SEP] token is
            seps = (batch.text_vec == self.dict.end_idx).nonzero()
            if len(seps) > 0:
                for row in seps:
                    # set last utterance to segment 1
                    segment_idx[row[0], list(range(row[1], batch_len))] = 1
            else:
                # only one utterance: everything after [CLS] token
                # should be segment 1
                segment_idx = (batch.text_vec != self.dict.start_idx).long()
        mask = (batch.text_vec != self.NULL_IDX).long()
        token_idx = batch.text_vec * mask
        return self.model(token_idx, segment_idx, mask)"
67,"def debugcreatescratchbookmark(ui, repo, *args, **opts):
    """"""create scratch bookmark on the specified revision""""""","    if not common.isserver(ui):
        raise error.Abort(
            _(""scratch bookmarks can only be created on an infinitepush server"")
        )

    scratchbookmarkname = _resolvescratchbookmark(ui, opts.get(""bookmark""))
    index = repo.bundlestore.index
    with index:
        if index.getnode(scratchbookmarkname):
            raise error.Abort(
                _(""scratch bookmark '%s' already exists"") % scratchbookmarkname
            )

        targetnode = _resolvetargetnode(repo, opts.get(""rev""))
        index.addbookmark(scratchbookmarkname, targetnode, False)"
68,"    def value_summary(self):
        """"""
        :return: XCUIElement value summary
        :rtype: str | None
        """"""","        if len(self.value_value) == 0:
            return None
        return ""value: '{}'"".format(self.value_value)"
69,"def hyperlink(context, mapping, args):
    """"""Render hyperlink in terminal""""""","    if len(args) != 2:
        raise error.ParseError(""hyperlink expects two arguments, got %s"" % len(args))

    title = evalstring(context, mapping, args[1])
    if not title:
        return """"

    ui = mapping[""ui""]
    hyperlink = ui.formatted and ui.configbool(""ui"", ""hyperlink"", False)

    if hyperlink:
        return ""\x1b]8;;{link}\a{title}\x1b]8;;\a"".format(
            link=evalstring(context, mapping, args[0]), title=title
        )
    return title"
70,"    def _setup_args(self):
        """"""
        gets the extra arguments.
        """"""","        user_args = self.opt['extra_args_path']
        if user_args is None:
            user_args = os.path.join(self.opt['folder_to_save'], 'args.json')

        try:
            # now setting up args.json
            with open(user_args, 'rb') as f:
                self.all_args = json.load(f)
        except Exception:
            self.all_args = {}"
71,"    def explore_expr(expr, value, is_child):
        """"""Main function to explore an expression value.

        Arguments:
            expr: The expression string that is being explored.
            value: The gdb.Value value of the expression.
            is_child: Boolean value to indicate if the expression is a child.
                      An expression is a child if it is derived from the main
                      expression entered by the user.  For example, if the user
                      entered an expression which evaluates to a struct, then
                      when exploring the fields of the struct, is_child is set
                      to True internally.

        Returns:
            No return value.
        """"""","        type_code = value.type.code
        if type_code in Explorer.type_code_to_explorer_map:
            explorer_class = Explorer.type_code_to_explorer_map[type_code]
            while explorer_class.explore_expr(expr, value, is_child):
                pass
        else:
            print (""Explorer for type '%s' not yet available.\n"" %
                   str(value.type))"
72,"    def _update_eden_state(self, base_dir: Path, uid: int, gid: int) -> None:
        """"""Update Eden's stored state for the snapshot so it will work in a new
        location.

        - Replace absolute path names in various data files to refer to the new
          location.  This is needed so that a snapshot originally created in one
          location can be unpacked and used in another location.

        - Update UID and GID values stored by Eden's to reflect the specified values.
          This is needed so that unpacked snapshots can be used by the current user
          without getting permissions errors when they try to access files inside the
          Eden checkouts.
        """"""","        info = self._read_metadata()
        old_base_dir = Path(info[""base_dir""])

        # A few files in the RocksDB directory end up with the absolute path
        # embedded in them.
        rocks_db_path = self.eden_state_dir / ""storage"" / ""rocks-db""
        for entry in rocks_db_path.iterdir():
            if entry.name.startswith(""LOG"") or entry.name.startswith(""OPTIONS""):
                self._replace_file_contents(entry, bytes(old_base_dir), bytes(base_dir))

        # Parse eden's config.json to get the list of checkouts, and update each one.
        eden_config_path = self.eden_state_dir / ""config.json""
        with eden_config_path.open(""r+"") as config_file:
            eden_data = json.load(config_file)
            new_config_data = {}
            for _old_checkout_path, checkout_name in eden_data.items():
                new_checkout_path = self.data_dir / checkout_name
                new_config_data[str(new_checkout_path)] = checkout_name
                checkout_state_dir = self.eden_state_dir / ""clients"" / checkout_name
                self._relocate_checkout(checkout_state_dir, old_base_dir, base_dir)
                self._update_ownership(checkout_state_dir, uid, gid)

            config_file.seek(0)
            config_file.truncate()
            json.dump(new_config_data, config_file, indent=2, sort_keys=True)

        # Update the info file with the new base path
        info[""base_dir""] = str(base_dir)
        self._write_metadata(info)"
73,"def register_finder(importer_type, distribution_finder):
    """"""Register `distribution_finder` to find distributions in sys.path items

    `importer_type` is the type or class of a PEP 302 ""Importer"" (sys.path item
    handler), and `distribution_finder` is a callable that, passed a path
    item and the importer instance, yields ``Distribution`` instances found on
    that path item.  See ``pkg_resources.find_on_path`` for an example.""""""",    _distribution_finders[importer_type] = distribution_finder
74,"def CheckInvalidIncrement(filename, clean_lines, linenum, error):
  """"""Checks for invalid increment *count++.

  For example following function:
  void increment_counter(int* count) {
    *count++;
  }
  is invalid, because it effectively does count++, moving pointer, and should
  be replaced with ++*count, (*count)++ or *count += 1.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    error: The function to call with any errors found.
  """"""","  line = clean_lines.elided[linenum]
  if _RE_PATTERN_INVALID_INCREMENT.match(line):
    error(filename, linenum, 'runtime/invalid_increment', 5,
          'Changing pointer instead of value (or unused value of operator*).')"
75,"def branch(ui, repo, label=None, **opts):
    """"""(deprecated. use '@prog@ bookmark' instead)

    This command does nothing meaningful and will be removed in the future.

    For now, it always prints ""default"" or raise an exception if NAME or -C is
    provided.
    """"""","    if not util.istest():
        ui.deprecate(""hg-branch"", ""branches are deprecated at Facebook"")
    hintutil.trigger(""branch-command-deprecate"")
    if not opts.get(""clean"") and not label:
        ui.write(""%s\n"" % repo.dirstate.branch())
        return

    raise error.Abort(
        _(""named branches are disabled in this repository""),
        hint=ui.config(""ui"", ""disallowedbrancheshint"", _(""use bookmarks instead"")),
    )"
76,"def shared_random_seed():
    """"""
    Returns:
        int: a random number that is the same across all workers.
        If workers need a shared RNG, they can use this shared seed to
        create one.

    All workers must call this function, otherwise it will deadlock.
    """"""","    ints = np.random.randint(2**31)
    all_ints = all_gather(ints)
    return all_ints[0]"
77,"    def getinstance() -> ""TerminalSettings"":
        """"""Get the TerminalSettings singleton object for this programs TTY.

        This function calls curses.setupterm() to initialize the terminal the first time
        it is called.  Subsequent calls return the previously looked up terminal
        information.
        """"""","        global _term_settings
        if _term_settings is not None:
            return _term_settings

        import curses

        curses.setupterm()

        set_foreground = curses.tigetstr(""setaf"") or b""""
        foreground = {
            Color.RED: curses.tparm(set_foreground, curses.COLOR_RED),
            Color.GREEN: curses.tparm(set_foreground, curses.COLOR_GREEN),
            Color.YELLOW: curses.tparm(set_foreground, curses.COLOR_YELLOW),
        }

        set_background = curses.tigetstr(""setab"") or b""""
        background = {
            Color.RED: curses.tparm(set_background, curses.COLOR_RED),
            Color.GREEN: curses.tparm(set_background, curses.COLOR_GREEN),
            Color.YELLOW: curses.tparm(set_background, curses.COLOR_YELLOW),
        }

        attributes = {
            Attribute.BOLD: curses.tigetstr(""bold"") or b"""",
            Attribute.UNDERLINE: curses.tigetstr(""smul"") or b"""",
        }

        reset = curses.tigetstr(""sgr0"") or b""""

        _term_settings = TerminalSettings(
            foreground=foreground,
            background=background,
            attributes=attributes,
            reset=reset,
        )
        return _term_settings"
78,"def _pushb2ctx(pushop, bundler):
    """"""handle changegroup push through bundle2

    addchangegroup result is stored in the ``pushop.cgresult`` attribute.
    """"""","    if ""changesets"" in pushop.stepsdone:
        return
    pushop.stepsdone.add(""changesets"")
    # Send known heads to the server for race detection.
    if not _pushcheckoutgoing(pushop):
        return

    b2caps = bundle2.bundle2caps(pushop.remote)
    version = ""01""
    cgversions = b2caps.get(""changegroup"")
    if cgversions:  # 3.1 and 3.2 ship with an empty value
        cgversions = [
            v
            for v in cgversions
            if v in changegroup.supportedoutgoingversions(pushop.repo)
        ]
        if not cgversions:
            raise ValueError(_(""no common changegroup version""))
        version = max(cgversions)
    cgstream = changegroup.makestream(
        pushop.repo, pushop.outgoing, version, ""push"", b2caps=b2caps
    )
    cgpart = bundler.newpart(""changegroup"", data=cgstream)
    if cgversions:
        cgpart.addparam(""version"", version)
    if ""treemanifest"" in pushop.repo.requirements:
        cgpart.addparam(""treemanifest"", ""1"")

    def handlereply(op):
        """"""extract addchangegroup returns from server reply""""""
        cgreplies = op.records.getreplies(cgpart.id)
        assert len(cgreplies[""changegroup""]) == 1
        pushop.cgresult = cgreplies[""changegroup""][0][""return""]

    return handlereply"
79,"    def __init__(self, h, w, angle, expand=True, center=None, interp=None):
        """"""
        Args:
            h, w (int): original image size
            angle (float): degrees for rotation
            expand (bool): choose if the image should be resized to fit the whole
                rotated image (default), or simply cropped
            center (tuple (width, height)): coordinates of the rotation center
                if left to None, the center will be fit to the center of each image
                center has no effect if expand=True because it only affects shifting
            interp: cv2 interpolation method, default cv2.INTER_LINEAR
        """"""","        super().__init__()
        image_center = np.array((w / 2, h / 2))
        if center is None:
            center = image_center
        if interp is None:
            interp = cv2.INTER_LINEAR
        abs_cos, abs_sin = (abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle))))
        if expand:
            # find the new width and height bounds
            bound_w, bound_h = np.rint(
                [h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]
            ).astype(int)
        else:
            bound_w, bound_h = w, h

        self._set_attributes(locals())
        self.rm_coords = self.create_rotation_matrix()
        # Needed because of this problem https://github.com/opencv/opencv/issues/11784
        self.rm_image = self.create_rotation_matrix(offset=-0.5)"
80,"def debugrevspec(ui, repo, expr, **opts) -> Optional[int]:
    """"""parse and apply a revision specification

    Use -p/--show-stage option to print the parsed tree at the given stages.
    Use -p all to print tree at every stage.

    Use --no-show-revs option with -s or -p to print only the set
    representation or the parsed tree respectively.

    Use --verify-optimized to compare the optimized result with the unoptimized
    one. Returns 1 if the optimized result differs.
    """"""","    aliases = ui.configitems(""revsetalias"")
    stages = [
        (""parsed"", lambda tree: tree),
        (""expanded"", lambda tree: revsetlang.expandaliases(tree, aliases, ui.warn)),
        (""concatenated"", revsetlang.foldconcat),
        (""analyzed"", revsetlang.analyze),
        (""optimized"", revsetlang.optimize),
    ]
    if opts[""no_optimized""]:
        stages = stages[:-1]
    if opts[""verify_optimized""] and opts[""no_optimized""]:
        raise error.Abort(_(""cannot use --verify-optimized with "" ""--no-optimized""))
    stagenames = set(n for n, f in stages)

    showalways = set()
    showchanged = set()
    if ui.verbose and not opts[""show_stage""]:
        # show parsed tree by --verbose (deprecated)
        showalways.add(""parsed"")
        showchanged.update([""expanded"", ""concatenated""])
        if opts[""optimize""]:
            showalways.add(""optimized"")
    if opts[""show_stage""] and opts[""optimize""]:
        raise error.Abort(_(""cannot use --optimize with --show-stage""))
    if opts[""show_stage""] == [""all""]:
        showalways.update(stagenames)
    else:
        for n in opts[""show_stage""]:
            if n not in stagenames:
                raise error.Abort(_(""invalid stage name: %s"") % n)
        showalways.update(opts[""show_stage""])

    treebystage = {}
    printedtree = None
    tree = revsetlang.parse(expr, lookup=repo.__contains__)
    for n, f in stages:
        treebystage[n] = tree = f(tree)
        if n in showalways or (n in showchanged and tree != printedtree):
            if opts[""show_stage""] or n != ""parsed"":
                ui.write(_x(""* %s:\n"") % n)
            ui.write(revsetlang.prettyformat(tree), ""\n"")
            printedtree = tree

    if opts[""verify_optimized""]:
        arevs = revset.makematcher(treebystage[""analyzed""])(repo)
        brevs = revset.makematcher(treebystage[""optimized""])(repo)
        if opts[""show_set""] or (opts[""show_set""] is None and ui.verbose):
            ui.write(_x(""* analyzed set:\n""), smartset.prettyformat(arevs), ""\n"")
            ui.write(_x(""* optimized set:\n""), smartset.prettyformat(brevs), ""\n"")
        arevs = list(arevs)
        brevs = list(brevs)
        if arevs == brevs:
            return 0
        ui.write(_x(""--- analyzed\n""), label=""diff.file_a"")
        ui.write(_x(""+++ optimized\n""), label=""diff.file_b"")
        sm = difflib.SequenceMatcher(None, arevs, brevs)
        for tag, alo, ahi, blo, bhi in sm.get_opcodes():
            if tag in (""delete"", ""replace""):
                for c in arevs[alo:ahi]:
                    ui.write(""-%s\n"" % c, label=""diff.deleted"")
            if tag in (""insert"", ""replace""):
                for c in brevs[blo:bhi]:
                    ui.write(""+%s\n"" % c, label=""diff.inserted"")
            if tag == ""equal"":
                for c in arevs[alo:ahi]:
                    ui.write("" %s\n"" % c)
        return 1

    func = revset.makematcher(tree)
    revs = func(repo)
    if opts[""show_set""] or (opts[""show_set""] is None and ui.verbose):
        ui.write(_x(""* set:\n""), smartset.prettyformat(revs), ""\n"")
    if not opts[""show_revs""]:
        return
    for c in revs:
        ui.write(""%s\n"" % c)"
81,"def _forcebundle1(op):
    """"""return true if a pull/push must use bundle1

    This function is used to allow testing of the older bundle version""""""","    ui = op.repo.ui
    forcebundle1 = False
    # The goal is this config is to allow developer to choose the bundle
    # version used during exchanged. This is especially handy during test.
    # Value is a list of bundle version to be picked from, highest version
    # should be used.
    #
    # developer config: devel.legacy.exchange
    exchange = ui.configlist(""devel"", ""legacy.exchange"")
    forcebundle1 = ""bundle2"" not in exchange and ""bundle1"" in exchange
    if forcebundle1:
        op.repo.ui.setconfig(""format"", ""allowbundle1"", True)
    return forcebundle1 or not op.remote.capable(""bundle2"")"
82,"def _SetCountingStyle(level):
  """"""Sets the module's counting options.""""""",  _cpplint_state.SetCountingStyle(level)
83,"    def _get_gt_keypoints(self, obj):
        """"""Return ground truth keypoints.""""""","        if 'keypoints' not in obj:
            return None
        kp = np.array(obj['keypoints'])
        x = kp[0::3]  # 0-indexed x coordinates
        y = kp[1::3]  # 0-indexed y coordinates
        # 0: not labeled; 1: labeled, not inside mask;
        # 2: labeled and inside mask
        v = kp[2::3]
        num_keypoints = len(obj['keypoints']) / 3
        assert num_keypoints == self.num_keypoints
        gt_kps = np.ones((3, self.num_keypoints), dtype=np.int32)
        for i in range(self.num_keypoints):
            gt_kps[0, i] = x[i]
            gt_kps[1, i] = y[i]
            gt_kps[2, i] = v[i]
        return gt_kps"
84,"    def _produce_mask_and_results(
        self, instance: Instances, bbox_xywh: IntTupleBox
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """"""
        Method to get labels and DensePose results from an instance

        Args:
            instance (Instances): an instance of `DensePoseEmbeddingPredictorOutput`
            bbox_xywh (IntTupleBox): the corresponding bounding box

        Return:
            mask (torch.Tensor): shape [H, W], DensePose segmentation mask
            embeddings (Tuple[torch.Tensor]): a tensor of shape [D, H, W],
                DensePose CSE Embeddings
            other_values (Tuple[torch.Tensor]): a tensor of shape [0, H, W],
                for potential other values
        """"""","        densepose_output = instance.pred_densepose
        S = densepose_output.coarse_segm
        E = densepose_output.embedding
        _, _, w, h = bbox_xywh
        embeddings = F.interpolate(E, size=(h, w), mode=""bilinear"")[0]
        coarse_segm_resized = F.interpolate(S, size=(h, w), mode=""bilinear"")[0]
        mask = coarse_segm_resized.argmax(0) > 0
        other_values = torch.empty((0, h, w), device=E.device)
        return mask, embeddings, other_values"
85,"def correlator(ui):
    """"""
    The correlator is a random string that is logged on both the client and
    server.  This can be used to correlate the client logging to the server
    logging.
    """"""",    return ui.correlator()
86,"    def reset_metrics(self):
        """"""
        Reset metrics for all agents.
        """"""","        for a in self.agents:
            a.reset_metrics()"
87,"    def report(self):
        """"""
        Return metrics showing total examples and accuracy if available.
        """"""",        return self.metrics.report()
88,"def _handle_ns(packageName, path_item):
    """"""Ensure that named package includes a subpath of path_item (if needed)""""""","
    importer = get_importer(path_item)
    if importer is None:
        return None
    loader = importer.find_module(packageName)
    if loader is None:
        return None
    module = sys.modules.get(packageName)
    if module is None:
        module = sys.modules[packageName] = types.ModuleType(packageName)
        module.__path__ = []
        _set_parent_ns(packageName)
    elif not hasattr(module,'__path__'):
        raise TypeError(""Not a package:"", packageName)
    handler = _find_adapter(_namespace_handlers, importer)
    subpath = handler(importer, path_item, packageName, module)
    if subpath is not None:
        path = module.__path__
        path.append(subpath)
        loader.load_module(packageName)
        for path_item in path:
            if path_item not in module.__path__:
                module.__path__.append(path_item)
    return subpath"
89,"def clean_text(text: str) -> str:
    """"""
    Removes all special tokens from an incoming text.
    """"""","    for token in CONST.ALL_SPECIAL_TOKENS:
        text = text.replace(f"" {token}"", '')
        text = text.replace(f""{token} "", '')
        text = text.replace(token, '')

    return text"
90,"    def as_uri(self):
        """"""Return the path as a 'file' URI.""""""","        if not self.is_absolute():
            raise ValueError(""relative path can't be expressed as a file URI"")
        return self._flavour.make_uri(self)"
91,"def maybe_fsdp_wrap(opt):
    """"""
    Context manager for enabling wrapping in FullyShardedDataParallel.
    """"""","    if not should_use_fsdp(opt):
        # make a no-op
        yield
        return

    # zero3 not supported at this time. Throw an exception
    if opt['ddp_backend'] == 'zero3':
        raise NotImplementedError(
            '--ddp-backend zero3 is not supported at this time. For details, see '
            'https://github.com/facebookresearch/ParlAI/issues/3753.'
        )

    reshard_after_forward = opt['ddp_backend'] == 'zero3'
    compute_dtype = torch.float16 if opt['fp16'] else torch.float32
    mixed_precision = opt['fp16'] and opt['fp16_impl'] == 'safe'
    fsdp_args = dict(
        reshard_after_forward=reshard_after_forward,
        mixed_precision=mixed_precision,
        compute_dtype=compute_dtype,
        state_dict_device=torch.device('cpu'),
        flatten_parameters=True,
        process_group=get_dist_group(),
    )
    with fairscale_enable_wrap(wrapper_cls=FSDP, **fsdp_args):
        yield"
92,"def _unshelverestorecommit(ui, repo, basename):
    """"""Recreate commit in the repository during the unshelve""""""","    with ui.configoverride({(""ui"", ""quiet""): True}):
        md = shelvedfile(repo, basename, ""oshelve"").readobsshelveinfo()
        shelvenode = nodemod.bin(md[""node""])
        try:
            shelvectx = repo[shelvenode]
        except error.RepoLookupError:
            m = _(
                ""shelved node %s not found in repo\nIf you think this shelve ""
                ""should exist, try running '@prog@ import --no-commit .hg/shelved/%s.patch' ""
                ""from the root of the repository.""
            )
            raise error.Abort(m % (md[""node""], basename))
    return repo, shelvectx"
93,"    def parse(self, next):
        """""" Take a string of arguments and parse them into args and kwargs """"""","        args = []
        kwargs = {}
        while next:
            getnext = None
            keymatch = self._keywordRE.search(next)
            if keymatch:
                name, getnext, next = keymatch.groups()
                kwargs[name] = True
                if not getnext:  # So keywords don't get lost
                    continue

            argmatch = self._argRE.search(next)
            if argmatch:
                quotetype, quoted, naked, next = argmatch.groups()
                #Could be a empty string
                arg = quoted if quoted is not None else naked
                #Get rid of any escaped strings
                arg = re.sub(r""""""\\(['""])"""""", r'\1', arg)
                if getnext:
                    kwargs[name] = arg
                else:
                    args.append(arg)
        return [args, kwargs]"
94,"def _prompt_version(attr):
    ""The version of GDB.""",    return gdb.VERSION
95,"    def omit(self, path):
        """"""
        Omit iff matches any of the omit_patterns or the include patterns are
        not empty and none is matched
        """"""","        path = os.path.realpath(path)
        return any(fnmatch.fnmatch(path, p) for p in self.omit_patterns) or (
            self.include_patterns
            and not any(fnmatch.fnmatch(path, p) for p in self.include_patterns)
        )"
96,"    def getavailable(self):
        """"""Returns an iterator over fully decoded values.

        Once values are retrieved, they won't be available on the next call.
        """"""","
        l = list(self._decodedvalues)
        self._decodedvalues = []
        return l"
97,"def manifest(ui, repo, node=None, rev=None, **opts):
    """"""output the current or given revision of the project manifest

    Print a list of version controlled files for the given revision.
    If no revision is given, the first parent of the working directory
    is used, or the null revision if no revision is checked out.

    With -v, print file permissions, symlink and executable bits.
    With --debug, print file revision hashes.

    If option --all is specified, the list of all files from all revisions
    is printed. This includes deleted and renamed files.

    Returns 0 on success.
    """"""","    fm = ui.formatter(""manifest"", opts)

    if opts.get(""all""):
        if rev or node:
            raise error.Abort(_(""can't specify a revision with --all""))

        res = []
        prefix = ""data/""
        suffix = "".i""
        plen = len(prefix)
        slen = len(suffix)
        with repo.lock():
            for fn, b, size in repo.store.datafiles():
                if size != 0 and fn[-slen:] == suffix and fn[:plen] == prefix:
                    res.append(fn[plen:-slen])
        ui.pager(""manifest"")
        for f in res:
            fm.startitem()
            fm.write(""path"", ""%s\n"", f)
        fm.end()
        return

    if rev and node:
        raise error.Abort(_(""please specify just one revision""))

    if not node:
        node = rev

    char = {""l"": ""@"", ""x"": ""*"", """": """"}
    mode = {""l"": ""644"", ""x"": ""755"", """": ""644""}
    ctx = scmutil.revsingle(repo, node)
    mf = ctx.manifest()
    ui.pager(""manifest"")
    for f in ctx:
        fm.startitem()
        fl = ctx[f].flags()
        fm.condwrite(ui.debugflag, ""hash"", ""%s "", hex(mf[f]))
        fm.condwrite(ui.verbose, ""mode type"", ""%s %1s "", mode[fl], char[fl])
        fm.write(""path"", ""%s\n"", f)
    fm.end()"
98,"  def SetOutputFormat(self, output_format):
    """"""Sets the output format for errors.""""""",    self.output_format = output_format
99,"    def save_conversations(
        cls,
        act_list,
        datapath,
        opt,
        save_keys='all',
        context_ids='context',
        self_chat=False,
        **kwargs,
    ):
        """"""
        Write Conversations to file from an act list.

        Conversations assume the act list is of the following form: a list of episodes,
        each of which is comprised of a list of act pairs (i.e. a list dictionaries
        returned from one parley)
        """"""","        cls._check_parent_dir_exits(datapath)
        to_save = cls._get_path(datapath)

        context_ids = context_ids.strip().split(',')
        # save conversations
        speakers = []
        with PathManager.open(to_save, 'w') as f:
            for ep in act_list:
                if not ep:
                    continue
                convo = {
                    'dialog': [],
                    'context': [],
                    'metadata_path': Metadata._get_path(to_save),
                }
                for act_pair in ep:
                    new_pair = []
                    for ex in act_pair:
                        ex_id = ex.get('id')
                        if ex_id in context_ids:
                            context = True
                        else:
                            context = False
                            if ex_id not in speakers:
                                speakers.append(ex_id)

                        # set turn
                        turn = {}
                        if save_keys != 'all':
                            save_keys_lst = save_keys.split(',')
                        else:
                            save_keys_lst = ex.keys()
                        for key in save_keys_lst:
                            turn[key] = ex.get(key, '')
                            if key == 'metrics':
                                turn[key] = dict_report(turn[key])
                        turn['id'] = ex_id
                        if not context:
                            new_pair.append(turn)
                        else:
                            convo['context'].append(turn)
                    if new_pair:
                        convo['dialog'].append(new_pair)
                json_convo = json.dumps(convo, default=lambda v: '<not serializable>')
                f.write(json_convo + '\n')
        logging.info(f'Conversations saved to file: {to_save}')

        # save metadata
        Metadata.save_metadata(
            to_save, opt, self_chat=self_chat, speakers=speakers, **kwargs
        )"
100,"    def is_dir(self):
        """"""
        Whether this path is a directory.
        """"""","        try:
            return S_ISDIR(self.stat().st_mode)
        except OSError as e:
            if e.errno != ENOENT:
                raise
            # Path doesn't exist or is a broken symlink
            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)
            return False"
101,"def findsections(blocks):
    """"""Finds sections.

    The blocks must have a 'type' field, i.e., they should have been
    run through findliteralblocks first.
    """"""","    for block in blocks:
        # Searching for a block that looks like this:
        #
        # +------------------------------+
        # | Section title                |
        # | -------------                |
        # +------------------------------+
        if (
            block[""type""] == ""paragraph""
            and len(block[""lines""]) == 2
            and encoding.colwidth(block[""lines""][0]) <= len(block[""lines""][1])
            and _sectionre.match(block[""lines""][1])
        ):
            block[""underline""] = block[""lines""][1][0:1]
            block[""type""] = ""section""
            del block[""lines""][1]
    return blocks"
102,"def rollback(ui, repo, **opts):
    """"""roll back the last transaction (DANGEROUS) (DEPRECATED)

    Please use :prog:`commit --amend` instead of rollback to correct
    mistakes in the last commit.

    This command should be used with care. There is only one level of
    rollback, and there is no way to undo a rollback. It will also
    restore the dirstate at the time of the last transaction, losing
    any dirstate changes since that time. This command does not alter
    the working directory.

    Transactions are used to encapsulate the effects of all commands
    that create new commits or propagate existing commits into a
    repository.

    .. container:: verbose

      For example, the following commands are transactional, and their
      effects can be rolled back:

      - commit
      - import
      - pull
      - push (with this repository as the destination)
      - unbundle

      To avoid permanent data loss, rollback will refuse to rollback a
      commit transaction if it isn't checked out. Use --force to
      override this protection.

      The rollback command can be entirely disabled by setting the
      ``ui.rollback`` configuration setting to false. If you're here
      because you want to use rollback and it's disabled, you can
      re-enable the command by setting ``ui.rollback`` to true.

    This command is not intended for use on public repositories. Once
    changes are visible for pull by other users, rolling a transaction
    back locally is ineffective (someone else may already have pulled
    the changes). Furthermore, a race is possible with readers of the
    repository; for example an in-progress pull from the repository
    may fail if a rollback is performed.

    Returns 0 on success, 1 if no rollback data is available.
    """"""","    raise error.Abort(_(""rollback is dangerous and should not be used""))"
103,"    def identifier(self):
        """"""
        :return: XCUIElement identifier
        :rtype: lldb.SBValue
        """"""","        if self._identifier is None:
            name = ""_identifier""
            if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:
                self._identifier = fb.evaluateExpressionValue(
                    ""(NSString *)[{} identifier]"".format(self.element_value)
                )
            else:
                self._identifier = self.element.GetChildMemberWithName(name)
        return self._identifier"
104,"    def _add(self, tensors: List[Optional[torch.Tensor]]) -> torch.Tensor:
        """"""
        Handle addition of None tensors.

        Smart addition. Adds tensors if they are not None.

        :param tensors:
            A list of torch.Tensor, with at least one non-null object

        :return:
            The result of adding all non-null objects in tensors
        """"""","        non_null_tensors: List[torch.Tensor] = [t for t in tensors if t is not None]
        return reduce(lambda a, b: a + b, non_null_tensors)"
105,"def debugremotebookmark(ui, repo, name, rev):
    """"""Change a remote bookmark under the 'debugremote' namespace.""""""","    node = scmutil.revsingle(repo, rev).node()
    setremotebookmark(repo, ""debugremote/%s"" % name, node)"
106,"    def num_examples(self):
        """"""
        Return the number of examples for the root world.
        """"""",        return self.world.num_examples()
107,"def rle_mask_voting(
    top_masks, all_masks, all_dets, iou_thresh, binarize_thresh, method='AVG'
):
    """"""Returns new masks (in correspondence with `top_masks`) by combining
    multiple overlapping masks coming from the pool of `all_masks`. Two methods
    for combining masks are supported: 'AVG' uses a weighted average of
    overlapping mask pixels; 'UNION' takes the union of all mask pixels.
    """"""","    if len(top_masks) == 0:
        return

    all_not_crowd = [False] * len(all_masks)
    top_to_all_overlaps = mask_util.iou(top_masks, all_masks, all_not_crowd)
    decoded_all_masks = [
        np.array(mask_util.decode(rle), dtype=np.float32) for rle in all_masks
    ]
    decoded_top_masks = [
        np.array(mask_util.decode(rle), dtype=np.float32) for rle in top_masks
    ]
    all_boxes = all_dets[:, :4].astype(np.int32)
    all_scores = all_dets[:, 4]

    # Fill box support with weights
    mask_shape = decoded_all_masks[0].shape
    mask_weights = np.zeros((len(all_masks), mask_shape[0], mask_shape[1]))
    for k in range(len(all_masks)):
        ref_box = all_boxes[k]
        x_0 = max(ref_box[0], 0)
        x_1 = min(ref_box[2] + 1, mask_shape[1])
        y_0 = max(ref_box[1], 0)
        y_1 = min(ref_box[3] + 1, mask_shape[0])
        mask_weights[k, y_0:y_1, x_0:x_1] = all_scores[k]
    mask_weights = np.maximum(mask_weights, 1e-5)

    top_segms_out = []
    for k in range(len(top_masks)):
        # Corner case of empty mask
        if decoded_top_masks[k].sum() == 0:
            top_segms_out.append(top_masks[k])
            continue

        inds_to_vote = np.where(top_to_all_overlaps[k] >= iou_thresh)[0]
        # Only matches itself
        if len(inds_to_vote) == 1:
            top_segms_out.append(top_masks[k])
            continue

        masks_to_vote = [decoded_all_masks[i] for i in inds_to_vote]
        if method == 'AVG':
            ws = mask_weights[inds_to_vote]
            soft_mask = np.average(masks_to_vote, axis=0, weights=ws)
            mask = np.array(soft_mask > binarize_thresh, dtype=np.uint8)
        elif method == 'UNION':
            # Any pixel that's on joins the mask
            soft_mask = np.sum(masks_to_vote, axis=0)
            mask = np.array(soft_mask > 1e-5, dtype=np.uint8)
        else:
            raise NotImplementedError('Method {} is unknown'.format(method))
        rle = mask_util.encode(np.array(mask[:, :, np.newaxis], order='F'))[0]
        top_segms_out.append(rle)

    return top_segms_out"
108,"        def fetchthread(self, queue, abort, fetchstart, fetchend):
            """"""Fetches every revision from fetchstart to fetchend (inclusive)
            and places them on the queue. This function is meant to run on a
            background thread and listens to the abort event to abort early.
            """"""","            clrev = fetchstart
            chunksize = 1000
            try:
                while True:
                    if abort.isSet():
                        break

                    maxrev = min(clrev + chunksize, fetchend + 1)
                    self.sqlcursor.execute(
                        """"""SELECT path, chunk, chunkcount,
                        linkrev, entry, data0, data1 FROM revisions WHERE repo = %s
                        AND linkrev > %s AND linkrev < %s ORDER BY linkrev ASC"""""",
                        (self.sqlreponame, clrev - 1, maxrev),
                    )

                    # Put split chunks back together into a single revision
                    groupedrevdata = {}
                    for revdata in self.sqlcursor:
                        revdata = (decodeutf8(revdata[0]),) + revdata[1:]

                        name = revdata[0]
                        chunk = revdata[1]
                        linkrev = revdata[3]

                        # Some versions of the MySQL Python connector have a
                        # bug where it converts aa column containing a single
                        # null byte into None. hgsql needs this workaround to
                        # handle file revisions that are exactly a single null
                        # byte.
                        #
                        # The only column that can contain a single null byte
                        # here is data1 (column 6):
                        # * path is a path, so Unix rules prohibit it from
                        #    containing null bytes.
                        # * chunk, chunkcount and linkrev are integers.
                        # * entry is a binary blob that matches a revlog index
                        #   entry, which cannot be ""\0"".
                        # * data0 is either empty or ""u"".
                        if revdata[6] is None:
                            revdata = revdata[:6] + (b""\0"",)

                        groupedrevdata.setdefault((name, linkrev), {})[chunk] = revdata

                    if not groupedrevdata:
                        break

                    fullrevisions = []
                    for chunks in pycompat.itervalues(groupedrevdata):
                        chunkcount = chunks[0][2]
                        if chunkcount == 1:
                            fullrevisions.append(chunks[0])
                        elif chunkcount == len(chunks):
                            fullchunk = list(chunks[0])
                            data1 = b""""
                            for i in range(0, chunkcount):
                                data1 += chunks[i][6]
                            fullchunk[6] = data1
                            fullrevisions.append(tuple(fullchunk))
                        else:
                            raise Exception(
                                ""missing revision chunk - expected %s got %s""
                                % (chunkcount, len(chunks))
                            )

                    fullrevisions = sorted(
                        fullrevisions, key=lambda revdata: revdata[3]
                    )
                    for revdata in fullrevisions:
                        queue.put(revdata)

                    clrev += chunksize
            except Exception as ex:
                queue.put(ex)
                return

            queue.put(False)"
109,"def localgetdiff(repo, diffid):
    """"""Scans the changelog for commit lines mentioning the Differential ID""""""","
    if repo.ui.configbool(""phrevset"", ""graphqlonly""):
        raise error.Abort(
            _(""phrevset.graphqlonly is set and Phabricator cannot resolve D%s"") % diffid
        )

    repo.ui.debug(""[diffrev] Traversing log for %s\n"" % diffid)

    def check(repo, rev, diffid):
        changectx = repo[rev]
        desc = changectx.description()
        match = DIFFERENTIAL_REGEX.search(desc)

        if match and match.group(""id"") == diffid:
            return changectx.node()
        else:
            return None

    # Search through draft commits first. This is still needed as there are
    # cases where Phabricator GraphQL cannot resolve the commit for some reason
    # and the user really wants to resolve the commit locally (ex. S199694).
    for rev in repo.revs(""sort(draft(), -rev)""):
        matched = check(repo, rev, diffid)
        if matched is not None:
            return matched

    repo.ui.warn(
        _(""D%s not found in drafts. Perform (slow) full changelog scan.\n"") % diffid
    )

    # Search through the whole changelog. This does not scale. Log this as we
    # plan to remove it at some point.
    repo.ui.log(
        ""features"",
        fullargs=repr(pycompat.sysargv),
        feature=""phrevset-full-changelog-scan"",
    )
    for rev in repo.changelog.revs(start=len(repo.changelog), stop=0):
        matched = check(repo, rev, diffid)
        if matched is not None:
            return matched

    return None"
110,"    def clip(self, box_size: Tuple[int, int]) -> None:
        """"""
        Clip (in place) the boxes by limiting x coordinates to the range [0, width]
        and y coordinates to the range [0, height].

        Args:
            box_size (height, width): The clipping box's size.
        """"""","        assert torch.isfinite(self.tensor).all(), ""Box tensor contains infinite or NaN!""
        h, w = box_size
        x1 = self.tensor[:, 0].clamp(min=0, max=w)
        y1 = self.tensor[:, 1].clamp(min=0, max=h)
        x2 = self.tensor[:, 2].clamp(min=0, max=w)
        y2 = self.tensor[:, 3].clamp(min=0, max=h)
        self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)"
111,"    def _set_query_vec(self, observation: Message) -> Message:
        """"""
        Override RAG.set_query_vec to optionally filter keys.
        """"""","        query_str = observation[self._query_key]
        if self.opt['retriever_ignore_phrases']:
            retriever_ignore_phrases = self.opt['retriever_ignore_phrases'].split("","")
            for retriever_ignore_phrase in retriever_ignore_phrases:
                query_str = self._filter_text(
                    query_str,
                    retriever_ignore_phrase,
                    delimiter=self.opt['retriever_delimiter'],
                )
        model = self.model
        if isinstance(model, torch.nn.parallel.DistributedDataParallel):
            model = self.model.module
        observation['query_vec'] = model.tokenize_query(query_str)  # type: ignore
        return observation"
112,"    def _addbackupentry(self, entry):
        """"""register a new backup entry and write it to disk""""""","        self._backupentries.append(entry)
        self._backupmap[entry[1]] = len(self._backupentries) - 1
        self._backupsfile.write(encodeutf8(""%s\0%s\0%s\0%d\n"" % entry))
        self._backupsfile.flush()"
113,"    def filter_goals(self, goals):
        """"""
        Used in single goal/api schema agents only.
        """"""","        result = []
        for goal in goals:
            if goal[""api_name""] in VALID_OUT_DOMAIN_API_NAMES:
                result.append(goal)
        return result"
114,"    def from_matches(
        packed_annotations: Any, densepose_outputs_size_hw: Tuple[int, int]
    ) -> ""BilinearInterpolationHelper"":
        """"""
        Args:
            packed_annotations: annotations packed into tensors, the following
                attributes are required:
                 - bbox_xywh_gt
                 - bbox_xywh_est
                 - x_gt
                 - y_gt
                 - point_bbox_with_dp_indices
                 - point_bbox_indices
            densepose_outputs_size_hw (tuple [int, int]): resolution of
                DensePose predictor outputs (H, W)
        Return:
            An instance of `BilinearInterpolationHelper` used to perform
            interpolation for the given annotation points and output resolution
        """"""","
        zh, zw = densepose_outputs_size_hw
        x0_gt, y0_gt, w_gt, h_gt = packed_annotations.bbox_xywh_gt[
            packed_annotations.point_bbox_with_dp_indices
        ].unbind(dim=1)
        x0_est, y0_est, w_est, h_est = packed_annotations.bbox_xywh_est[
            packed_annotations.point_bbox_with_dp_indices
        ].unbind(dim=1)
        x_lo, x_hi, x_w, jx_valid = _linear_interpolation_utilities(
            packed_annotations.x_gt, x0_gt, w_gt, x0_est, w_est, zw
        )
        y_lo, y_hi, y_w, jy_valid = _linear_interpolation_utilities(
            packed_annotations.y_gt, y0_gt, h_gt, y0_est, h_est, zh
        )
        j_valid = jx_valid * jy_valid

        w_ylo_xlo = (1.0 - x_w) * (1.0 - y_w)
        w_ylo_xhi = x_w * (1.0 - y_w)
        w_yhi_xlo = (1.0 - x_w) * y_w
        w_yhi_xhi = x_w * y_w

        return BilinearInterpolationHelper(
            packed_annotations,
            j_valid,
            y_lo,
            y_hi,
            x_lo,
            x_hi,
            w_ylo_xlo,  # pyre-ignore[6]
            w_ylo_xhi,
            # pyre-fixme[6]: Expected `Tensor` for 9th param but got `float`.
            w_yhi_xlo,
            w_yhi_xhi,
        )"
115,"    def get_partial_only_reranker_class(cls) -> AbstractReranker:
        """"""
        Return class to instantiate classifier.
        """"""",        return RPAReranker
116,"    def _copy_hg_exe(self, dirtocopy):
        """"""Copy main mercurial executable which would load the embedded Python""""""","        bindir = scriptdir
        if not self.local_bins:
            # copy .exe's from ./build/lib.win-amd64/, not from ./
            bindir = pjoin(scriptdir, ""build"", distutils_dir_name(""scripts""))
            sourcename = f""{hgname}.exe"" if iswindows else f""{hgname}.rust""
        else:
            sourcename = f""{hgname}.exe"" if iswindows else hgname
        targetname = f""{hgname}.exe"" if iswindows else hgname
        log.debug(""copying main mercurial binary from %s"" % bindir)
        copy_to(pjoin(bindir, sourcename), pjoin(dirtocopy, targetname))
        # On Windows, debuginfo is not embedded, but stored as .pdb.
        # Copy it for better debugging.
        if iswindows:
            pdbname = pjoin(bindir, f""{hgname}.pdb"")
            copy_to(pdbname, pjoin(dirtocopy, f""{hgname}.pdb""))"
117,"    def _get_message_fields(self, text, speaker, speakers, prev_context):
        """"""
        If `include_speaker_in_context` is True, keep speaker ids in the text.

        If `add_speaker_to_context_end` is True, add speaker ids at the end of text, and
        remove speaker ids from the labels. If `include_speaker_in_context` is False,
        but `add_speaker_to_context_end` is True, add an empty sentence at the end of
        text and add the current speaker id to the list of speakers, to indicate the
        speaker for the empty sentence.
        """"""","        hasAddedSpeaker = False
        if self.include_speaker_in_context:
            if self.add_speaker_to_context_end:
                label = text
                text = prev_context + f'{self.utterance_delimiter}{speaker}: '
                # Save current spaker as the speaker for the empty utterance
                speakers.append(speaker)
                hasAddedSpeaker = True
            else:
                label = f'{speaker}: {text}'
                text = prev_context
        else:
            if self.add_speaker_to_context_end:
                label = text
                # The whitespace is left at the end to indicate an empty utterance
                text = prev_context + f'{self.utterance_delimiter} '
                # Save current spaker as the speaker for the empty utterance
                speakers.append(speaker)
                hasAddedSpeaker = True
            else:
                label = f'{speaker}: {text}'
                text = prev_context

        return text, label, speakers, hasAddedSpeaker"
118,"def clienttelemetryfunc(f):
    """"""Decorator for registering client telemetry functions.""""""","    _clienttelemetryfuncs[f.__name__] = f
    return f"
119,"    def parley(self):
        """"""
        A parley should represent one turn of your onboarding task.
        """"""",        self.episodeDone = True
120,"    def _setup_data(self, datatype):
        """"""
        Passthrough.
        """"""",        pass
121,"    def __init__(self):
        """""" the constructor should set the following fields: """"""","        self.d = -1
        self.metric = 'L2'   # or IP
        self.nq = -1
        self.nb = -1
        self.nt = -1"
122,"    def lastnode(self):
        """"""return last node in revmap, or None if revmap is empty""""""","        if self._revmap is None:
            # fast path, read revmap without loading its full content
            return revmapmod.getlastnode(self.revmappath)
        else:
            return self._revmap.rev2hsh(self._revmap.maxrev)"
123,"    def forward(self, xs, encoder_output, incremental_state=None):
        """"""
        Decode from input tokens.

        :param xs: (bsz x seqlen) LongTensor of input token indices
        :param encoder_output: output from HredEncoder. Tuple containing
            (enc_out, enc_hidden, attn_mask, context_hidden) tuple.
        :param incremental_state: most recent hidden state to the decoder.
        :returns: (output, hidden_state) pair from the RNN.
            - output is a bsz x time x latentdim matrix. This value must be passed to
                the model's OutputLayer for a final softmax.
            - hidden_state depends on the choice of RNN
        """"""","        (
            enc_state,
            (hidden_state, cell_state),
            attn_mask,
            context_hidden,
        ) = encoder_output

        # sequence indices => sequence embeddings
        seqlen = xs.size(1)
        xes = self.dropout(self.lt(xs))

        # concatenate context lstm hidden state
        context_hidden_final_layer = context_hidden[:, -1, :].unsqueeze(1)
        resized_context_h = context_hidden_final_layer.expand(-1, seqlen, -1)
        xes = torch.cat((xes, resized_context_h), dim=-1).to(xes.device)

        # run through rnn with None as initial decoder state
        # source for zeroes hidden state: http://www.cs.toronto.edu/~lcharlin/papers/vhred_aaai17.pdf
        output, new_hidden = self.rnn(xes, None)

        return output, _transpose_hidden_state(new_hidden)"
124,"    def commitpending(self):
        """"""Used in alternative manifestlog implementations to flush additions to
        disk.""""""",
125,"    def inference(self, predictions, proposals):
        """"""
        Returns:
            list[Instances]: same as `fast_rcnn_inference_rotated`.
            list[Tensor]: same as `fast_rcnn_inference_rotated`.
        """"""","        boxes = self.predict_boxes(predictions, proposals)
        scores = self.predict_probs(predictions, proposals)
        image_shapes = [x.image_size for x in proposals]

        return fast_rcnn_inference_rotated(
            boxes,
            scores,
            image_shapes,
            self.test_score_thresh,
            self.test_nms_thresh,
            self.test_topk_per_image,
        )"
126,"    def _apply_model_parallel(
        self, tensor: torch.Tensor, *extra_args, incr_state: DecoderIncrState
    ) -> Tuple[torch.Tensor, DecoderIncrState]:
        """"""
        Pipeline application of model parallelism.
        """"""","        chunks = PipelineHelper.split((tensor, *extra_args, incr_state))
        work_items = PipelineHelper.schedule_work_items(self.layers, chunks)

        new_incr_state = {i: [] for i, _ in enumerate(self.layers)}

        for chunk_idx, layer_nos, next_device in work_items:
            s_tensor, *s_extra_args, s_incr_state = chunks[chunk_idx]
            for layer_no in layer_nos:
                s_tensor, nis = self.layers[layer_no](
                    s_tensor, *s_extra_args, incr_state=s_incr_state.get(layer_no)
                )
                new_incr_state[layer_no].append(nis)
            # don't move incr state, it's always on the correct device
            s_layer_args = PipelineHelper.chunk_to(
                (s_tensor, *s_extra_args), next_device
            )
            chunks[chunk_idx] = (*s_layer_args, s_incr_state)

        tensor_out = PipelineHelper.join([c[0] for c in chunks])
        new_incr_state = {
            layer_no: PipelineHelper.join(pieces)
            for layer_no, pieces in new_incr_state.items()
        }

        return tensor_out, new_incr_state"
127,"def parsebundlespec(repo, spec, strict=True, externalnames=False):
    """"""Parse a bundle string specification into parts.

    Bundle specifications denote a well-defined bundle/exchange format.
    The content of a given specification should not change over time in
    order to ensure that bundles produced by a newer version of Mercurial are
    readable from an older version.

    The string currently has the form:

       <compression>-<type>[;<parameter0>[;<parameter1>]]

    Where <compression> is one of the supported compression formats
    and <type> is (currently) a version string. A "";"" can follow the type and
    all text afterwards is interpreted as URI encoded, "";"" delimited key=value
    pairs.

    If ``strict`` is True (the default) <compression> is required. Otherwise,
    it is optional.

    If ``externalnames`` is False (the default), the human-centric names will
    be converted to their internal representation.

    Returns a 3-tuple of (compression, version, parameters). Compression will
    be ``None`` if not in strict mode and a compression isn't defined.

    An ``InvalidBundleSpecification`` is raised when the specification is
    not syntactically well formed.

    An ``UnsupportedBundleSpecification`` is raised when the compression or
    bundle type/version is not recognized.

    Note: this function will likely eventually return a more complex data
    structure, including bundle2 part information.
    """"""","
    def parseparams(s):
        if "";"" not in s:
            return s, {}

        params = {}
        version, paramstr = s.split("";"", 1)

        for p in paramstr.split("";""):
            if ""="" not in p:
                raise error.InvalidBundleSpecification(
                    _(""invalid bundle specification: "" 'missing ""="" in parameter: %s')
                    % p
                )

            key, value = p.split(""="", 1)
            key = urlreq.unquote(key)
            value = urlreq.unquote(value)
            params[key] = value

        return version, params

    if strict and ""-"" not in spec:
        raise error.InvalidBundleSpecification(
            _(""invalid bundle specification; "" ""must be prefixed with compression: %s"")
            % spec
        )

    if ""-"" in spec:
        compression, version = spec.split(""-"", 1)

        if compression not in util.compengines.supportedbundlenames:
            raise error.UnsupportedBundleSpecification(
                _(""%s compression is not supported"") % compression
            )

        version, params = parseparams(version)

        if version not in _bundlespeccgversions:
            raise error.UnsupportedBundleSpecification(
                _(""%s is not a recognized bundle version"") % version
            )
    else:
        # Value could be just the compression or just the version, in which
        # case some defaults are assumed (but only when not in strict mode).
        assert not strict

        spec, params = parseparams(spec)

        if spec in util.compengines.supportedbundlenames:
            compression = spec
            # Default to bundlev2
            version = ""v2""
        elif spec in _bundlespeccgversions:
            if spec == ""packed1"":
                compression = ""none""
            else:
                compression = ""bzip2""
            version = spec
        else:
            raise error.UnsupportedBundleSpecification(
                _(""%s is not a recognized bundle specification"") % spec
            )

    # Bundle version 1 only supports a known set of compression engines.
    if version == ""v1"" and compression not in _bundlespecv1compengines:
        raise error.UnsupportedBundleSpecification(
            _(""compression engine %s is not supported on v1 bundles"") % compression
        )

    # The specification for packed1 can optionally declare the data formats
    # required to apply it. If we see this metadata, compare against what the
    # repo supports and error if the bundle isn't compatible.
    if version == ""packed1"" and ""requirements"" in params:
        requirements = set(params[""requirements""].split("",""))
        missingreqs = requirements - repo.supportedformats
        if missingreqs:
            raise error.UnsupportedBundleSpecification(
                _(""missing support for repository features: %s"")
                % "", "".join(sorted(missingreqs))
            )

    if not externalnames:
        engine = util.compengines.forbundlename(compression)
        compression = engine.bundletype()[1]
        version = _bundlespeccgversions[version]
    return compression, version, params"
128,"    def _init_attributes(self, opt: Opt):
        """"""
        Initialize teacher attributes.
        """"""","        self.add_missing_turns = opt.get('add_missing_turns', 'none')
        self.label_type = opt.get('label_type', 'response')
        self.include_knowledge = opt.get('include_knowledge', True)
        self.include_checked_sentence = opt.get('include_checked_sentence', False)
        self.knowledge_separator = opt.get('include_knowledge_separator', False)
        self.chosen_topic_delimiter = opt.get('chosen_topic_delimiter', '\n')"
129,"    def create_message(self, queue_output: ChunkOutput, entry_idx=0) -> Message:
        """"""
        Given the tuple output of the queue, return an act.

        May depend on entry index if queue output is a multi-turn episode.
        """"""",        return queue_output
130,"def CheckForNonConstReference(filename, clean_lines, linenum,
                              nesting_state, error):
  """"""Check for non-const references.

  Separate from CheckLanguage since it scans backwards from current
  line, instead of scanning forward.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    nesting_state: A _NestingState instance which maintains information about
                   the current stack of nested blocks being parsed.
    error: The function to call with any errors found.
  """"""","  # Do nothing if there is no '&' on current line.
  line = clean_lines.elided[linenum]
  if '&' not in line:
    return

  # Long type names may be broken across multiple lines, usually in one
  # of these forms:
  #   LongType
  #       ::LongTypeContinued &identifier
  #   LongType::
  #       LongTypeContinued &identifier
  #   LongType<
  #       ...>::LongTypeContinued &identifier
  #
  # If we detected a type split across two lines, join the previous
  # line to current line so that we can match const references
  # accordingly.
  #
  # Note that this only scans back one line, since scanning back
  # arbitrary number of lines would be expensive.  If you have a type
  # that spans more than 2 lines, please use a typedef.
  if linenum > 1:
    previous = None
    if Match(r'\s*::(?:[\w<>]|::)+\s*&\s*\S', line):
      # previous_line\n + ::current_line
      previous = Search(r'\b((?:const\s*)?(?:[\w<>]|::)+[\w<>])\s*$',
                        clean_lines.elided[linenum - 1])
    elif Match(r'\s*[a-zA-Z_]([\w<>]|::)+\s*&\s*\S', line):
      # previous_line::\n + current_line
      previous = Search(r'\b((?:const\s*)?(?:[\w<>]|::)+::)\s*$',
                        clean_lines.elided[linenum - 1])
    if previous:
      line = previous.group(1) + line.lstrip()
    else:
      # Check for templated parameter that is split across multiple lines
      endpos = line.rfind('>')
      if endpos > -1:
        (_, startline, startpos) = ReverseCloseExpression(
            clean_lines, linenum, endpos)
        if startpos > -1 and startline < linenum:
          # Found the matching < on an earlier line, collect all
          # pieces up to current line.
          line = ''
          for i in xrange(startline, linenum + 1):
            line += clean_lines.elided[i].strip()

  # Check for non-const references in function parameters.  A single '&' may
  # found in the following places:
  #   inside expression: binary & for bitwise AND
  #   inside expression: unary & for taking the address of something
  #   inside declarators: reference parameter
  # We will exclude the first two cases by checking that we are not inside a
  # function body, including one that was just introduced by a trailing '{'.
  # TODO(unknwon): Doesn't account for preprocessor directives.
  # TODO(unknown): Doesn't account for 'catch(Exception& e)' [rare].
  check_params = False
  if not nesting_state.stack:
    check_params = True  # top level
  elif (isinstance(nesting_state.stack[-1], _ClassInfo) or
        isinstance(nesting_state.stack[-1], _NamespaceInfo)):
    check_params = True  # within class or namespace
  elif Match(r'.*{\s*$', line):
    if (len(nesting_state.stack) == 1 or
        isinstance(nesting_state.stack[-2], _ClassInfo) or
        isinstance(nesting_state.stack[-2], _NamespaceInfo)):
      check_params = True  # just opened global/class/namespace block
  # We allow non-const references in a few standard places, like functions
  # called ""swap()"" or iostream operators like ""<<"" or "">>"".  Do not check
  # those function parameters.
  #
  # We also accept & in static_assert, which looks like a function but
  # it's actually a declaration expression.
  whitelisted_functions = (r'(?:[sS]wap(?:<\w:+>)?|'
                           r'operator\s*[<>][<>]|'
                           r'static_assert|COMPILE_ASSERT'
                           r')\s*\(')
  if Search(whitelisted_functions, line):
    check_params = False
  elif not Search(r'\S+\([^)]*$', line):
    # Don't see a whitelisted function on this line.  Actually we
    # didn't see any function name on this line, so this is likely a
    # multi-line parameter list.  Try a bit harder to catch this case.
    for i in xrange(2):
      if (linenum > i and
          Search(whitelisted_functions, clean_lines.elided[linenum - i - 1])):
        check_params = False
        break

  if check_params:
    decls = ReplaceAll(r'{[^}]*}', ' ', line)  # exclude function body
    for parameter in re.findall(_RE_PATTERN_REF_PARAM, decls):
      if not Match(_RE_PATTERN_CONST_REF_PARAM, parameter):
        error(filename, linenum, 'runtime/references', 2,
              'Is this a non-const reference? '
              'If so, make const or use a pointer: ' +
              ReplaceAll(' *<', '<', parameter))"
131,"    def space_tokenize(text):
        """"""
        Tokenize exactly on spaces.

        Useful when text is pre-tokenized.
        """"""",        return text.strip().split(' ')
132,"def CheckPosixThreading(filename, clean_lines, linenum, error):
  """"""Checks for calls to thread-unsafe functions.

  Much code has been originally written without consideration of
  multi-threading. Also, engineers are relying on their old experience;
  they have learned posix before threading extensions were added. These
  tests guide the engineers to use thread-safe functions (when using
  posix directly).

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    error: The function to call with any errors found.
  """"""","  line = clean_lines.elided[linenum]
  for single_thread_function, multithread_safe_function in threading_list:
    ix = line.find(single_thread_function)
    # Comparisons made explicit for clarity -- pylint: disable-msg=C6403
    if ix >= 0 and (ix == 0 or (not line[ix - 1].isalnum() and
                                line[ix - 1] not in ('_', '.', '>'))):
      error(filename, linenum, 'runtime/threadsafe_fn', 2,
            'Consider using ' + multithread_safe_function +
            '...) instead of ' + single_thread_function +
            '...) for improved thread safety.')"
133,"    def placeholder_summary(self):
        """"""
        :return: XCUIElement placeholderValue summary
        :rtype: str | None
        """"""","        if len(self.placeholder_value) == 0:
            return None
        return ""placeholderValue: '{}'"".format(self.placeholder_value)"
134,"    def transaction(self):
        """"""Return an open transaction object, constructing if necessary""""""","        if not self._tr:
            trname = ""%s\n%s"" % (self.source, util.hidepassword(self.url))
            self._tr = self.repo.transaction(trname)
            self._tr.hookargs[""source""] = self.source
            self._tr.hookargs[""url""] = self.url
        return self._tr"
135,"    def release(self):
        """"""release transaction if created""""""","        if self._tr is not None:
            self._tr.release()"
136,"def _win32_strerror(err):
    """""" expand a win32 error code into a human readable message """"""","
    # FormatMessage will allocate memory and assign it here
    buf = ctypes.c_char_p()
    FormatMessage(
        FORMAT_MESSAGE_FROM_SYSTEM | FORMAT_MESSAGE_ALLOCATE_BUFFER
        | FORMAT_MESSAGE_IGNORE_INSERTS, None, err, 0, buf, 0, None)
    try:
        return buf.value
    finally:
        LocalFree(buf)"
137,"def insert(sqlconn, tablename, argsdict):
    """"""
    Inserts new row into a table, given a name of a table and a mapping
    column name -> column value
    """"""","
    sqlcursor = sqlconn.cursor()

    items = list(argsdict.items())
    columns = "", "".join(
        (""{column_name}"".format(column_name=column_name) for column_name, _ in items)
    )

    placeholders = "", "".join((""%s"" for _ in items))

    insertstmt = ""INSERT INTO {table} ({columns}) VALUES ({placeholders})"".format(
        table=tablename, columns=columns, placeholders=placeholders
    )

    sqlcursor.execute(insertstmt, params=[value for _, value in items])
    sqlconn.commit()"
138,"def get_worker_from_agent(agent: Agent):
    """"""
    Returns Mephisto worker for a given ParlAI agent.
    """"""",    return agent.mephisto_agent.get_worker()
139,"    def copy(self):
        """"""return a copy of the part

        The new part have the very same content but no partid assigned yet.
        Parts with generated data cannot be copied.""""""","        assert not util.safehasattr(self.data, ""next"")
        return self.__class__(
            self.type,
            self._mandatoryparams,
            self._advisoryparams,
            self._data,
            self.mandatory,
        )"
140,"    def handle_user_utt(
        self, message: Message, prefix_stripped_text: str
    ) -> Optional[Dict[str, Metric]]:
        """"""
        Grab slots out of the user utterance based on an exact match.
        """"""","        utterance = prefix_stripped_text

        def get_slots(utt, options):
            results = set()
            for option in options:
                if option in utt:
                    results.add(option)
            return results

        all_slot_values_here = get_slots(utterance, self.all_goal_slot_values)
        req_slot_values_here = get_slots(utterance, self.all_req_goal_slot_values)

        self.mentioned_all_slot_values |= all_slot_values_here
        self.mentioned_req_slot_values |= req_slot_values_here

        metrics = {}
        metrics[""user_utt_avg_any_slot""] = AverageMetric(len(all_slot_values_here))
        metrics[""user_utt_avg_req_slot""] = AverageMetric(len(req_slot_values_here))
        return metrics"
141,"def id_pairs_table():
    """"""Returns a lookup table for glyph -> NLE id pairs.""""""","    table = np.zeros([MAX_GLYPH, 2], dtype=np.int16)

    num_nle_ids = 0

    for glyph in range(GLYPH_MON_OFF, GLYPH_PET_OFF):
        table[glyph] = (glyph, GlyphGroup.MON)
        num_nle_ids += 1

    for glyph in range(GLYPH_PET_OFF, GLYPH_INVIS_OFF):
        table[glyph] = (glyph - GLYPH_PET_OFF, GlyphGroup.PET)

    for glyph in range(GLYPH_INVIS_OFF, GLYPH_DETECT_OFF):
        table[glyph] = (num_nle_ids, GlyphGroup.INVIS)
        num_nle_ids += 1

    for glyph in range(GLYPH_DETECT_OFF, GLYPH_BODY_OFF):
        table[glyph] = (glyph - GLYPH_DETECT_OFF, GlyphGroup.DETECT)

    for glyph in range(GLYPH_BODY_OFF, GLYPH_RIDDEN_OFF):
        table[glyph] = (glyph - GLYPH_BODY_OFF, GlyphGroup.BODY)

    for glyph in range(GLYPH_RIDDEN_OFF, GLYPH_OBJ_OFF):
        table[glyph] = (glyph - GLYPH_RIDDEN_OFF, GlyphGroup.RIDDEN)

    for glyph in range(GLYPH_OBJ_OFF, GLYPH_CMAP_OFF):
        table[glyph] = (num_nle_ids, GlyphGroup.OBJ)
        num_nle_ids += 1

    for glyph in range(GLYPH_CMAP_OFF, GLYPH_EXPLODE_OFF):
        table[glyph] = (num_nle_ids, GlyphGroup.CMAP)
        num_nle_ids += 1

    for glyph in range(GLYPH_EXPLODE_OFF, GLYPH_ZAP_OFF):
        id_ = num_nle_ids + (glyph - GLYPH_EXPLODE_OFF) // MAXEXPCHARS
        table[glyph] = (id_, GlyphGroup.EXPLODE)

    num_nle_ids += EXPL_MAX

    for glyph in range(GLYPH_ZAP_OFF, GLYPH_SWALLOW_OFF):
        id_ = num_nle_ids + (glyph - GLYPH_ZAP_OFF) // 4
        table[glyph] = (id_, GlyphGroup.ZAP)

    num_nle_ids += NUM_ZAP

    for glyph in range(GLYPH_SWALLOW_OFF, GLYPH_WARNING_OFF):
        table[glyph] = (num_nle_ids, GlyphGroup.SWALLOW)
    num_nle_ids += 1

    for glyph in range(GLYPH_WARNING_OFF, GLYPH_STATUE_OFF):
        table[glyph] = (num_nle_ids, GlyphGroup.WARNING)
        num_nle_ids += 1

    for glyph in range(GLYPH_STATUE_OFF, MAX_GLYPH):
        table[glyph] = (glyph - GLYPH_STATUE_OFF, GlyphGroup.STATUE)

    return table"
142,"    def prep_save_data(self, agent: Agent):
        """"""
        Saving session data after the world is closed.
        """"""","        rejection_reason = self.reason_to_reject()
        qualified_role = constants.APPRENTICE if self.episodeDone else constants.NO_ROLE
        return {
            constants.SAVED_DATA_IS_WIZARD_KEY: False,
            constants.SAVED_DATA_WORKER_KEY: self.get_worker_name(),
            constants.SAVED_DATA_ROLE_QUALIFICATION_DATA_KEY: (
                self.role_training_qname,
                qualified_role,
            ),
            constants.WORKER_REJECT_REASON: rejection_reason,
        }"
143,"    def set_fixed_candidates(self, shared):
        """"""
        Load a set of fixed candidates and their vectors (or vectorize them here).

        self.fixed_candidates will contain a [num_cands] list of strings
        self.fixed_candidate_vecs will contain a [num_cands, seq_len] LongTensor

        See the note on the --fixed-candidate-vecs flag for an explanation of the
        'reuse', 'replace', or path options.

        Note: TorchRankerAgent by default converts candidates to vectors by vectorizing
        in the common sense (i.e., replacing each token with its index in the
        dictionary). If a child model wants to additionally perform encoding, it can
        overwrite the vectorize_fixed_candidates() method to produce encoded vectors
        instead of just vectorized ones.
        """"""","        if shared:
            self.fixed_candidates = shared['fixed_candidates']
            self.fixed_candidate_vecs = shared['fixed_candidate_vecs']
            self.fixed_candidate_encs = shared['fixed_candidate_encs']
            self.num_fixed_candidates = shared['num_fixed_candidates']
        else:
            self.num_fixed_candidates = 0
            opt = self.opt
            cand_path = self.fixed_candidates_path
            if 'fixed' in (self.candidates, self.eval_candidates):
                if not cand_path:
                    # Attempt to get a standard candidate set for the given task
                    path = self.get_task_candidates_path()
                    if path:
                        logging.info(f""setting fixed_candidates path to: {path}"")
                        self.fixed_candidates_path = path
                        cand_path = self.fixed_candidates_path
                # Load candidates
                logging.info(f""Loading fixed candidate set from {cand_path}"")
                with PathManager.open(cand_path, 'r', encoding='utf-8') as f:
                    cands = [line.strip() for line in f.readlines()]
                # Load or create candidate vectors
                if PathManager.exists(self.opt['fixed_candidate_vecs']):
                    vecs_path = opt['fixed_candidate_vecs']
                    vecs = self.load_candidates(vecs_path)
                else:
                    setting = self.opt['fixed_candidate_vecs']
                    model_dir, model_file = os.path.split(self.opt['model_file'])
                    model_name = os.path.splitext(model_file)[0]
                    cands_name = os.path.splitext(os.path.basename(cand_path))[0]
                    vecs_path = os.path.join(
                        model_dir, '.'.join([model_name, cands_name, 'vecs'])
                    )
                    if setting == 'reuse' and PathManager.exists(vecs_path):
                        vecs = self.load_candidates(vecs_path)
                    else:  # setting == 'replace' OR generating for the first time
                        vecs = self._make_candidate_vecs(cands)
                        self._save_candidates(vecs, vecs_path)

                self.fixed_candidates = cands
                self.num_fixed_candidates = len(self.fixed_candidates)
                self.fixed_candidate_vecs = vecs
                if self.use_cuda:
                    self.fixed_candidate_vecs = self.fixed_candidate_vecs.cuda()

                if self.encode_candidate_vecs:
                    # candidate encodings are fixed so set them up now
                    enc_path = os.path.join(
                        model_dir, '.'.join([model_name, cands_name, 'encs'])
                    )
                    if setting == 'reuse' and PathManager.exists(enc_path):
                        encs = self.load_candidates(enc_path, cand_type='encodings')
                    else:
                        encs = self._make_candidate_encs(self.fixed_candidate_vecs)
                        self._save_candidates(
                            encs, path=enc_path, cand_type='encodings'
                        )
                    self.fixed_candidate_encs = encs
                    if self.use_cuda:
                        self.fixed_candidate_encs = self.fixed_candidate_encs.cuda()
                    if self.fp16:
                        self.fixed_candidate_encs = self.fixed_candidate_encs.half()
                    else:
                        self.fixed_candidate_encs = self.fixed_candidate_encs.float()
                else:
                    self.fixed_candidate_encs = None

            else:
                self.fixed_candidates = None
                self.fixed_candidate_vecs = None
                self.fixed_candidate_encs = None"
144,"    def test_topk(self):
        """"""
        Test topk generation.
        """"""","        # Topk is inherently stochastic, just ensure no crash.
        opt = ParlaiParser(True, True).parse_kwargs(
            model_file='zoo:unittest/transformer_generator2/model',
            inference='topk',
            topp=10,
        )
        agent = create_agent(opt, True)
        agent.observe({'text': '1', 'episode_done': True})
        result = agent.act()
        assert 'text' in result
        assert result['text'] != ''"
145,"def smoothen_convo(conversation, opt):
    """"""
    Aggregates contiguous responses by the same speaker in the data so that data
    eventually contains alternations between USER and ASSISTANT.

    :param conversation:
        The dialogue between USER and ASSISTANT with possible multiple contiguous
        speeches by the same speaker
    :param opt:
        options dict, mainly useful for accessing value of exclude_invalid_data
    """"""","    dialogue = conversation['utterances']
    conversation_stack = []
    for speech in dialogue:
        if (
            conversation_stack
            and speech[""speaker""] == conversation_stack[-1][""speaker""]
        ):
            conversation_stack.append(join_speech(conversation_stack.pop(), speech))
        else:
            conversation_stack.append(speech)
    processed_conversation = []
    corrupt = False
    for speech in conversation_stack:
        if (
            opt.get('exclude_invalid_data', True)
            and 'ctr' in speech
            and speech['ctr'] > 5
        ):
            corrupt = True
        processed_conversation += [speech]
    return update_indexes(processed_conversation), corrupt"
146,"def _makelogrevset(repo, pats, opts, revs):
    """"""Return (expr, filematcher) where expr is a revset string built
    from log options and file patterns or None. If --stat or --patch
    are not passed filematcher is None. Otherwise it is a callable
    taking a revision number and returning a match objects filtering
    the files to be detailed when displaying the revision.
    """"""","    opt2revset = {
        ""no_merges"": (""not merge()"", None),
        ""only_merges"": (""merge()"", None),
        ""_ancestors"": (""ancestors(%(val)s)"", None),
        ""_fancestors"": (""_firstancestors(%(val)s)"", None),
        ""_descendants"": (""descendants(%(val)s)"", None),
        ""_fdescendants"": (""_firstdescendants(%(val)s)"", None),
        ""_matchfiles"": (""_matchfiles(%(val)s)"", None),
        ""date"": (""date(%(val)r)"", None),
        ""branch"": (""branch(%(val)r)"", "" or ""),
        ""_patslog"": (""filelog(%(val)r)"", "" or ""),
        ""_patsfollow"": (""follow(%(val)r)"", "" or ""),
        ""_patsfollowfirst"": (""_followfirst(%(val)r)"", "" or ""),
        ""_pathhistory"": (""_pathhistory(%(val)s)"", "" or ""),
        ""keyword"": (""keyword(%(val)r)"", "" or ""),
        ""prune"": (""not (%(val)r or ancestors(%(val)r))"", "" and ""),
        ""user"": (""user(%(val)r)"", "" or ""),
    }

    opts = dict(opts)
    # follow or not follow?
    follow = opts.get(""follow"") or opts.get(""follow_first"")
    usepathhistory = _usepathhistory(repo)
    if opts.get(""follow_first""):
        followfirst = 1
    else:
        followfirst = 0
    # --follow with FILE behavior depends on revs...
    it = iter(revs)
    startrev = next(it)
    followdescendants = startrev < next(it, startrev)

    # branch and only_branch are really aliases and must be handled at
    # the same time
    opts[""branch""] = opts.get(""branch"", []) + opts.get(""only_branch"", [])
    opts[""branch""] = [repo.lookupbranch(b) for b in opts[""branch""]]
    # pats/include/exclude are passed to match.match() directly in
    # _matchfiles() revset but walkchangerevs() builds its matcher with
    # scmutil.match(). The difference is input pats are globbed on
    # platforms without shell expansion (windows).
    wctx = repo[None]
    match, pats = scmutil.matchandpats(wctx, pats, opts)
    slowpath = match.anypats() or (
        (match.isexact() or match.prefix())
        and opts.get(""removed"")
        and not usepathhistory
    )
    # pathhistory can deal with directories and removed files.
    if not slowpath and not usepathhistory:
        for f in match.files():
            if follow and f not in wctx:
                # If the file exists, it may be a directory. The ""follow""
                # revset can handle directories fine. So no need to use
                # the slow path.
                if os.path.exists(repo.wjoin(f)):
                    continue
                else:
                    raise error.Abort(
                        _(""cannot follow file not in parent "" 'revision: ""%s""') % f
                    )
            filelog = repo.file(f)
            if not filelog:
                # A zero count may be a directory or deleted file, so
                # try to find matching entries on the slow path.
                if follow:
                    raise error.Abort(_('cannot follow nonexistent file: ""%s""') % f)
                slowpath = True

        # We decided to fall back to the slowpath because at least one
        # of the paths was not a file. Check to see if at least one of them
        # existed in history - in that case, we'll continue down the
        # slowpath; otherwise, we can turn off the slowpath
        if slowpath:
            for path in match.files():
                if path == ""."" or path in repo.store:
                    break
            else:
                slowpath = False

    fpats = (""_patsfollow"", ""_patsfollowfirst"")
    fnopats = ((""_ancestors"", ""_fancestors""), (""_descendants"", ""_fdescendants""))
    if slowpath:
        # See walkchangerevs() slow path.
        #
        # pats/include/exclude cannot be represented as separate
        # revset expressions as their filtering logic applies at file
        # level. For instance ""-I a -X a"" matches a revision touching
        # ""a"" and ""b"" while ""file(a) and not file(b)"" does
        # not. Besides, filesets are evaluated against the working
        # directory.
        matchargs = [""r:"", ""d:relpath""]
        for p in pats:
            matchargs.append(""p:"" + p)
        for p in opts.get(""include"", []):
            matchargs.append(""i:"" + p)
        for p in opts.get(""exclude"", []):
            matchargs.append(""x:"" + p)
        matchargs = "","".join((""%r"" % p) for p in matchargs)
        opts[""_matchfiles""] = matchargs
        if follow:
            opts[fnopats[0][followfirst]] = "".""
    else:
        # pathhistory: force ""follow"" if ""pats"" is given.
        if usepathhistory:
            if pats:
                paths = list(match.files())
                if followfirst:
                    phrevs = ""_firstancestors(rev(%d))"" % startrev
                else:
                    phrevs = ""ancestors(rev(%d))"" % startrev
                phfiles = "","".join(map(repr, paths))
                opts[""_pathhistory""] = ""%s,%s"" % (phrevs, phfiles)
        if follow:
            if pats:
                # pathhistory handled this above
                if not usepathhistory:
                    # follow() revset interprets its file argument as a
                    # manifest entry, so use match.files(), not pats.
                    opts[fpats[followfirst]] = list(match.files())
            else:
                op = fnopats[followdescendants][followfirst]
                opts[op] = ""rev(%d)"" % startrev
        else:
            # avoid using filelog() (_patslog) if pathhistory is used
            if not usepathhistory:
                opts[""_patslog""] = list(pats)

    filematcher = None
    if opts.get(""patch"") or opts.get(""stat""):
        # When following files, track renames via a special matcher.
        # If we're forced to take the slowpath it means we're following
        # at least one pattern/directory, so don't bother with rename tracking.
        #
        # If path history is used, avoid using filelog and linkrev.
        if follow and not match.always() and not slowpath and not usepathhistory:
            # _makefollowlogfilematcher expects its files argument to be
            # relative to the repo root, so use match.files(), not pats.
            filematcher = _makefollowlogfilematcher(
                repo, match.files(), followfirst, repo[startrev]
            )
        else:
            filematcher = _makenofollowlogfilematcher(repo, pats, opts)
            if filematcher is None:
                filematcher = lambda rev: match

    expr = []
    for op, val in sorted(pycompat.iteritems(opts)):
        if not val:
            continue
        if op not in opt2revset:
            continue
        revop, andor = opt2revset[op]
        if ""%(val)"" not in revop:
            expr.append(revop)
        else:
            if not isinstance(val, list):
                e = revop % {""val"": val}
            else:
                e = ""("" + andor.join((revop % {""val"": v}) for v in val) + "")""
            expr.append(e)

    if expr:
        expr = ""("" + "" and "".join(expr) + "")""
        tracing.debug(""log revset: %s\n"" % expr, target=""log::makelogrevset"")
    else:
        expr = None
    return expr, filematcher"
147,"    def type_summary(self):
        """"""
        :return: XCUIElementType summary
        :rtype: str
        """"""",        return self.get_type_value_string(self.type_value)
148,"def PrintCategories():
  """"""Prints a list of all the error-categories used by error messages.

  These are the categories used to filter messages via --filter.
  """"""","  sys.stderr.write(''.join('  %s\n' % cat for cat in _ERROR_CATEGORIES))
  sys.exit(0)"
149,"    def batch_act_simple(
        self,
        observations: List[Dict[Union[str, Module], Message]],
        module: Module,
    ) -> List[Message]:
        """"""
        Return either vanilla batch act or opening batch act.

        :param observations:
            list of observations
        :param module:
            module to perform simple batch act with

        :return batch_act:
            return batch act from appropriate agent module.
        """"""","        batch_agent = self.batch_agents[module]
        if module is Module.OPENING_DIALOGUE:
            batch_act = self.get_opening(observations)
        else:
            batch_act = batch_agent.batch_act([o[module] for o in observations])

        batch_reply_final = [Message({}) for _ in range(len(observations))]
        failed = self.detect_and_handle_failures(module, batch_act, batch_reply_final)

        for i, reply in enumerate(batch_act):
            if i in failed:
                set_failed_reply(reply)
            else:
                reply[module.message_name()] = reply['text']
                reply[f""{module.message_name()}_score""] = reply.get(
                    'logprobs', float('inf')
                )
        batch_reply_final = self.collate_batch_acts(
            batch_reply_mdm=[Message({})] * len(observations),
            batch_reply_sdm=[Message({})] * len(observations),
            batch_reply_mgm_partner=[Message({})] * len(observations),
            batch_reply_mgm_self=[Message({})] * len(observations),
            batch_reply_sgm=[Message({})] * len(observations),
            batch_reply_knowledge=[Message({})] * len(observations),
            batch_reply_dialogue=batch_act,
            available_memory=[o['raw']['memories'] for o in observations],
        )
        return self.detect_and_handle_final_failures(batch_reply_final)"
150,"    def list(self):
        """"""
        List all registered metadata.

        Returns:
            list[str]: keys (names of datasets) of all registered metadata
        """"""",        return list(self.keys())
151,"    def _merge_globals(self, mod, dst):
        # type: (types.ModuleType, Dict[str, Any]) -> None
        """"""Copy the global definitions from one globals dict to another.

        Ignores special attributes and attributes starting with '_', which
        typically denote module-level private attributes.
        """"""","        keys = getattr(mod, ""__all__"", mod.__dict__.keys())

        for key in keys:
            # Block copying modules unless they were specified in '__all__'
            block_copying_module = not hasattr(mod, ""__all__"") and isinstance(
                mod.__dict__[key], types.ModuleType
            )
            if (
                not key.startswith(""_"")
                and key not in _HIDDEN_GLOBALS
                and not block_copying_module
            ):
                dst[key] = mod.__dict__[key]"
152,"    def _is_limited_frame(frame):
        """"""Internal utility to determine if the frame is special or
        limited.""""""","        sal = frame.find_sal()

        if (not sal.symtab or not sal.symtab.filename
            or frame.type() == gdb.DUMMY_FRAME
            or frame.type() == gdb.SIGTRAMP_FRAME):

            return True

        return False"
153,"def closable_named_temporary_file():
    """"""
    Due to a bug in python (https://bugs.python.org/issue14243), we need to be able to close() the
    temporary file without deleting it.
    """"""","    fp = tempfile.NamedTemporaryFile(delete=False)
    try:
        with fp:
            yield fp
    finally:
        os.remove(fp.name)"
154,"    def formatdate(self, date, fmt=""%a %b %d %H:%M:%S %Y %1%2""):
        """"""convert date tuple to appropriate format""""""","        return self._converter.formatdate(date, fmt)"
155,"    def __init__(self, matchee, expected):
        """"""Create a DoesNotEndWith Mismatch.

        :param matchee: the string that did not match.
        :param expected: the string that 'matchee' was expected to end with.
        """"""","        self.matchee = matchee
        self.expected = expected"
156,"    def _checklastmasterhead(self, fctx):
        """"""check if fctx is the master's head last time, raise if not""""""","        if fctx is None:
            llrev = 0
        else:
            llrev = self.revmap.hsh2rev(fctx.node())
            if not llrev:
                raise faerror.CannotReuseError()
        if self.linelog.maxrev != llrev:
            raise faerror.CannotReuseError()"
157,"    def findmissing(self, common=None, heads=None):
        """"""Return the ancestors of heads that are not ancestors of common.

        More specifically, return a list of nodes N such that every N
        satisfies the following constraints:

          1. N is an ancestor of some node in 'heads'
          2. N is not an ancestor of any node in 'common'

        The list is sorted by revision number, meaning it is
        topologically sorted.

        'heads' and 'common' are both lists of node IDs.  If heads is
        not supplied, uses all of the revlog's heads.  If common is not
        supplied, uses nullid.""""""","        if common is None:
            common = [nullid]
        if heads is None:
            heads = self.heads()

        common = [self.rev(n) for n in common]
        heads = [self.rev(n) for n in heads]

        inc = self.incrementalmissingrevs(common=common)
        return [self.node(r) for r in inc.missingancestors(heads)]"
158,"    def parse_seasonality_args(self, name, arg, auto_disable, default_order):
        """"""Get number of fourier components for built-in seasonalities.

        Parameters
        ----------
        name: string name of the seasonality component.
        arg: 'auto', True, False, or number of fourier components as provided.
        auto_disable: bool if seasonality should be disabled when 'auto'.
        default_order: int default fourier order

        Returns
        -------
        Number of fourier components, or 0 for disabled.
        """"""","        if arg == 'auto':
            fourier_order = 0
            if name in self.seasonalities:
                logger.info(
                    'Found custom seasonality named {name!r}, disabling '
                    'built-in {name!r} seasonality.'.format(name=name)
                )
            elif auto_disable:
                logger.info(
                    'Disabling {name} seasonality. Run prophet with '
                    '{name}_seasonality=True to override this.'
                    .format(name=name)
                )
            else:
                fourier_order = default_order
        elif arg is True:
            fourier_order = default_order
        elif arg is False:
            fourier_order = 0
        else:
            fourier_order = int(arg)
        return fourier_order"
159,"def _phasemove(orig, pushop, nodes, phase=phases.public) -> None:
    """"""prevent commits from being marked public

    Since these are going to a scratch branch, they aren't really being
    published.""""""","
    if phase != phases.public:
        orig(pushop, nodes, phase)"
160,"def _VerboseLevel():
  """"""Returns the module's verbosity setting.""""""",  return _cpplint_state.verbose_level
161,"def padded_tensor(
    items: List[Union[List[int], torch.LongTensor]],
    pad_idx: int = 0,
    left_padded: bool = False,
    max_len: Optional[int] = None,
    fp16friendly: bool = False,
) -> Tuple[torch.LongTensor, List[int]]:
    """"""
    Create a padded matrix from an uneven list of lists.

    Returns (padded, lengths), where padded is the padded matrix, and lengths
    is a list containing the lengths of each row.

    Matrix is right-padded (filled to the right) by default, but can be
    left padded if the flag is set to True.

    Matrix can also be placed on cuda automatically.

    :param list[iter[int]] items: List of items
    :param bool sort: If True, orders by the length
    :param int pad_idx: the value to use for padding
    :param bool left_padded:
    :param int max_len: if None, the max length is the maximum item length
    :param bool fp16friendly: if True, pads the time dimension to be a multiple of 4.

    :returns: (padded, lengths) tuple
    :rtype: (Tensor[int64], list[int])
    """"""","
    # number of items
    n = len(items)
    # length of each item
    lens: List[int] = [len(item) for item in items]  # type: ignore
    # max in time dimension
    t = max(lens) if max_len is None else max_len

    # if input tensors are empty, we should expand to nulls
    t = max(t, 1)

    if fp16friendly and (t % FP16_PAD_SIZE != 0):
        # pad to be fp16 friendly
        t += FP16_PAD_SIZE - (t % FP16_PAD_SIZE)

    if isinstance(items[0], torch.Tensor):
        # keep type of input tensors, they may already be cuda ones
        output = items[0].new(n, t)  # type: ignore
    else:
        output = torch.LongTensor(n, t)  # type: ignore
    output.fill_(pad_idx)

    for i, (item, length) in enumerate(zip(items, lens)):
        if length == 0:
            # skip empty items
            continue
        if not isinstance(item, torch.Tensor):
            # put non-tensors into a tensor
            item = torch.LongTensor(item)  # type: ignore
        if left_padded:
            # place at end
            output[i, t - length :] = item
        else:
            # place at beginning
            output[i, :length] = item

    return output, lens"
162,"    def group(
        self,
        nodelist: ""Sequence[bytes]"",
        revlog: ""Any"",
        lookup: ""Callable[..., Any]"",
        prog: ""Optional[Any]"" = None,
    ) -> ""Iterable[bytes]"":
        """"""Calculate a delta group, yielding a sequence of changegroup chunks
        (strings).

        Given a list of changeset revs, return a set of deltas and
        metadata corresponding to nodes. The first delta is
        first parent(nodelist[0]) -> nodelist[0], the receiver is
        guaranteed to have this parent as it has all history before
        these changesets. In the case firstparent is nullrev the
        changegroup starts with a full revision.

        If prog is not None, its value attribute will be updated with progress.
        """"""","        # if we don't have any revisions touched by these changesets, bail
        if len(nodelist) == 0:
            yield self.close()
            return

        revs = self._sortgroup(revlog, nodelist, lookup)

        # add the parent of the first rev
        p = revlog.parentrevs(revs[0])[0]
        revs.insert(0, p)

        # build deltas
        if prog is not None:
            prog._total = len(revs) - 1
        for r in range(len(revs) - 1):
            if prog is not None:
                prog.value = r + 1
            prev, curr = revs[r], revs[r + 1]
            linknode = lookup(revlog.node(curr))
            if self._cgdeltaconfig == CFG_CGDELTA_ALWAYS_NULL:
                prev = nullrev
            elif self._cgdeltaconfig == CFG_CGDELTA_NO_EXTERNAL and r == 0:
                prev = nullrev
            for c in self.revchunk(revlog, curr, prev, linknode):
                yield c

        yield self.close()"
163,"    def __getitem__(self, parameters):
        """"""A thin wrapper around __getitem_inner__ to provide the latter
        with hashable arguments to improve speed.
        """"""","
        if  self.__origin__ is not None or not _geqv(self, Callable):
            return super().__getitem__(parameters)
        if not isinstance(parameters, tuple) or len(parameters) != 2:
            raise TypeError(""Callable must be used as ""
                            ""Callable[[arg, ...], result]."")
        args, result = parameters
        if args is Ellipsis:
            parameters = (Ellipsis, result)
        else:
            if not isinstance(args, list):
                raise TypeError(""Callable[args, result]: args must be a list.""
                                "" Got %.100r."" % (args,))
            parameters = (tuple(args), result)
        return self.__getitem_inner__(parameters)"
164,"    def _set_text_vec(self, *args, **kwargs):
        """"""
        Add the start and end token to the text.
        """"""","        obs = super()._set_text_vec(*args, **kwargs)

        if 'text_vec' in obs and 'added_start_end' not in obs:
            obs.force_set(
                'text_vec', self._add_start_end_tokens(obs['text_vec'], True, True)
            )
            obs['added_start_end'] = True

        # check truncation after adding start end tokens
        if obs.get('text_vec') is not None:
            truncated_vec = self._check_truncate(
                obs['text_vec'], self.text_truncate, True
            )
            obs.force_set('text_vec', torch.LongTensor(truncated_vec))

        return obs"
165,"    def _worker(self):
        """"""Main routine for worker thread.""""""","        while True:
            try:
                fh = self._queue.get(block=True, timeout=0.100)
                # Need to catch or the thread will terminate and
                # we could orphan file descriptors.
                try:
                    fh.close()
                except Exception as e:
                    # Stash so can re-raise from main thread later.
                    self._threadexception = e
            except util.empty:
                if not self._running:
                    break"
166,"def add_ResNet_roi_conv5_head_for_masks(model, blob_in, dim_in, spatial_scale):
    """"""Add a ResNet ""conv5"" / ""stage5"" head for predicting masks.""""""","    model.RoIFeatureTransform(
        blob_in,
        blob_out='_[mask]_pool5',
        blob_rois='mask_rois',
        method=cfg.MRCNN.ROI_XFORM_METHOD,
        resolution=cfg.MRCNN.ROI_XFORM_RESOLUTION,
        sampling_ratio=cfg.MRCNN.ROI_XFORM_SAMPLING_RATIO,
        spatial_scale=spatial_scale
    )

    dilation = cfg.MRCNN.DILATION
    stride_init = int(cfg.MRCNN.ROI_XFORM_RESOLUTION / 7)  # by default: 2

    s, dim_in = ResNet.add_stage(
        model,
        '_[mask]_res5',
        '_[mask]_pool5',
        3,
        dim_in,
        2048,
        512,
        dilation,
        stride_init=stride_init
    )

    return s, 2048"
167,"    def _one_bit_flipped(cls, a, b):
        """"""Return true if a and b differ at at most one bit position""""""","        diff = a ^ b  # Bits set to 1 where a and b differ
        # If diff has only one `1` bit, subtracting one will clear that bit
        # Otherwise, the most significant 1 will not be cleared
        return 0 == (diff & (diff - 1))"
168,"    def _connect(self):
        """"""establish transport connection""""""","
        if self.recvConn:
            if self.pid != os.getpid():
                raise UseAfterFork(
                    ""do not re-use a connection after fork; open a new client instead""
                )
            return

        if self.sockpath is None:
            self.sockpath = self._resolvesockname()

        kwargs = {}
        if self.transport == CLIProcessTransport:
            kwargs[""binpath""] = self.binpath

        self.tport = self.transport(self.sockpath, self.timeout, **kwargs)
        self.sendConn = self.sendCodec(self.tport)
        self.recvConn = self.recvCodec(self.tport)
        self.pid = os.getpid()"
169,"    def _time(self):
        """"""Private-ish time to help with mocking/unittests""""""",        return time.time()
170,"def RemoveMultiLineCommentsFromRange(lines, begin, end):
  """"""Clears a range of lines for multi-line comments.""""""","  # Having // dummy comments makes the lines non-empty, so we will not get
  # unnecessary blank line warnings later in the code.
  for i in range(begin, end):
    lines[i] = '// dummy'"
171,"    def params(self):
        """"""dictionary of stream level parameters""""""","        indebug(self.ui, ""reading bundle2 stream parameters"")
        params = {}
        paramssize = self._unpack(_fstreamparamsize)[0]
        if paramssize < 0:
            raise error.BundleValueError(""negative bundle param size: %i"" % paramssize)
        if paramssize:
            params = self._readexact(paramssize)
            params = self._processallparams(params)
        return params"
172,"    def frame(self):
        """"""
        :return: XCUIElement frame
        :rtype: lldb.SBValue
        """"""","        if self._frame is None:
            import_uikit()
            name = ""_frame""
            if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:
                self._frame = fb.evaluateExpressionValue(
                    ""(CGRect)[{} frame]"".format(self.element_value)
                )
            else:
                self._frame = self.element.GetChildMemberWithName(name)
        return self._frame"
173,"    def generate_report(self):
        # type: () -> str
        """"""Generate string with a nice visualization of the result of the profiling.""""""","        Profiler._recursive_mark_useful_leaf(self._aggregated_callstack)

        content = [""# Highlights\n""]
        highlights = []
        if self._reverse_callstack:
            for node in self._aggregated_callstack.children.values():
                if node.duration > HIGHLIGHTS_THRESHOLD_DELAY:
                    highlights.append(node)
        else:
            Profiler._recursive_collect_highlights_report(
                self._aggregated_callstack, highlights
            )
        highlights = Profiler._items_sorted_by_duration(highlights)

        content += Profiler._generate_highlights_report(highlights)

        content += [""\n"", ""# More details\n""]
        content += self._generate_callstack_report(highlights)
        return """".join(content)"
174,"def templatepartsmap(spec, t, partnames):
    """"""Create a mapping of {part: ref}""""""","    partsmap = {spec.ref: spec.ref}  # initial ref must exist in t
    if spec.mapfile:
        partsmap.update((p, p) for p in partnames if p in t)
    elif spec.ref:
        for part in partnames:
            ref = ""%s:%s"" % (spec.ref, part)  # select config sub-section
            if ref in t:
                partsmap[part] = ref
    return partsmap"
175,"def CheckForBadCharacters(filename, lines, error):
  """"""Logs an error for each line containing bad characters.

  Two kinds of bad characters:

  1. Unicode replacement characters: These indicate that either the file
  contained invalid UTF-8 (likely) or Unicode replacement characters (which
  it shouldn't).  Note that it's possible for this to throw off line
  numbering if the invalid UTF-8 occurred adjacent to a newline.

  2. NUL bytes.  These are problematic for some tools.

  Args:
    filename: The name of the current file.
    lines: An array of strings, each representing a line of the file.
    error: The function to call with any errors found.
  """"""","  for linenum, line in enumerate(lines):
    if u'\ufffd' in line:
      error(filename, linenum, 'readability/utf8', 5,
            'Line contains invalid UTF-8 (or Unicode replacement character).')
    if '\0' in line:
      error(filename, linenum, 'readability/nul', 5, 'Line contains NUL byte.')"
176,"def _normalize_persona_line(x: str) -> str:
    """"""
    Normalize a persona line.
    """"""","    if x.startswith('your persona: '):
        # Normalize the sentence appearing after 'your persona:'
        x = x[len('your persona: ') :]
        x = normalize_reply(x)
        x = 'your persona: ' + x
    elif x.startswith(""partner's persona: ""):
        x = x[len(""partner's persona: "") :]
        x = normalize_reply(x)
        x = ""partner's persona: "" + x
    return x"
177,"    def nonnormalentries(self):
        """"""Returns a set of filenames.""""""","        # type() -> Tuple[Set[str], Set[str]]
        nonnorm = set()
        otherparent = set()
        for path, entry in iteritems(self._map):
            if entry[0] != ""n"":
                nonnorm.add(path)
            elif entry[2] == MERGE_STATE_OTHER_PARENT:
                otherparent.add(path)
        return nonnorm, otherparent"
178,"    def revs(self, start=0, stop=None):
        """"""filtered version of revlog.revs""""""","        allrevs = self.torevs(self.dag.all())
        if stop is not None:
            # exclusive -> inclusive
            stop = stop - 1
        revs = bindings.dag.spans.unsaferange(start, stop) & allrevs
        for i in revs.iterasc():
            yield i"
179,"def binaryencode(bookmarks: typing.Iterable[typing.Tuple[str, bytes]]) -> bytes:
    """"""encode a '(bookmark, node)' iterable into a binary stream

    the binary format is:

        <node><bookmark-length><bookmark-name>

    :node: is a 20 bytes binary node,
    :bookmark-length: an unsigned short,
    :bookmark-name: the name of the bookmark (of length <bookmark-length>)

    wdirid (all bits set) will be used as a special value for ""missing""
    """"""","    binarydata = []
    for book, node in bookmarks:
        if not node:  # None or ''
            node = wdirid
        book = pycompat.encodeutf8(book)
        binarydata.append(_binaryentry.pack(node, len(book)))
        binarydata.append(book)
    return b"""".join(binarydata)"
180,"    def report(self):
        """"""
        Report all metrics of all subagents.
        """"""","        from parlai.core.metrics import Metric, LegacyMetric

        metrics = {}
        for a in self.agents:
            if hasattr(a, 'report'):
                m = a.report()
                for k, v in m.items():
                    if not isinstance(v, Metric):
                        v = LegacyMetric(v)
                    if k not in metrics:
                        # first agent gets priority in settings values for keys
                        # this way model can't e.g. override accuracy to 100%
                        metrics[k] = v
        if metrics and 'exs' in metrics:
            self.total_exs += metrics['exs'].value()
        return metrics"
181,"def absorbcmd(ui, repo, *pats, **opts):
    """"""intelligently integrate pending changes into current stack

    Attempt to amend each pending change to the proper commit in your
    stack. Absorb does not write to the working copy.

    If absorb cannot find an unambiguous commit to amend for a change, that
    change will be left in the working copy, untouched. The unabsorbed
    changes can be observed by :prog:`status` or :prog:`diff` afterwards.

    Commits outside the revset `::. and not public() and not merge()` will
    not be changed.

    Commits that become empty after applying the changes will be deleted.

    By default, absorb will show what it plans to do and prompt for
    confirmation.  If you are confident that the changes will be absorbed
    to the correct place, run :prog:`absorb -a` to apply the changes
    immediately.

    Returns 0 if anything was absorbed, 1 if nothing was absorbed.
    """"""","    state = absorb(ui, repo, pats=pats, opts=opts)
    if sum(s[0] for s in state.chunkstats.values()) == 0:
        return 1"
182,"    def is_socket(self):
        """"""
        Whether this path is a socket.
        """"""","        try:
            return S_ISSOCK(self.stat().st_mode)
        except OSError as e:
            if e.errno != ENOENT:
                raise
            # Path doesn't exist or is a broken symlink
            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)
            return False"
183,"    def initialize_scales(self, initialize_scales, df):
        """"""Initialize model scales.

        Sets model scaling factors using df.

        Parameters
        ----------
        initialize_scales: Boolean set the scales or not.
        df: pd.DataFrame for setting scales.
        """"""","        if not initialize_scales:
            return
        if self.growth == 'logistic' and 'floor' in df:
            self.logistic_floor = True
            floor = df['floor']
        else:
            floor = 0.
        self.y_scale = float((df['y'] - floor).abs().max())
        if self.y_scale == 0:
            self.y_scale = 1.0
        self.start = df['ds'].min()
        self.t_scale = df['ds'].max() - self.start
        for name, props in self.extra_regressors.items():
            standardize = props['standardize']
            n_vals = len(df[name].unique())
            if n_vals < 2:
                standardize = False
            if standardize == 'auto':
                if set(df[name].unique()) == {1, 0}:
                    standardize = False #  Don't standardize binary variables.
                else:
                    standardize = True
            if standardize:
                mu = df[name].mean()
                std = df[name].std()
                self.extra_regressors[name]['mu'] = mu
                self.extra_regressors[name]['std'] = std"
184,"def convert_to_dialogData(
    premise_raw, hypo_raw, answer_raw, dialog_format=False, binary_classes=False
):
    """"""
    Convert from NLI context to dialog text.

    :param premise_raw: raw premise extracted from jsonl file.
    :param hypo_raw: raw hypothesis extracted from jsonl file.
    :param answer_raw: raw answer extracted from jsonl file.
    :param dialog_format: if set True, omit the special tokens 'Hypothesis' and 'Premise' in the text.
    :param binary_classes: if set True, bucketize (neutral, entailment) into one (no_contradiction)
    :return: a tuple (question, answer, clas)
        - ``question`` (str) is a query and possibly context
        - ``answers`` (iter) is an iterable of label(s) for that query
        - ``clas`` (iter) is an iterable of label candidates that the student can choose from
    """"""","    premise_raw = premise_raw.strip('\n').strip('\t')
    hypo_raw = hypo_raw.strip('\n').strip('\t')
    clas = MULTINLI_LABELS

    if binary_classes:
        answer_raw = BICLASS_DICT[answer_raw]
        clas = BICLASS_LABELS
    if not dialog_format:
        premise_raw = MULTINLI_PREMISE_PREFIX + premise_raw
        hypo_raw = MULTINLI_HYPO_PREFIX + hypo_raw

    question = premise_raw + '\n' + hypo_raw
    answers = [answer_raw]

    return question, answers, clas"
185,"def unescape(st):
    """"""
    Unescapes the characters ``\\\\``, ``\\t``, ``\\n``, ``\\r`` and ``""`` in
    the given string ``st`` and returns it.
    """"""","
    def unescape_repl(m):
        m = m.group(1)
        if m == ""n"":
            return ""\n""
        if m == ""t"":
            return ""\t""
        if m == ""r"":
            return ""\r""
        if m == ""\\"":
            return ""\\""
        return m  # handles escaped double quote

    return re.sub(r'\\(\\|n|t|r|"")', unescape_repl, st)"
186,"    def close(self) -> None:
        """"""
        Called on `f.close()` or automatically by the context manager.
        We add the `close` call to the file's queue to make sure that
        the file is not closed before all of the write jobs are complete.
        """"""","        self.flush()
        # Close the last buffer created by `flush`.
        self._notify_manager(lambda: self._buffers[-1].close())
        # `ThreadPool` first closes the file and then executes the callback.
        self._notify_manager(lambda: self._io.close())
        if not self._close_called and self._callback_after_file_close:
            # pyre-fixme[6]: For 1st param expected `() -> None` but got `(None) ->
            #  None`.
            self._notify_manager(self._callback_after_file_close)
        self._close_called = True"
187,"    def index_data(self, tensors: List[torch.Tensor]):
        """"""
        Index data.

        The HNSW Flat Indexer computes an auxiliary dimension that converts inner product
        similarity to L2 distance similarity.

        :param data:
            List of torch.Tensor
        """"""","        data = torch.cat(tensors).float()
        n = data.size(0)
        # max norm is required before putting all vectors in the index to convert inner product similarity to L2
        if self.built:
            raise RuntimeError(
                'HNSW index needs to index all data at once, results will be unpredictable otherwise.'
            )
        phi = 0
        norms = (data**2).sum(dim=1)
        max_norms = norms.max().item()
        phi = max(phi, max_norms)
        logging.info(f'HNSWF DotProduct -> L2 space phi={phi}')
        start = time.time()

        for i in range(0, n, self.buffer_size):
            vectors_i = data[i : i + self.buffer_size]
            norms_i = norms[i : i + self.buffer_size]
            aux_dims = torch.sqrt(phi - norms_i)
            hnsw_vectors = torch.cat([vectors_i, aux_dims.unsqueeze(1)], dim=1)
            self.index.add(hnsw_vectors.numpy())
            logging.info(
                f'{time.time() - start}s Elapsed: data indexed {i + len(vectors_i)}'
            )

        logging.info(f'Total data indexed {n}')"
188,"    def put_sint32(self, value):
        """"""Encode a int32_t into the file at the current file position""""""","        self.file.write(struct.pack(self.byte_order + ""i"", value))"
189,"    def bundle2rebase(op, part):
        """"""unbundle a bundle2 containing a changegroup to rebase""""""","
        params = part.params

        bundlefile = None
        bundle = None
        markerdate = util.makedate()
        ui = op.repo.ui

        # Patch ctx._fileinfo so it can look into treemanifests. This covers more
        # code paths (ex. fctx.renamed -> _copied -> ctx.filenode -> ctx._fileinfo
        # -> ""repo.manifestlog[self._changeset.manifest].find(path)"")
        def _fileinfo(orig, self, path):
            try:
                return orig(self, path)
            except LookupError:
                # Try look up again
                mf = _getmanifest(op, self)
                try:
                    return mf.find(path)
                except KeyError:
                    raise error.ManifestLookupError(
                        self._node, path, _(""not found in manifest"")
                    )

        with extensions.wrappedfunction(context.basectx, ""_fileinfo"", _fileinfo):
            ontoparam = params.get(""onto"", donotrebasemarker)
            try:  # guards bundlefile
                cgversion = params.get(""cgversion"", ""01"")
                bundlefile = _makebundlefile(op, part, cgversion)
                bundlepath = ""bundle:%s+%s"" % (op.repo.root, bundlefile)
                bundle = _createbundlerepo(op, bundlepath)

                ontoctx = resolveonto(op.repo, ontoparam)

                prepushrebasehooks(op, params, bundle, bundlefile)

                ui.setconfig(""pushrebase"", pushrebasemarker, True)
                verbose = ontoctx is not None and ui.configbool(""pushrebase"", ""verbose"")
                usestackpush = ontoctx is not None and ui.configbool(
                    ""pushrebase"", ""trystackpush"", True
                )

                def log(msg, force=False):
                    if verbose or force:
                        ui.write_err(msg)
                    ui.log(""pushrebase"", msg)

                if usestackpush:
                    try:
                        pushrequest = stackpush.pushrequest.fromrevset(
                            bundle, ""bundle()""
                        )
                    except StackPushUnsupportedError as ex:
                        # stackpush is unsupported. Fallback to old code path.
                        if verbose:
                            ui.write_err(_(""not using stackpush: %s\n"") % ex)

                        usestackpush = False
                if usestackpush:
                    # This can happen in the following (rare) case:
                    #
                    # Client:         Server:
                    #
                    #  C
                    #  |
                    #  B               B
                    #  |               |
                    #  A               A master
                    #
                    # Client runs ""push -r C --to master"". ""bundle()"" only contains
                    # ""C"". The non-stackpush code path would fast-forward master to
                    # ""C"". The stackpush code path will try rebasing ""C"" to ""A"".
                    # Prevent that. An alternative fix is to pass ""::bundle() % onto""
                    # to pushrequest.fromrevset. But that's more expensive and adds
                    # other complexities.
                    if (
                        ontoctx.node() != pushrequest.stackparentnode
                        and op.repo.changelog.isancestor(
                            ontoctx.node(), pushrequest.stackparentnode
                        )
                    ):
                        if verbose:
                            ui.write_err(
                                _(""not using stackpush: not rebasing backwards\n"")
                            )
                        usestackpush = False

                if usestackpush:
                    # stackpush code path - use ""pushrequest"" instead of ""bundlerepo""

                    # Check conflicts before entering the critical section. This is
                    # optional since there is another check inside the critical
                    # section.
                    log(_(""checking conflicts with %s\n"") % (ontoctx,))

                    pushrequest.check(ontoctx)

                    # Print and log what commits to push.
                    log(
                        getpushmessage(
                            pushrequest.pushcommits,
                            lambda c: ""%s  %s""
                            % (short(c.orignode), c.desc.split(""\n"", 1)[0][:50]),
                        ),
                        force=True,
                    )

                    # Enter the critical section! This triggers a hgsql sync.
                    tr = op.gettransaction()
                    hookargs = dict(tr.hookargs)
                    op.repo.hook(""prechangegroup"", throw=True, **hookargs)

                    # ontoctx could move. Fetch the new one.
                    # Print rebase source and destination.
                    ontoctx = resolveonto(op.repo, ontoparam)
                    log(
                        _(""rebasing stack from %s onto %s\n"")
                        % (short(pushrequest.stackparentnode), ontoctx)
                    )
                    added, replacements = pushrequest.pushonto(
                        ontoctx, getcommitdatefn=common.commitdategenerator(op)
                    )
                else:
                    # Old code path - use a bundlerepo

                    # Create a cache of rename sources while we don't have the lock.
                    renamesrccache = {
                        bundle[r].node(): _getrenamesrcs(op, bundle[r])
                        for r in bundle.revs(""bundle()"")
                    }

                    # Opening the transaction takes the lock, so do it after prepushrebase
                    # and after we've fetched all the cache information we'll need.
                    tr = op.gettransaction()
                    hookargs = dict(tr.hookargs)

                    # Recreate the bundle repo, since taking the lock in gettransaction()
                    # may have caused it to become out of date.
                    # (but grab a copy of the cache first)
                    bundle.close()
                    bundle = _createbundlerepo(op, bundlepath)

                    onto = getontotarget(op, params, bundle)

                    revs, oldonto = _getrevs(op, bundle, onto, renamesrccache)

                    op.repo.hook(""prechangegroup"", throw=True, **hookargs)

                    log(
                        getpushmessage(
                            revs,
                            lambda r: ""%s  %s""
                            % (r, bundle[r].description().split(""\n"", 1)[0][:50]),
                        ),
                        force=True,
                    )

                    # Prepopulate the revlog _cache with the original onto's fulltext. This
                    # means reading the new onto's manifest will likely have a much shorter
                    # delta chain to traverse.
                    log(_(""rebasing onto %s\n"") % (short(onto.node()),))

                    # Perform the rebase + commit to the main repo
                    added, replacements = runrebase(op, revs, oldonto, onto)

                    # revs is modified by runrebase to ensure garbage collection of
                    # manifests, so don't use it from here on.
                    revs = None

                op.repo.pushrebaseaddedchangesets = added
                op.repo.pushrebasereplacements = replacements

                markers = _buildobsolete(replacements, bundle, op.repo, markerdate)
            finally:
                try:
                    if bundlefile:
                        os.unlink(bundlefile)
                except OSError as e:
                    if e.errno != errno.ENOENT:
                        raise
                if bundle:
                    bundle.close()

        # Move public phase forward
        publishing = op.repo.ui.configbool(""phases"", ""publish"")
        if publishing:
            phasesmod.advanceboundary(op.repo, tr, phasesmod.public, [added[-1]])

        addfinalhooks(op, tr, hookargs, added)

        # Send new commits back to the client
        clientobsmarkerversions = [
            int(v) for v in params.get(""obsmarkerversions"", """").split(""\0"") if v
        ]
        _addpushbackparts(
            op, replacements, markers, markerdate, clientobsmarkerversions
        )

        for k in list(replacements.keys()):
            replacements[hex(k)] = hex(replacements[k])
        op.records.add(rebaseparttype, replacements)

        return 1"
190,"        def set_up_done(exception_caught):
            """"""Set up is done, either clean up or run the test.""""""","            if self.exception_caught == exception_caught:
                fails.append(None)
                return clean_up()
            else:
                d = self._run_user(self.case._run_test_method, self.result)
                d.addCallback(fail_if_exception_caught)
                d.addBoth(tear_down)
                return d"
191,"def _configure_libraries():
    """"""
    Configurations for some libraries.
    """"""","    # An environment option to disable `import cv2` globally,
    # in case it leads to negative performance impact
    disable_cv2 = int(os.environ.get(""DETECTRON2_DISABLE_CV2"", False))
    if disable_cv2:
        sys.modules[""cv2""] = None
    else:
        # Disable opencl in opencv since its interaction with cuda often has negative effects
        # This envvar is supported after OpenCV 3.4.0
        os.environ[""OPENCV_OPENCL_RUNTIME""] = ""disabled""
        try:
            import cv2

            if int(cv2.__version__.split(""."")[0]) >= 3:
                cv2.ocl.setUseOpenCL(False)
        except ModuleNotFoundError:
            # Other types of ImportError, if happened, should not be ignored.
            # Because a failed opencv import could mess up address space
            # https://github.com/skvark/opencv-python/issues/381
            pass

    def get_version(module, digit=2):
        return tuple(map(int, module.__version__.split(""."")[:digit]))

    # fmt: off
    assert get_version(torch) >= (1, 4), ""Requires torch>=1.4""
    import fvcore
    assert get_version(fvcore, 3) >= (0, 1, 2), ""Requires fvcore>=0.1.2""
    import yaml
    assert get_version(yaml) >= (5, 1), ""Requires pyyaml>=5.1"""
192,"    def _compile_convos_and_reasons(self) -> str:
        """"""
        Create a human-readable string of all pairs of conversations, as well as which
        conversation each Turker chose and their reason for choosing it.
        """"""","
        pairing_outputs = []

        for _, pairing_sr in self.dataframe.iterrows():
            winning_dialogue = self._dialogue_to_string(
                pairing_sr['winner_dialogue']['dialogue']
            )
            loser_dialogue = self._dialogue_to_string(
                pairing_sr['loser_dialogue']['dialogue']
            )
            pairing_output = f""""""CONVO PAIR ID: {pairing_sr['pairing_id']}

WINNING DIALOGUE: {pairing_sr['winner']}
{winning_dialogue}

LOSING DIALOGUE: {pairing_sr['loser']}
{loser_dialogue}

QUESTION: {pairing_sr['question']}
TURKER'S CHOICE: {pairing_sr['winner']}
REASON: {pairing_sr['reason']}



""""""
            pairing_outputs.append(pairing_output)

        return ''.join(pairing_outputs)"
193,"    def _build(cls, **kwargs):
        """"""Build the shape with the parameters.

        Returns either
            shape_or_list_shapes
        or tuple
            (shape_or_list_shapes, phantom_vertices).
        """"""",        pass
194,"    def _get_local_path(self, path: str, force: bool = False, **kwargs: Any) -> str:
        """"""
        Get a filepath which is compatible with native Python I/O such as `open`
        and `os.path`.

        If URI points to a remote resource, this function may download and cache
        the resource to local disk. In this case, the cache stays on filesystem
        (under `file_io.get_cache_dir()`) and will be used by a different run.
        Therefore this function is meant to be used with read-only resources.

        Args:
            path (str): A URI supported by this PathHandler
            force(bool): Forces a download from backend if set to True.

        Returns:
            local_path (str): a file path which exists on the local file system
        """"""",        raise NotImplementedError()
195,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Command line args for OPT BB3.
        """"""","        cls.add_additional_subagent_args(parser)
        parser = R2C2Agent.add_cmdline_args(parser, partial_opt)
        group = parser.add_argument_group('OPT Agent Group')
        group.add_argument(
            '--opt-server',
            type=str,
            help='which server to connect to for hosted OPT model',
            default=None,
        )
        group.add_argument(
            '--raw-search-server',
            type=str,
            help='specify a search server address.',
            default=None,
        )
        group.add_argument(
            '--num-shots',
            type=int,
            default=None,
            help='how many k-shot examples to put in the prompt. Default <0 is all',
        )
        group.add_argument(
            '--include-prompt',
            type='bool',
            default=None,
            help='Whether to include prompt',
        )
        group.add_argument(
            '--all-vanilla-prompt',
            type='bool',
            default=None,
            help='If True, and --include-prompt, then all prompts are just the vanilla, ""a conversation"", prompts.',
        )
        group.add_argument(
            '--knowledge-chunk-size',
            type=int,
            default=100,
            help='Chunk size, in words, of knowledge to keep',
        )
        group.add_argument('--debug-bb3', type='bool', default=False, dest='debug_bb3')
        group.add_argument(
            '--max-prompt-len',
            type=int,
            default=PROMPT.MAX_PROMPT_LEN,
            help='Longest sequence to send to API',
        )
        parser.add_argument(
            '--metaseq-max-retry-api',
            default=-1,
            type=int,
            help='Number of times to retry on API request failures (< 0 for unlimited retry).',
        )
        parser.add_argument('--metaseq-server-timeout', default=20.0, type=float)
        return parser"
196,"def GetHeaderGuardCPPVariable(filename):
  """"""Returns the CPP variable that should be used as a header guard.

  Args:
    filename: The name of a C++ header file.

  Returns:
    The CPP variable that should be used as a header guard in the
    named file.

  """"""","
  # Restores original filename in case that cpplint is invoked from Emacs's
  # flymake.
  filename = re.sub(r'_flymake\.h$', '.h', filename)
  filename = re.sub(r'/\.flymake/([^/]*)$', r'/\1', filename)

  fileinfo = FileInfo(filename)
  file_path_from_root = fileinfo.RepositoryName()
  if _root:
    file_path_from_root = re.sub('^' + _root + os.sep, '', file_path_from_root)
  return re.sub(r'[-./\s]', '_', file_path_from_root).upper() + '_'"
197,"    def _copied(self):
        """"""check if file was actually renamed in this changeset revision

        If rename logged in file revision, we report copy for changeset only
        if file revisions linkrev points back to the changeset in question
        or both changeset parents contain different file revisions.
        """"""","
        renamed = self._filelog.renamed(self._filenode)
        if not renamed:
            return renamed

        if self.rev() == self.linkrev():
            return renamed

        name = self.path()
        fnode = self._filenode
        for p in self._changectx.parents():
            try:
                if fnode == p.filenode(name):
                    return None
            except error.LookupError:
                pass
        return renamed"
198,"    def serialize_texception(cls, fname, seqid, exception):
        """"""This saves us a bit of processing time for timeout handling by
        reusing the Thrift structs involved in exception serialization.

        NOTE: this is not thread-safe nor it is meant to be.
        """"""","        # the serializer is a singleton
        if cls._exception_serializer is None:
            buffer = TWriteOnlyBuffer()
            transport = THeaderTransport(buffer)
            cls._exception_serializer = THeaderProtocol(transport)
        else:
            transport = cls._exception_serializer.trans
            buffer = transport.getTransport()
            buffer.reset()

        serializer = cls._exception_serializer
        serializer.writeMessageBegin(fname, TMessageType.EXCEPTION, seqid)
        exception.write(serializer)
        serializer.writeMessageEnd()
        serializer.trans.flush()
        return buffer.getvalue()"
199,"def applychanges(ui, repo, ctx, opts):
    """"""Merge changeset from ctx (only) in the current working directory""""""","    with repo.wlock(), repo.lock(), repo.transaction(""histedit""):
        wcpar = repo.dirstate.p1()
        if ctx.p1().node() == wcpar:
            # edits are ""in place"" we do not need to make any merge,
            # just applies changes on parent for editing
            cmdutil.revert(ui, repo, ctx, (wcpar, node.nullid), all=True)
            stats = None
        else:
            try:
                # ui.forcemerge is an internal variable, do not document
                repo.ui.setconfig(""ui"", ""forcemerge"", opts.get(""tool"", """"), ""histedit"")
                stats = mergemod.graft(repo, ctx, ctx.p1(), [""local"", ""histedit""])
            finally:
                repo.ui.setconfig(""ui"", ""forcemerge"", """", ""histedit"")
    return stats"
200,"    def get_openers(self, episode_num: int) -> Optional[List[str]]:
        """"""
        Override to return one or more opening messages with which to seed the self chat
        episode.

        The return value should be an array of strings, each string being a message in
        response to the string before it.
        """"""","        if self._openers:
            return [random.choice(self._openers)]
        return None"
201,"def pairwise_intersection(boxes1: Boxes, boxes2: Boxes) -> torch.Tensor:
    """"""
    Given two lists of boxes of size N and M,
    compute the intersection area between __all__ N x M pairs of boxes.
    The box order must be (xmin, ymin, xmax, ymax)

    Args:
        boxes1,boxes2 (Boxes): two `Boxes`. Contains N & M boxes, respectively.

    Returns:
        Tensor: intersection, sized [N,M].
    """"""","    boxes1, boxes2 = boxes1.tensor, boxes2.tensor
    width_height = torch.min(boxes1[:, None, 2:], boxes2[:, 2:]) - torch.max(
        boxes1[:, None, :2], boxes2[:, :2]
    )  # [N,M,2]

    width_height.clamp_(min=0)  # [N,M,2]
    intersection = width_height.prod(dim=2)  # [N,M]
    return intersection"
202,"    def close_connection(self, host):
        """"""close connection(s) to <host>
        host is the host:port spec, as in 'www.cnn.com:8080' as passed in.
        no error occurs if there is no connection to that host.""""""","        for h in self._cm.get_all(host):
            self._cm.remove(h)
            h.close()"
203,"    def read(self, node):
        """"""Obtain data from a parsed changelog revision.

        Returns a 6-tuple of:

           - manifest node in binary
           - author/user as a localstr
           - date as a 2-tuple of (time, timezone)
           - list of files
           - commit message as a localstr
           - dict of extra metadata

        Unless you need to access all fields, consider calling
        ``changelogrevision`` instead, as it is faster for partial object
        access.
        """"""","        c = changelogrevision(self.revision(node))
        return (c.manifest, c.user, c.date, c.files, c.description, c.extra)"
204,"def tokenize_rulekey_line(line):
    """"""
    Tokenizes a line of the form: '<type>(<value>):<type>(<value>):...<type>(<value>):'
    Tokens are returned in reverse order, because the serialization was also reversed.
    """"""","    # Since ':' can appear within a token value, we cheat and delimit tokens by '):'.
    delimiter = ""):""
    return [token + "")"" for token in reversed(line.split(delimiter)) if len(token) > 1]"
205,"def nullformatter(ui, topic):
    """"""formatter that prints nothing""""""","    return baseformatter(ui, topic, opts={}, converter=_nullconverter)"
206,"def is_branch_does_not_exist_error(response) -> bool:
    r""""""
    >>> response = {
    ...   ""data"": {
    ...     ""mergeBranch"": None
    ...   },
    ...   ""errors"": [
    ...     {
    ...       ""type"": ""NOT_FOUND"",
    ...       ""path"": [
    ...         ""mergeBranch""
    ...       ],
    ...       ""locations"": [
    ...         {
    ...           ""line"": 2,
    ...           ""column"": 3
    ...         }
    ...       ],
    ...       ""message"": ""No such base.""
    ...     }
    ...   ]
    ... }
    """"""","    errors = response.get(""errors"")
    if not errors or not isinstance(errors, list):
        return False
    for err in errors:
        if err.get(""type"") != ""NOT_FOUND"":
            continue
        message = err.get(""message"")
        if isinstance(message, str) and ""No such base."" in message:
            return True
    return False"
207,"def get_closest_vertices_mask_from_ES(
    E: torch.Tensor,
    S: torch.Tensor,
    h: int,
    w: int,
    mesh_vertex_embeddings: torch.Tensor,
    device: torch.device,
):
    """"""
    Interpolate Embeddings and Segmentations to the size of a given bounding box,
    and compute closest vertices and the segmentation mask

    Args:
        E (tensor [1, D, H, W]): D-dimensional embedding vectors for every point of the
            default-sized box
        S (tensor [1, 2, H, W]): 2-dimensional segmentation mask for every point of the
            default-sized box
        h (int): height of the target bounding box
        w (int): width of the target bounding box
        mesh_vertex_embeddings (tensor [N, D]): vertex embeddings for a chosen mesh
            N is the number of vertices in the mesh, D is feature dimensionality
        device (torch.device): device to move the tensors to
    Return:
        Closest Vertices (tensor [h, w]), int, for every point of the resulting box
        Segmentation mask (tensor [h, w]), boolean, for every point of the resulting box
    """"""","    embedding_resized = F.interpolate(E, size=(h, w), mode=""bilinear"")[0].to(device)
    coarse_segm_resized = F.interpolate(S, size=(h, w), mode=""bilinear"")[0].to(device)
    mask = coarse_segm_resized.argmax(0) > 0
    closest_vertices = torch.zeros(mask.shape, dtype=torch.long, device=device)
    all_embeddings = embedding_resized[:, mask].t()
    size_chunk = 10_000  # Chunking to avoid possible OOM
    edm = []
    if len(all_embeddings) == 0:
        return closest_vertices, mask
    for chunk in range((len(all_embeddings) - 1) // size_chunk + 1):
        chunk_embeddings = all_embeddings[size_chunk * chunk : size_chunk * (chunk + 1)]
        edm.append(
            torch.argmin(
                squared_euclidean_distance_matrix(chunk_embeddings, mesh_vertex_embeddings), dim=1
            )
        )
    closest_vertices[mask] = torch.cat(edm)
    return closest_vertices, mask"
208,"def CudaDevice(gpu_id):
    """"""Create a Cuda device.""""""","    return core.DeviceOption(caffe2_pb2.CUDA, gpu_id)"
209,"def is_distributed():
    """"""
    Return if we are in distributed mode.
    """"""",    return TORCH_AVAILABLE and dist.is_available() and dist.is_initialized()
210,"    def has_memory(self) -> bool:
        """"""
        Return whether there is memory.
        """"""",        return bool(self.active_memory_slots)
211,"def _rootsanddirs(kindpats):
    """"""Returns roots and exact directories from patterns.

    roots are directories to match recursively, whereas exact directories should
    be matched non-recursively. The returned (roots, dirs) tuple will also
    include directories that need to be implicitly considered as either, such as
    parent directories.

    >>> _rootsanddirs(
    ...     [('glob', 'g/h/*', ''), ('glob', 'g/h', ''),
    ...      ('glob', 'g*', '')])
    (['g/h', 'g/h', ''], ['', 'g'])
    >>> _rootsanddirs(
    ...     [('rootfilesin', 'g/h', ''), ('rootfilesin', '', '')])
    ([], ['g/h', '', '', 'g'])
    >>> _rootsanddirs(
    ...     [('relpath', 'r', ''), ('path', 'p/p', ''),
    ...      ('path', '', '')])
    (['r', 'p/p', ''], ['', 'p'])
    >>> _rootsanddirs(
    ...     [('relglob', 'rg*', ''), ('re', 're/', ''),
    ...      ('relre', 'rr', '')])
    (['', '', ''], [''])
    """"""","    r, d = _patternrootsanddirs(kindpats)

    # Append the parents as non-recursive/exact directories, since they must be
    # scanned to get to either the roots or the other exact directories.
    d.extend(sorted(util.dirs(d)))
    d.extend(sorted(util.dirs(r)))

    return r, d"
212,"def reassign_centroids(hassign, centroids, rs=None):
    """""" reassign centroids when some of them collapse """"""","    if rs is None:
        rs = np.random
    k, d = centroids.shape
    nsplit = 0
    empty_cents = np.where(hassign == 0)[0]

    if empty_cents.size == 0:
        return 0

    fac = np.ones(d)
    fac[::2] += 1 / 1024.
    fac[1::2] -= 1 / 1024.

    # this is a single pass unless there are more than k/2
    # empty centroids
    while empty_cents.size > 0:
        # choose which centroids to split
        probas = hassign.astype('float') - 1
        probas[probas < 0] = 0
        probas /= probas.sum()
        nnz = (probas > 0).sum()

        nreplace = min(nnz, empty_cents.size)
        cjs = rs.choice(k, size=nreplace, p=probas)

        for ci, cj in zip(empty_cents[:nreplace], cjs):

            c = centroids[cj]
            centroids[ci] = c * fac
            centroids[cj] = c / fac

            hassign[ci] = hassign[cj] // 2
            hassign[cj] -= hassign[ci]
            nsplit += 1

        empty_cents = empty_cents[nreplace:]

    return nsplit"
213,"def debugpull(ui, repo, **opts) -> None:
    """"""test repo.pull interface""""""","    headnames = []
    headnodes = []
    for rev in opts.get(""rev"", []):
        if len(rev) == 40:
            headnodes.append(bin(rev))
        else:
            headnames.append(rev)
    repo.pull(bookmarknames=opts[""bookmark""], headnodes=headnodes, headnames=headnames)"
214,"    def exportenv(self, name: str):
        """"""mark an env as exported""""""",        self.exportedenvvars.add(name)
215,"def getfiles(repo, proto):
    """"""A server api for requesting particular versions of particular files.""""""","    if shallowrepo.requirement in repo.requirements:
        raise error.Abort(_(""cannot fetch remote files from shallow repo""))
    if not isinstance(proto, sshserver.sshserver):
        raise error.Abort(_(""cannot fetch remote files over non-ssh protocol""))

    def streamer():
        fin = proto.fin
        args = []
        responselen = 0
        starttime = time.time()

        while True:
            request = fin.readline()[:-1]
            if not request:
                break

            hexnode = request[:40]
            node = bin(hexnode)
            if node == nullid:
                yield ""0\n""
                continue

            path = request[40:]

            args.append([hexnode, path])

            text = _loadfileblob(repo, path, node)

            response = ""%d\n%s"" % (len(text), text)
            responselen += len(response)
            yield response

            # it would be better to only flush after processing a whole batch
            # but currently we don't know if there are more requests coming
            proto.fout.flush()

        if repo.ui.configbool(""wireproto"", ""loggetfiles""):
            _logwireprotorequest(repo, ""getfiles"", starttime, responselen, args)

    return wireproto.streamres(streamer())"
216,"def threshold_radius_nres(nres, dis, ids, thresh, keep_max=False):
    """""" select a set of results """"""","    if keep_max:
        mask = dis > thresh
    else:
        mask = dis < thresh
    new_nres = np.zeros_like(nres)
    o = 0
    for i, nr in enumerate(nres):
        nr = int(nr)   # avoid issues with int64 + uint64
        new_nres[i] = mask[o:o + nr].sum()
        o += nr
    return new_nres, dis[mask], ids[mask]"
217,"    def str_to_api_dict(cls, string):
        """"""
        Used for API Call and Response Utterances -> Dict.
        """"""","        slot_strs = string.split("" ; "")
        result = {}
        for slot_str in slot_strs:
            if "" = "" not in slot_str:
                continue
            name, value = slot_str.split("" = "", 1)
            name = name.strip()
            value = SerializationHelpers.inner_list_split(value.strip())
            result[name] = value
        return result"
218,"    def __init__(self, tfidf_path=None, strict=True):
        """"""
        Args:
            tfidf_path: path to saved model file
            strict: fail on empty queries or continue (and return empty result)
        """"""","        # Load from disk
        logger.info('Loading %s' % tfidf_path)
        matrix, metadata = utils.load_sparse_csr(tfidf_path)
        self.doc_mat = matrix
        self.ngrams = metadata['ngram']
        self.hash_size = metadata['hash_size']
        self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()
        self.doc_freqs = metadata['doc_freqs'].squeeze()
        self.doc_dict = metadata.get('doc_dict', None)
        self.num_docs = self.doc_mat.shape[1] - 1
        self.strict = strict"
219,"    def setup_episodes(self, fold: str) -> List[tod.TodStructuredEpisode]:
        """"""
        Fold here is a data fold.
        """"""","        raise NotImplementedError(
            ""Must have method for generating an episode. Must be set in downstream Parser for a given task""
        )"
220,"def migratetosegments(repo):
    """"""Migrate to full ""segmentedchangelog"" backend.

    Commit graph, IdMap, commit text are all backed by segmented changelog.
    This can take 10+ minutes for a large repo.
    """"""","    if ""segmentedchangelog"" in repo.storerequirements:
        return
    svfs = repo.svfs
    revlogdir = svfs.join("""")
    segmentsdir = svfs.join(SEGMENTS_DIR)
    hgcommitsdir = svfs.join(HGCOMMITS_DIR)
    with repo.lock():
        zstore = bindings.zstore.zstore(svfs.join(HGCOMMITS_DIR))
        cl = repo.changelog
        with progress.bar(
            repo.ui, _(""migrating commit text""), _(""commits""), len(cl)
        ) as prog:
            clnode = cl.node
            clparents = cl.parents
            clrevision = cl.revision
            contains = zstore.__contains__
            insert = zstore.insert
            textwithheader = changelogmod.textwithheader
            try:
                for rev in cl.revs():
                    node = clnode(rev)
                    if contains(node):
                        continue
                    text = clrevision(rev)
                    p1, p2 = clparents(node)
                    newnode = insert(textwithheader(text, p1, p2))
                    assert node == newnode
                    prog.value += 1
            finally:
                # In case of Ctrl+C, flush commits in memory so we can continue
                # next time.
                zstore.flush()

        master = list(repo.nodes(""present(%s)"", bookmod.mainbookmark(repo)))
        with progress.spinner(repo.ui, _(""migrating commit graph"")):
            bindings.dag.commits.migraterevlogtosegments(
                revlogdir, segmentsdir, hgcommitsdir, master
            )

        _removechangelogrequirements(repo)
        repo.storerequirements.add(""segmentedchangelog"")
        repo._writestorerequirements()
        repo.invalidatechangelog()"
221,"  def __init__(self, chroot_base):
    """"""Create the chroot.

    :chroot_base Directory for the creation of the target chroot.
    """"""","    try:
      safe_mkdir(chroot_base)
    except OSError as e:
      raise self.ChrootException('Unable to create chroot in %s: %s' % (chroot_base, e))
    self.chroot = chroot_base
    self.filesets = defaultdict(set)"
222,"    def list_dir(self, path: Union[Path, str]) -> List[Path]:
        """"""List the contents of a directory in the checkout.
        This can be used to ensure the directory has been loaded by Eden.
        """"""","        dir_path = self.checkout_path / path
        return list(dir_path.iterdir())"
223,"    def __init__(self, stackparentnode, pushcommits, fileconditions):
        """"""constructor for pushrequest

        This class is designed to only include simple types (list, dict,
        strings), without coupling with Mercurial internals, for maximum
        portability.

        Do not add states that are not simple types (ex. repo, ui, or bundle).
        """"""","
        self.stackparentnode = stackparentnode
        self.pushcommits = pushcommits
        self.fileconditions = fileconditions  # {path: None | filenode}"
224,"def ProcessFileData(filename, file_extension, lines, error,
                    extra_check_functions=[]):
  """"""Performs lint checks and reports any errors to the given error function.

  Args:
    filename: Filename of the file that is being processed.
    file_extension: The extension (dot not included) of the file.
    lines: An array of strings, each representing a line of the file, with the
           last element being empty if the file is terminated with a newline.
    error: A callable to which errors are reported, which takes 4 arguments:
           filename, line number, error level, and message
    extra_check_functions: An array of additional check functions that will be
                           run on each source line. Each function takes 4
                           arguments: filename, clean_lines, line, error
  """"""","  lines = (['// marker so line numbers and indices both start at 1'] + lines +
           ['// marker so line numbers end in a known way'])

  include_state = _IncludeState()
  function_state = _FunctionState()
  nesting_state = _NestingState()

  ResetNolintSuppressions()

  CheckForCopyright(filename, lines, error)

  if file_extension == 'h':
    CheckForHeaderGuard(filename, lines, error)

  RemoveMultiLineComments(filename, lines, error)
  clean_lines = CleansedLines(lines)
  for line in xrange(clean_lines.NumLines()):
    ProcessLine(filename, file_extension, clean_lines, line,
                include_state, function_state, nesting_state, error,
                extra_check_functions)
  nesting_state.CheckCompletedBlocks(filename, error)

  CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error)

  # We check here rather than inside ProcessLine so that we see raw
  # lines rather than ""cleaned"" lines.
  CheckForBadCharacters(filename, lines, error)

  CheckForNewlineAtEOF(filename, lines, error)"
225,"    def get_subtype_requiredness(self):
        """"""Return a mapping from each subtype to its requiredness. The keys are
        the same as for get_subtypes and the values are booleans
        """"""",        return {}
226,"def unimethod(bytesfunc):
    """"""Create a proxy method that forwards __unicode__() and __str__() of
    Python 3 to __bytes__()""""""","
    def unifunc(obj):
        return unifromlocal(bytesfunc(obj))

    return unifunc"
227,"def localrepolistkeys(orig, self, namespace, patterns=None):
    """"""Wrapper of localrepo.listkeys()""""""","
    if namespace == ""bookmarks"" and patterns:
        index = self.bundlestore.index
        # Using sortdict instead of a dictionary to ensure that bookmaks are
        # restored in the same order after a pullbackup. See T24417531
        results = util.sortdict()
        bookmarks = orig(self, namespace)
        for pattern in patterns:
            results.update(index.getbookmarks(pattern))
            if pattern.endswith(""*""):
                pattern = ""re:^"" + pattern[:-1] + "".*""
            kind, pat, matcher = util.stringmatcher(pattern)
            for bookmark, node in pycompat.iteritems(bookmarks):
                if matcher(bookmark):
                    results[bookmark] = node
        return results
    else:
        return orig(self, namespace)"
228,"    def get_model_output(self, batch: Batch) -> Tuple[Any, ...]:
        """"""
        Return model output.

        :param batch:
            batch to process

        :return model_output:
            return output from model
        """"""","        if not self.regret:
            model_output = self.model(
                *self._model_input(batch), ys=batch.label_vec
            )  # type: ignore
            scores, preds, enc_state, *_ = model_output
        else:
            with torch.no_grad():
                beam_preds_scores, beams = self._regret_generate(
                    batch, self.beam_size, self.regret_intermediate_maxlen
                )
            regret_preds, _, _ = zip(*beam_preds_scores)
            new_batch = self._regret_rebatchify(batch, regret_preds)  # type: ignore
            regret_model_output = self.model(
                *self._model_input(new_batch), ys=batch.label_vec
            )  # type: ignore
            regret_scores, preds, enc_state = regret_model_output
            scores = regret_scores

        return (scores, preds, enc_state)"
229,"    def share(self):
        """"""
        Share the episodes.
        """"""","        shared = super().share()
        shared['episodes'] = self.episodes
        return shared"
230,"    def _compareondisk(self, path, wctx, pctx):
        """"""Compares the on-disk file content with the clean-checkout content.
        Return True if on-disk is different, False if it is the same, and None
        of the on-disk file is deleted or no longer accessible.
        """"""","        try:
            # This will return True for a file that got replaced by a
            # directory in the interim, but fixing that is pretty hard.
            if path not in pctx or wctx.flags(path) != pctx.flags(path):
                # Has changed
                return True

            pfctx = pctx[path]
            # Normally we would use pfctx.cmp(wctx[path]) instead of the filelog
            # cmp() function. Unfortunately filectx.cmp accesses the file size,
            # and getting the size with the current data store requires fetching
            # the whole file content. For now let's avoid that and go straight
            # to the filelog.cmp. Later we will fix this comparison to be pure
            # content sha comparisons, which will remove the need for this hack.
            return pfctx.filelog().cmp(pfctx.filenode(), wctx[path].data())
        except (IOError, OSError):
            # A file become inaccessible in between? Mark it as deleted,
            # matching dirstate behavior (issue5584).
            # The dirstate has more complex behavior around whether a
            # missing file matches a directory, etc, but we don't need to
            # bother with that: if f has made it to this point, we're sure
            # it's in the dirstate.
            return None"
231,"def prunecontainers(blocks, keep):
    """"""Prune unwanted containers.

    The blocks must have a 'type' field, i.e., they should have been
    run through findliteralblocks first.
    """"""","    pruned = []
    i = 0
    while i + 1 < len(blocks):
        # Searching for a block that looks like this:
        #
        # +-------+---------------------------+
        # | "".. container ::"" type            |
        # +---+                               |
        #     | blocks                        |
        #     +-------------------------------+
        if blocks[i][""type""] == ""paragraph"" and blocks[i][""lines""][0].startswith(
            "".. container::""
        ):
            indent = blocks[i][""indent""]
            adjustment = blocks[i + 1][""indent""] - indent
            containertype = blocks[i][""lines""][0][15:]
            prune = True
            for c in keep:
                if c in containertype.split("".""):
                    prune = False
            if prune:
                pruned.append(containertype)

            # Always delete ""..container:: type"" block
            del blocks[i]
            j = i
            i -= 1
            while j < len(blocks) and blocks[j][""indent""] > indent:
                if prune:
                    del blocks[j]
                else:
                    blocks[j][""indent""] -= adjustment
                    j += 1
        i += 1
    return blocks, pruned"
232,"    def suggested_hit_points_value(self):
        """"""
        :return: XCUIElement suggested hit points
        :rtype: str
        """"""","        return normalize_array_description(
            self.suggested_hit_points.GetObjectDescription()
        )"
233,"    def get_edenfs_processes(self) -> Iterable[EdenFSProcess]:
        """"""Returns a list of running EdenFS processes on the system.""""""",        raise NotImplementedError()
234,"def replace_method(the_class, name, replacement, ignore_missing=False):
    """""" Replaces a method in a class with another version. The old method
    is renamed to method_name_c (because presumably it was implemented in C) """"""","    try:
        orig_method = getattr(the_class, name)
    except AttributeError:
        if ignore_missing:
            return
        raise
    if orig_method.__name__ == 'replacement_' + name:
        # replacement was done in parent class
        return
    setattr(the_class, name + '_c', orig_method)
    setattr(the_class, name, replacement)"
235,"    def share(self):
        """"""
        Share internal states between parent and child instances.
        """"""","        shared = super().share()
        shared['dict'] = self.dict
        shared['model'] = self.model
        return shared"
236,"def parsebindag(data: Sized):
    """"""Reverse of `bindag`. Translated binary DAG to revs and parentrevs.

    The returned revs use integer commit identities starting from 0.
    """"""","
    def readiter(data, decodeat=vlq.decodeat):
        offset = 0
        while offset < len(data):
            value, size = decodeat(data, offset)
            yield value
            offset += size

    it = readiter(data)
    parents = []  # index: id, value: parentids
    append = parents.append

    # build dag in-memory
    while True:
        i = next(it, None)
        lastid = len(parents) - 1
        if i is None:
            break
        elif i == 0:
            append(())
        elif i == 1:
            p1 = lastid - next(it)
            append((p1,))
        elif i == 2:
            p1 = lastid - next(it)
            p2 = lastid - next(it)
            append((p1, p2))
        elif i == 3:
            p1 = lastid
            p2 = lastid - next(it)
            append((p1, p2))
        elif i == 4:
            p1 = lastid - next(it)
            p2 = lastid
            append((p1, p2))
        else:
            n = i - 4
            while n > 0:
                p1 = len(parents) - 1
                parents.append((p1,))
                n -= 1

    revs = range(len(parents))
    parentrevs = parents.__getitem__
    return revs, parentrevs"
237,"    def __init__(
        self,
        attn_type,
        hiddensize,
        embeddingsize,
        bidirectional=False,
        attn_length=-1,
        attn_time='pre',
    ):
        """"""
        Initialize attention layer.
        """"""","        super().__init__()
        self.attention = attn_type

        if self.attention != 'none':
            hsz = hiddensize
            hszXdirs = hsz * (2 if bidirectional else 1)
            if attn_time == 'pre':
                # attention happens on the input embeddings
                input_dim = embeddingsize
            elif attn_time == 'post':
                # attention happens on the output of the rnn
                input_dim = hsz
            else:
                raise RuntimeError('unsupported attention time')

            # linear layer for combining applied attention weights with input
            self.attn_combine = nn.Linear(hszXdirs + input_dim, input_dim, bias=False)

            if self.attention == 'local':
                # local attention over fixed set of output states
                if attn_length < 0:
                    raise RuntimeError('Set attention length to > 0.')
                self.max_length = attn_length
                # combines input and previous hidden output layer
                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=False)
                # combines attention weights with encoder outputs
            elif self.attention == 'concat':
                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=False)
                self.attn_v = nn.Linear(hsz, 1, bias=False)
            elif self.attention == 'general':
                # equivalent to dot if attn is identity
                self.attn = nn.Linear(hsz, hszXdirs, bias=False)"
238,"    def scan(self, search_path=None):
        """"""Scan `search_path` for distributions usable in this environment

        Any distributions found are added to the environment.
        `search_path` should be a sequence of ``sys.path`` items.  If not
        supplied, ``sys.path`` is used.  Only distributions conforming to
        the platform/python version defined at initialization are added.
        """"""","        if search_path is None:
            search_path = sys.path

        for item in search_path:
            for dist in find_distributions(item):
                self.add(dist)"
239,"    def dictionary_class():
        """"""
        Replace normal dictionary class with mock one.
        """"""",        return MockDict
240,"    def test_recommendations_group(self):
        """"""
        Test whether recommended args work for a group.
        """"""","        parser = ParlaiParser(False, False)
        parser_grp = parser.add_argument_group('Test Group')
        parser_grp.add_argument(
            '-bs',
            '--batchsize',
            default=1,
            type=int,
            help='batch size for minibatch training schemes',
            recommended=1337,
        )
        parser.parse_args([])

        help_str = parser.format_help()
        assert 'Test Group' in help_str
        _, latter = help_str.split('Test Group')
        assert 'recommended:' in latter
        assert '1337' in latter"
241,"    def getPeerName(self):
        """"""Gets the address of the client.

        Returns:
          The equivalent value of socket.getpeername() on the client socket
        """"""",        raise NotImplementedError
242,"def _makenofollowlogfilematcher(repo, pats, opts):
    """"""hook for extensions to override the filematcher for non-follow cases""""""",    return None
243,"  def PEX_INTERPRETER(self):
    """"""Boolean

    Drop into a REPL instead of invoking the predefined entry point of this PEX. This can be
    useful for inspecting the PEX environment interactively.  It can also be used to treat the PEX
    file as an interpreter in order to execute other scripts in the context of the PEX file, e.g.
    ""PEX_INTERPRETER=1 ./app.pex my_script.py"".  Equivalent to setting PEX_MODULE to empty.
    Default: false.
    """"""","    return self._get_bool('PEX_INTERPRETER', default=False)"
244,"def get_product_list():
    """"""Return a list of product configs available.""""""","    instance = adobe_api.AdobeAPIObject(
        ""fake@fake.com"",
        allow_nonexistent_user=True
    )
    productlist = instance.gather_product_list()
    return [x['groupName'] for x in productlist]"
245,"def _iteritems(data):
    """"""iterate key-value pairs in stable order""""""","    if isinstance(data, dict):
        return sorted(pycompat.iteritems(data))
    return data"
246,"    def endSequence(self, seq, state):
        """"""Invoked at the end of a sequence of tests.""""""",
247,"    def opent(
        self, path: str, mode: str = ""r"", buffering: int = 32, **kwargs: Any
    ) -> TabularIO:
        """"""
        Open a tabular data source. Only reading is supported.
        The opent() returns a Python iterable collection object, compared to bytes/text data with open()

        Args:
            path (str): A URI supported by this PathHandler
            mode (str): Specifies the mode in which the file is opened. It defaults
                to 'r'
            buffering (int): number of rows fetched and cached

        Returns:
            A TabularIO context manager object
        """"""","        return self.__get_path_handler(path)._opent(path, mode, buffering, **kwargs)"
248,"def _makegetfctx(ctx):
    """"""return a 'getfctx' function suitable for _checkcopies usage

    We have to re-setup the function building 'filectx' for each
    '_checkcopies' to ensure the linkrev adjustment is properly setup for
    each. Linkrev adjustment is important to avoid bug in rename
    detection. Moreover, having a proper '_ancestrycontext' setup ensures
    the performance impact of this adjustment is kept limited. Without it,
    each file could do a full dag traversal making the time complexity of
    the operation explode (see issue4537).

    This function exists here mostly to limit the impact on stable. Feel
    free to refactor on default.
    """"""","    rev = ctx.rev()
    repo = ctx._repo
    ac = getattr(ctx, ""_ancestrycontext"", None)
    if ac is None:
        revs = [rev]
        if rev is None:
            revs = [p.rev() for p in ctx.parents()]
        ac = repo.changelog.ancestors(revs, inclusive=True)
        ctx._ancestrycontext = ac

    def makectx(f, n):
        if n in node.wdirnodes:  # in a working context?
            if ctx.rev() is None:
                return ctx.filectx(f)
            return repo[None][f]
        fctx = repo.filectx(f, fileid=n)
        # setup only needed for filectx not create from a changectx
        fctx._ancestrycontext = ac
        fctx._descendantrev = rev
        # make __repr__ / introrev / adjustlinkrev work
        fctx._changeid = rev
        return fctx

    return util.lrucachefunc(makectx)"
249,"    def forward(self, tokens, segments):
        """"""
        Scores each concatenation text + candidate.
        """"""","        encoded = self.encoder(tokens, None, segments)
        res = self.linear_layer(encoded)
        return res"
250,"def slurm_distributed_context(opt):
    """"""
    Initialize a distributed context, using the SLURM environment.

    Does some work to read the environment to find a list of participating nodes
    and the main node.

    :param opt:
        Command line options.
    """"""","    # We can determine the init method automatically for Slurm.
    # double check we're using SLURM
    node_list = os.environ.get('SLURM_JOB_NODELIST')
    if node_list is None:
        raise RuntimeError(
            'Does not appear to be in a SLURM environment. '
            'You should not call this script directly; see launch_distributed.py'
        )
    try:
        # Figure out the main host, and which rank we are.
        hostnames = subprocess.check_output(
            ['scontrol', 'show', 'hostnames', node_list]
        )
    except FileNotFoundError as e:
        # Slurm is not installed
        raise RuntimeError(
            f'SLURM does not appear to be installed. Missing file: {e.filename}'
        )

    main_host = hostnames.split()[0].decode('utf-8')
    distributed_rank = int(os.environ['SLURM_PROCID'])
    if opt.get('model_parallel'):
        # -1 signals to multiprocessing_train to use all GPUs available.
        # (A value of None signals to multiprocessing_train to use the GPU
        # corresponding to the rank.
        device_id = -1
    else:
        device_id = int(os.environ['SLURM_LOCALID'])
    port = opt['port']
    logging.info(
        f'Initializing host {socket.gethostname()} as rank {distributed_rank}, '
        f'main is {main_host}'
    )
    # Begin distributed training
    with distributed_context(
        distributed_rank, opt, 0, device_id, init_method=f""tcp://{main_host}:{port}""
    ) as opt:
        yield opt"
251,"def _pullchangeset(pullop):
    """"""pull changeset from unbundle into the local repo""""""","    # We delay the open of the transaction as late as possible so we
    # don't open transaction for nothing or you break future useful
    # rollback call
    if ""changegroup"" in pullop.stepsdone:
        return
    pullop.stepsdone.add(""changegroup"")
    if not pullop.fetch:
        pullop.repo.ui.status(_(""no changes found\n""))
        pullop.cgresult = 0
        return
    tr = pullop.gettransaction()
    if pullop.heads is None and list(pullop.common) == [nullid]:
        pullop.repo.ui.status(_(""requesting all changes\n""))
    elif pullop.heads is None and pullop.remote.capable(""changegroupsubset""):
        # issue1320, avoid a race if remote changed after discovery
        pullop.heads = pullop.rheads

    repo = pullop.repo
    if _httpcommitgraphenabled(repo, pullop.remote):
        return _pullcommitgraph(pullop)

    if pullop.remote.capable(""getbundle""):
        # TODO: get bundlecaps from remote
        cg = pullop.remote.getbundle(
            ""pull"", common=pullop.common, heads=pullop.heads or pullop.rheads
        )
    elif pullop.heads is None:
        cg = pullop.remote.changegroup(pullop.fetch, ""pull"")
    elif not pullop.remote.capable(""changegroupsubset""):
        raise error.Abort(
            _(
                ""partial pull cannot be done because ""
                ""other repository doesn't support ""
                ""changegroupsubset.""
            )
        )
    else:
        cg = pullop.remote.changegroupsubset(pullop.fetch, pullop.heads, ""pull"")
    bundleop = bundle2.applybundle(pullop.repo, cg, tr, ""pull"", pullop.remote.url())
    pullop.cgresult = bundle2.combinechangegroupresults(bundleop)"
252,"def register_pretty_printer(obj, printer, replace=False):
    """"""Register pretty-printer PRINTER with OBJ.

    The printer is added to the front of the search list, thus one can override
    an existing printer if one needs to.  Use a different name when overriding
    an existing printer, otherwise an exception will be raised; multiple
    printers with the same name are disallowed.

    Arguments:
        obj: Either an objfile, progspace, or None (in which case the printer
            is registered globally).
        printer: Either a function of one argument (old way) or any object
            which has attributes: name, enabled, __call__.
        replace: If True replace any existing copy of the printer.
            Otherwise if the printer already exists raise an exception.

    Returns:
        Nothing.

    Raises:
        TypeError: A problem with the type of the printer.
        ValueError: The printer's name contains a semicolon "";"".
        RuntimeError: A printer with the same name is already registered.

    If the caller wants the printer to be listable and disableable, it must
    follow the PrettyPrinter API.  This applies to the old way (functions) too.
    If printer is an object, __call__ is a method of two arguments:
    self, and the value to be pretty-printed.  See PrettyPrinter.
    """"""","
    # Watch for both __name__ and name.
    # Functions get the former for free, but we don't want to use an
    # attribute named __foo__ for pretty-printers-as-objects.
    # If printer has both, we use `name'.
    if not hasattr(printer, ""__name__"") and not hasattr(printer, ""name""):
        raise TypeError(""printer missing attribute: name"")
    if hasattr(printer, ""name"") and not hasattr(printer, ""enabled""):
        raise TypeError(""printer missing attribute: enabled"") 
    if not hasattr(printer, ""__call__""):
        raise TypeError(""printer missing attribute: __call__"")

    if hasattr(printer, ""name""):
      name = printer.name
    else:
      name = printer.__name__
    if obj is None or obj is gdb:
        if gdb.parameter(""verbose""):
            gdb.write(""Registering global %s pretty-printer ...\n"" % name)
        obj = gdb
    else:
        if gdb.parameter(""verbose""):
            gdb.write(""Registering %s pretty-printer for %s ...\n"" % (
                name, obj.filename))

    # Printers implemented as functions are old-style.  In order to not risk
    # breaking anything we do not check __name__ here.
    if hasattr(printer, ""name""):
        if not isinstance(printer.name, basestring):
            raise TypeError(""printer name is not a string"")
        # If printer provides a name, make sure it doesn't contain "";"".
        # Semicolon is used by the info/enable/disable pretty-printer commands
        # to delimit subprinters.
        if printer.name.find("";"") >= 0:
            raise ValueError(""semicolon ';' in printer name"")
        # Also make sure the name is unique.
        # Alas, we can't do the same for functions and __name__, they could
        # all have a canonical name like ""lookup_function"".
        # PERF: gdb records printers in a list, making this inefficient.
        i = 0
        for p in obj.pretty_printers:
            if hasattr(p, ""name"") and p.name == printer.name:
                if replace:
                    del obj.pretty_printers[i]
                    break
                else:
                  raise RuntimeError(""pretty-printer already registered: %s"" %
                                     printer.name)
            i = i + 1

    obj.pretty_printers.insert(0, printer)"
253,"    def filesnotin(self, m2, matcher=None):
        """"""Set of files in this manifest that are not in the other""""""","        if matcher:
            m1 = self.matches(matcher)
            m2 = m2.matches(matcher)
            return m1.filesnotin(m2)
        diff = self.diff(m2)
        files = set(
            filepath
            for filepath, hashflags in pycompat.iteritems(diff)
            if hashflags[1][0] is None
        )
        return files"
254,"def build_bottle_file(
    homebrew_dir,
    tap_repository,
    tap_path,
    release_version,
    target_macos_version,
    output_dir,
):
    """"""
    Builds the actual bottle file via brew

    Args:
        tap_repository: The name of the tap repository
        tap_path: The local path to the given tap repository
        release_version: The version that should be built (no ""v"" prefix)
        target_macos_version: The target macos short nameto use in the resulting path
        output_dir: The directory to move the build artifact to after building

    Returns:
        The path to the bottle.tar.gz
    """"""","    brew_target = tap_repository + ""/buck""

    logging.info(""Building bottle"")
    # Cool, so install --force will still not rebuild. Uninstall, and just don't
    # care if the uninstall fails
    brew(homebrew_dir, [""uninstall"", ""--force"", brew_target], tap_path, check=False)
    brew(homebrew_dir, [""install"", ""--force"", ""--build-bottle"", brew_target], tap_path)
    logging.info(""Creating bottle file"")
    brew(
        homebrew_dir,
        [""bottle"", ""--no-rebuild"", ""--skip-relocation"", brew_target],
        tap_path,
    )
    logging.info(""Created bottle file"")

    bottle_filename = ""buck-{ver}.{macos_ver}.bottle.tar.gz"".format(
        ver=release_version, macos_ver=target_macos_version
    )
    bottle_path = os.path.join(output_dir, bottle_filename)
    bottles = glob.glob(
        os.path.join(tap_path, ""buck--{}*.bottle.tar.gz"".format(release_version))
    )
    if len(bottles) != 1:
        raise ReleaseException(
            ""Got an invalid number of bottle files ({} files: {})"".format(
                len(bottles), "" "".join(bottles)
            )
        )
    shutil.move(bottles[0], bottle_path)
    return bottle_path"
255,"def verifymanifesthook(ui, repo, **kwargs):
    """"""pretxnclose hook that verifies that every newly added commit has a
    corresponding root manifest.""""""","    node = kwargs.get(""node"")
    if node is None:
        return

    newctxs = list(repo.set(""%s:"", node))
    mfnodes = set(ctx.manifestnode() for ctx in newctxs)

    mfdatastore = repo.manifestlog.datastore
    missing = mfdatastore.getmissing(("""", mfnode) for mfnode in mfnodes)

    if missing:
        missingmfnodes = set(hex(key[1]) for key in missing)
        hexnodes = list(
            ctx.hex() for ctx in newctxs if hex(ctx.manifestnode()) in missingmfnodes
        )
        raise error.Abort(
            _(
                ""attempting to close transaction which includes commits (%s) without ""
                ""manifests (%s)""
            )
            % ("", "".join(hexnodes), "", "".join(missingmfnodes))
        )"
256,"def modified(mctx, x):
    """"""File that is modified according to :prog:`status`.""""""","    # i18n: ""modified"" is a keyword
    getargs(x, 0, 0, _(""modified takes no arguments""))
    s = set(mctx.status().modified)
    return [f for f in mctx.subset if f in s]"
257,"    def _perform_forward_passes(self, batch: Batch) -> ForwardPassOutputs:
        """"""
        Perform forward passes through the student and teacher and pass back outputs.
        """"""","
        assert isinstance(self, TorchGeneratorAgent)
        # Code relies on methods

        mask = batch.label_vec != self.NULL_IDX

        self._clear_hook_outputs(self.hooks)

        # Forward pass through teacher model
        with torch.no_grad():
            teacher_scores, teacher_preds, teacher_enc_states = self.teacher_model(
                *self._model_input(batch), ys=batch.label_vec
            )
            teacher_enc_output, context_mask = teacher_enc_states

        # Forward pass through student model
        task_loss, student_output = super().compute_loss(batch, return_output=True)
        student_scores, student_preds, student_enc_states = student_output
        student_enc_output, _ = student_enc_states

        # Compile all outputs given the hooks
        teacher_embedding_outputs = self._extract_embedding_outputs(
            hooks=self.hooks['teacher']
        )
        student_embedding_outputs = self._extract_embedding_outputs(
            hooks=self.hooks['student']
        )
        teacher_hidden_states = self._extract_hidden_states(
            hooks=self.hooks['teacher'],
            num_enc_layers=self.teacher_num_enc_layers,
            num_dec_layers=self.teacher_num_dec_layers,
        )
        student_hidden_states = self._extract_hidden_states(
            hooks=self.hooks['student'],
            num_enc_layers=self.student_num_enc_layers,
            num_dec_layers=self.student_num_dec_layers,
        )
        teacher_attention_matrices = self._extract_attention_matrices(
            hooks=self.hooks['teacher'],
            num_enc_layers=self.teacher_num_enc_layers,
            num_dec_layers=self.teacher_num_dec_layers,
        )
        student_attention_matrices = self._extract_attention_matrices(
            hooks=self.hooks['student'],
            num_enc_layers=self.student_num_enc_layers,
            num_dec_layers=self.student_num_dec_layers,
        )
        self._clear_hook_outputs(self.hooks)

        tokens_per_example = mask.sum(dim=-1)  # Sum over tokens
        num_tokens = mask.sum()
        context_tokens_per_example = context_mask.sum(dim=-1)  # Sum over tokens
        num_context_tokens = context_mask.sum()

        # If needed, perform further manipulation of the mask tensor
        mask = self._manipulate_mask(
            mask=mask, student_scores=student_scores, batch=batch
        )
        decoder_mask = self._manipulate_mask(
            mask=mask, student_scores=student_embedding_outputs[""decoder""], batch=batch
        )

        # Record teacher accuracy
        teacher_acc = ((student_preds == teacher_preds) * mask).sum(dim=-1)
        # Sum over tokens
        self.record_local_metric(
            'teacher_acc', AverageMetric.many(teacher_acc, tokens_per_example)
        )

        return ForwardPassOutputs(
            mask=mask,
            decoder_mask=decoder_mask,
            tokens_per_example=tokens_per_example,
            num_tokens=num_tokens,
            context_mask=context_mask,
            context_tokens_per_example=context_tokens_per_example,
            num_context_tokens=num_context_tokens,
            task_loss=task_loss,
            teacher_scores=teacher_scores,
            teacher_enc_output=teacher_enc_output,
            teacher_embedding_outputs=teacher_embedding_outputs,
            teacher_hidden_states=teacher_hidden_states,
            teacher_attention_matrices=teacher_attention_matrices,
            student_output=student_output,
            student_scores=student_scores,
            student_enc_output=student_enc_output,
            student_embedding_outputs=student_embedding_outputs,
            student_hidden_states=student_hidden_states,
            student_attention_matrices=student_attention_matrices,
        )"
258,"def isbackedupnodes(getconnection, nodes):
    """"""
    check on the server side if the nodes are backed up using 'known' or 'knownnodes' commands
    """"""","    with getconnection() as conn:
        if ""knownnodes"" in conn.peer.capabilities():
            return conn.peer.knownnodes([nodemod.bin(n) for n in nodes])
        else:
            return conn.peer.known([nodemod.bin(n) for n in nodes])"
259,"def walktree(tree):
    """"""Walk through a tree recursively

    A tree is a S-exp represented by Python tuple and strings.
    For example, ('func', ('symbol', 'ancestors'), ('symbol', 'foo')).
    """"""","    if isinstance(tree, tuple):
        yield tree
        for subtree in tree:
            if isinstance(subtree, tuple):
                for t in walktree(subtree):
                    yield t"
260,"def _single_template(tier, seed=1, dev_seed=None,
                     train_share=TRAIN_SHARE) -> EvalSetup:
    """"""Each template is a separate group.""""""","    task_ids = get_task_ids_in_tier(tier)
    tasks_per_tpl = _get_task_per_tpl(task_ids)
    eval_setup = []
    for _, task_ids_group in sorted(tasks_per_tpl.items()):
        task_ids_group = phyre.util.stable_shuffle(
            task_ids_group, f'ball_online_ind_tasks{seed}')
        train_size = int(round(len(task_ids_group) * train_share))
        if not train_size:
            continue
        train_ids, eval_ids = (task_ids_group[:train_size],
                               task_ids_group[train_size:])
        eval_groups = (tuple(eval_ids),)
        train_set = tuple(train_ids)
        train_group = (train_set, eval_groups)
        eval_setup.append(train_group)

    if dev_seed is not None:
        eval_setup = create_dev_set(eval_setup, train_share, seed=dev_seed)

    return eval_setup"
261,"def movecursor(state, oldpos, newpos) -> None:
    """"""Change the rule/changeset that the cursor is pointing to, regardless of
    current mode (you can switch between patches from the view patch window).""""""","    state[""pos""] = newpos

    mode, _ = state[""mode""]
    if mode == MODE_RULES:
        # Scroll through the list by updating the view for MODE_RULES, so that
        # even if we are not currently viewing the rules, switching back will
        # result in the cursor's rule being visible.
        modestate = state[""modes""][MODE_RULES]
        if newpos < modestate[""line_offset""]:
            modestate[""line_offset""] = newpos
        elif newpos > modestate[""line_offset""] + state[""page_height""] - 1:
            modestate[""line_offset""] = newpos - state[""page_height""] + 1

    # Reset the patch view region to the top of the new patch.
    state[""modes""][MODE_PATCH][""line_offset""] = 0"
262,"    def grant_agent_training_qualification(self, role_id: int):
        """"""
        Granting the onboarding qualification to the agent, based on their assigned
        role.
        """"""","        role = constants.ROLE_QUALIFICATION_NAME_KEY[role_id]
        logging.info(f'Granting worker qualification for {role} role.')
        worker = self.get_worker()
        worker.grant_qualification(self.role_training_qname, role_id)"
263,"def applyupdates(repo, actions, wctx, mctx, overwrite, labels=None, ancestors=None):
    """"""apply the merge action list to the working directory

    wctx is the working copy context
    mctx is the context to be merged into the working copy

    Return a tuple of counts (updated, merged, removed, unresolved) that
    describes how many files were affected by the update.
    """"""","    perftrace.tracevalue(""Actions"", sum(len(v) for k, v in pycompat.iteritems(actions)))

    updated, merged, removed = 0, 0, 0

    ms = mergestate.clean(
        repo,
        node=wctx.p1().node(),
        other=mctx.node(),
        # Ancestor can include the working copy, so we use this helper:
        ancestors=[scmutil.contextnodesupportingwdir(c) for c in ancestors]
        if ancestors
        else None,
        labels=labels,
    )

    moves = []
    for m, l in actions.items():
        l.sort()

    # 'cd' and 'dc' actions are treated like other merge conflicts
    mergeactions = sorted(actions[""cd""])
    mergeactions.extend(sorted(actions[""dc""]))
    mergeactions.extend(actions[""m""])
    for f, args, msg in mergeactions:
        f1, f2, fa, move, anc = args
        if f1 is None:
            fcl = filemerge.absentfilectx(wctx, fa)
        else:
            repo.ui.debug("" preserving %s for resolve of %s\n"" % (f1, f))
            fcl = wctx[f1]
        if f2 is None:
            fco = filemerge.absentfilectx(mctx, fa)
        else:
            fco = mctx[f2]
        actx = repo[anc]
        if fa in actx:
            fca = actx[fa]
        else:
            # TODO: move to absentfilectx
            fca = repo.filectx(f1, changeid=nullid, fileid=nullid)
        ms.add(fcl, fco, fca, f)
        if f1 != f and move:
            moves.append(f1)

    # remove renamed files after safely stored
    for f in moves:
        if wctx[f].lexists():
            repo.ui.debug(""removing %s\n"" % f)
            wctx[f].audit()
            wctx[f].remove()

    numupdates = sum(len(l) for m, l in actions.items() if m != ""k"")
    z = 0

    def userustworker():
        return ""remotefilelog"" in repo.requirements and not wctx.isinmemory()

    rustworkers = userustworker()

    # record path conflicts
    with progress.bar(
        repo.ui, _(""updating""), _(""files""), numupdates
    ) as prog, repo.ui.timesection(""updateworker""):
        for f, args, msg in actions[""p""]:
            f1, fo = args
            s = repo.ui.status
            s(
                _(
                    ""%s: path conflict - a file or link has the same name as a ""
                    ""directory\n""
                )
                % f
            )
            if fo == ""l"":
                s(_(""the local file has been renamed to %s\n"") % f1)
            else:
                s(_(""the remote file has been renamed to %s\n"") % f1)
            s(_(""resolve manually then use '@prog@ resolve --mark %s'\n"") % f)
            ms.addpath(f, f1, fo)
            z += 1
            prog.value = (z, f)

        # Flush any pending data to disk before forking workers, so the workers
        # don't all flush duplicate data.
        repo.commitpending()

        # remove in parallel (must come before resolving path conflicts and
        # getting)
        if rustworkers:
            # Removing lots of files very quickly is known to cause FSEvents to
            # lose events which forces watchman to recrwawl the entire
            # repository. For very large repository, this can take many
            # minutes, slowing down all the other tools that rely on it. Thus
            # add a config that can be tweaked to specifically reduce the
            # amount of concurrency.
            numworkers = repo.ui.configint(
                ""experimental"", ""numworkersremover"", worker._numworkers(repo.ui)
            )
            remover = rustworker.removerworker(repo.wvfs.base, numworkers)
            for f, args, msg in actions[""r""] + actions[""rg""]:
                # The remove method will either return immediately or block if
                # the internal worker queue is full.
                remover.remove(f)
                z += 1
                prog.value = (z, f)
            retry = remover.wait()
            for f in retry:
                repo.ui.debug(""retrying %s\n"" % f)
                removeone(repo, wctx, f)
        else:
            for i, size, item in batchremove(repo, wctx, actions[""r""] + actions[""rg""]):
                z += i
                prog.value = (z, item)
        # ""rg"" actions are counted in updated below
        removed = len(actions[""r""])

        # resolve path conflicts (must come before getting)
        for f, args, msg in actions[""pr""]:
            repo.ui.debug("" %s: %s -> pr\n"" % (f, msg))
            (f0,) = args
            if wctx[f0].lexists():
                repo.ui.note(_(""moving %s to %s\n"") % (f0, f))
                wctx[f].audit()
                wctx[f].write(wctx.filectx(f0).data(), wctx.filectx(f0).flags())
                wctx[f0].remove()
            z += 1
            prog.value = (z, f)

        # get in parallel
        writesize = 0

        if rustworkers:
            numworkers = repo.ui.configint(
                ""experimental"", ""numworkerswriter"", worker._numworkers(repo.ui)
            )

            writer = rustworker.writerworker(
                repo.fileslog.contentstore, repo.wvfs.base, numworkers
            )
            fctx = mctx.filectx
            for f, (flags, backup), msg in actions[""g""] + actions[""rg""]:
                fnode = fctx(f).filenode()
                # The write method will either return immediately or block if
                # the internal worker queue is full.
                writer.write(f, fnode, flags)

                z += 1
                prog.value = (z, f)

            writesize, retry = writer.wait()
            for f, flag in retry:
                repo.ui.debug(""retrying %s\n"" % f)
                writesize += updateone(repo, fctx, wctx, f, flag)
        else:
            for i, size, item in batchget(
                repo, mctx, wctx, actions[""g""] + actions[""rg""]
            ):
                z += i
                writesize += size
                prog.value = (z, item)
        updated = len(actions[""g""]) + len(actions[""rg""])
        perftrace.tracebytes(""Disk Writes"", writesize)

        # forget (manifest only, just log it) (must come first)
        for f, args, msg in actions[""f""]:
            repo.ui.debug("" %s: %s -> f\n"" % (f, msg))
            z += 1
            prog.value = (z, f)

        # re-add (manifest only, just log it)
        for f, args, msg in actions[""a""]:
            repo.ui.debug("" %s: %s -> a\n"" % (f, msg))
            z += 1
            prog.value = (z, f)

        # re-add/mark as modified (manifest only, just log it)
        for f, args, msg in actions[""am""]:
            repo.ui.debug("" %s: %s -> am\n"" % (f, msg))
            z += 1
            prog.value = (z, f)

        # keep (noop, just log it)
        for f, args, msg in actions[""k""]:
            repo.ui.debug("" %s: %s -> k\n"" % (f, msg))
            # no progress

        # directory rename, move local
        for f, args, msg in actions[""dm""]:
            repo.ui.debug("" %s: %s -> dm\n"" % (f, msg))
            z += 1
            prog.value = (z, f)
            f0, flags = args
            repo.ui.note(_(""moving %s to %s\n"") % (f0, f))
            wctx[f].audit()
            wctx[f].write(wctx.filectx(f0).data(), flags)
            wctx[f0].remove()
            updated += 1

        # local directory rename, get
        for f, args, msg in actions[""dg""]:
            repo.ui.debug("" %s: %s -> dg\n"" % (f, msg))
            z += 1
            prog.value = (z, f)
            f0, flags = args
            repo.ui.note(_(""getting %s to %s\n"") % (f0, f))
            wctx[f].write(mctx.filectx(f0).data(), flags)
            updated += 1

        # exec
        for f, args, msg in actions[""e""]:
            repo.ui.debug("" %s: %s -> e\n"" % (f, msg))
            z += 1
            prog.value = (z, f)
            (flags,) = args
            wctx[f].audit()
            wctx[f].setflags(""l"" in flags, ""x"" in flags)
            updated += 1

        perftrace.tracevalue(""Deleted Files"", removed)
        perftrace.tracevalue(""Written Files"", updated)

        # the ordering is important here -- ms.mergedriver will raise if the
        # merge driver has changed, and we want to be able to bypass it when
        # overwrite is True
        usemergedriver = not overwrite and mergeactions and ms.mergedriver

        if usemergedriver:
            ms.commit()
            with repo.ui.timesection(""mergedriver""):
                # This will return False if the function raises an exception.
                failed = not driverpreprocess(repo, ms, wctx, labels=labels)
            driverresolved = [f for f in ms.driverresolved()]

            repo.ui.log(""command_metrics"", mergedriver_num_files=len(driverresolved))

            # If preprocess() marked any files as driver-resolved and we're
            # merging in-memory, abort on the assumption that driver scripts
            # require the working directory.
            if driverresolved and wctx.isinmemory():
                errorstr = (
                    ""some of your files require mergedriver to run, ""
                    ""which in-memory merge does not support""
                )
                raise error.InMemoryMergeConflictsError(
                    errorstr,
                    type=error.InMemoryMergeConflictsError.TYPE_MERGEDRIVER,
                    paths=driverresolved,
                )

            # NOTE(phillco): This used to say ""the driver might leave some files unresolved"",
            # but this actually only handles the case where preprocess() fails. A preprocess()
            # script can also leave files unmarked without failing.
            unresolvedf = set(ms.unresolved())
            if failed:
                # Preprocess failed, so don't proceed in either case.
                if wctx.isinmemory():
                    raise error.InMemoryMergeConflictsError(
                        ""preprocess() raised an exception"",
                        type=error.InMemoryMergeConflictsError.TYPE_FILE_CONFLICTS,
                        paths=list(unresolvedf),
                    )
                else:
                    # XXX setting unresolved to at least 1 is a hack to make sure we
                    # error out
                    return updated, merged, removed, max(len(unresolvedf), 1)
            newactions = []
            for f, args, msg in mergeactions:
                if f in unresolvedf:
                    newactions.append((f, args, msg))
            mergeactions = newactions

        try:
            # premerge
            tocomplete = []
            completed = []
            for f, args, msg in mergeactions:
                repo.ui.debug("" %s: %s -> m (premerge)\n"" % (f, msg))
                z += 1
                prog.value = (z, f)
                wctx[f].audit()
                complete, r = ms.preresolve(f, wctx)
                if not complete:
                    numupdates += 1
                    tocomplete.append((f, args, msg))
                else:
                    completed.append(f)

            # merge
            files = []
            for f, args, msg in tocomplete:
                repo.ui.debug("" %s: %s -> m (merge)\n"" % (f, msg))
                z += 1
                prog.value = (z, f)
                ms.resolve(f, wctx)
                files.append(f)
            reponame = repo.ui.config(""fbscmquery"", ""reponame"")
            command = "" "".join(util.shellquote(a) for a in pycompat.sysargv)
            repo.ui.log(
                ""manualmergefiles"",
                manual_merge_files="","".join(files),
                auto_merge_files="","".join(completed),
                command=command,
                repo=reponame,
            )

        finally:
            ms.commit()

        unresolved = ms.unresolvedcount()

        if usemergedriver and not unresolved and ms.mdstate() != ""s"":
            with repo.ui.timesection(""mergedriver""):
                if not driverconclude(repo, ms, wctx, labels=labels):
                    # XXX setting unresolved to at least 1 is a hack to make
                    # sure we error out
                    unresolved = max(unresolved, 1)

            ms.commit()

        msupdated, msmerged, msremoved = ms.counts()
        updated += msupdated
        merged += msmerged
        removed += msremoved

        extraactions = ms.actions()
        if extraactions:
            # A same file might exist both in extraactions[""r""] (to remove)
            # list, and actions[""g""] (to create) list. Remove them from
            # actions[""g""] to avoid conflicts.
            extraremoved = {item[0] for item in extraactions[""r""]}
            if extraremoved:
                actions[""g""] = [
                    item for item in actions[""g""] if item[0] not in extraremoved
                ]

            mfiles = set(a[0] for a in actions[""m""])
            for k, acts in pycompat.iteritems(extraactions):
                actions[k].extend(acts)
                # Remove these files from actions['m'] as well. This is
                # important because in recordupdates, files in actions['m'] are
                # processed after files in other actions, and the merge driver
                # might add files to those actions via extraactions above. This
                # can lead to a file being recorded twice, with poor results.
                # This is especially problematic for actions['r'] (currently
                # only possible with the merge driver in the initial merge
                # process; interrupted merges don't go through this flow).
                #
                # The real fix here is to have indexes by both file and action
                # so that when the action for a file is changed it is
                # automatically reflected in the other action lists. But that
                # involves a more complex data structure, so this will do for
                # now.
                #
                # We don't need to do the same operation for 'dc' and 'cd'
                # because those lists aren't consulted again.
                mfiles.difference_update(a[0] for a in acts)

            actions[""m""] = [a for a in actions[""m""] if a[0] in mfiles]

    return updated, merged, removed, unresolved"
264,"    def read(self, size=None):
        # type (Optional[int]) -> bytes
        """"""read payload data""""""","        if not self._initialized:
            self._readheader()
        if size is None:
            data = self._payloadstream.read()
        else:
            data = self._payloadstream.read(size)
        self._pos += len(data)
        if size is None or len(data) < size:
            if not self.consumed and self._pos:
                self.ui.debug(""bundle2-input-part: total payload size %i\n"" % self._pos)
            self.consumed = True
        return data"
265,"    def nodes(self, expr, *args):
        """"""Find revisions matching a revset and emit their nodes.

        This is a convenience wrapper around ``revs()`` that iterates the
        result and is a generator of nodes.

        Revset aliases from the configuration are not expanded. To expand
        user aliases, consider calling ``scmutil.revrange()``.
        """"""","        clnode = self.changelog.node
        for r in self.revs(expr, *args):
            yield clnode(r)"
266,"    def getID(self):
        """"""
        Return the name of the world, typically the task the world encodes.
        """"""",        return self.id
267,"def add_mask_rcnn_outputs(model, blob_in, dim):
    """"""Add Mask R-CNN specific outputs: either mask logits or probs.""""""","    num_cls = cfg.MODEL.NUM_CLASSES if cfg.MRCNN.CLS_SPECIFIC_MASK else 1

    if cfg.MRCNN.USE_FC_OUTPUT:
        # Predict masks with a fully connected layer (ignore 'fcn' in the blob
        # name)
        dim_fc = int(dim * (cfg.MRCNN.RESOLUTION / cfg.MRCNN.UPSAMPLE_RATIO)**2)
        blob_out = model.FC(
            blob_in,
            'mask_fcn_logits',
            dim_fc,
            num_cls * cfg.MRCNN.RESOLUTION**2,
            weight_init=gauss_fill(0.001),
            bias_init=const_fill(0.0)
        )
    else:
        # Predict mask using Conv

        # Use GaussianFill for class-agnostic mask prediction; fills based on
        # fan-in can be too large in this case and cause divergence
        fill = (
            cfg.MRCNN.CONV_INIT
            if cfg.MRCNN.CLS_SPECIFIC_MASK else 'GaussianFill'
        )
        blob_out = model.Conv(
            blob_in,
            'mask_fcn_logits',
            dim,
            num_cls,
            kernel=1,
            pad=0,
            stride=1,
            weight_init=(fill, {'std': 0.001}),
            bias_init=const_fill(0.0)
        )

        if cfg.MRCNN.UPSAMPLE_RATIO > 1:
            blob_out = model.BilinearInterpolation(
                'mask_fcn_logits', 'mask_fcn_logits_up', num_cls, num_cls,
                cfg.MRCNN.UPSAMPLE_RATIO
            )

    if not model.train:  # == if test
        blob_out = model.net.Sigmoid(blob_out, 'mask_fcn_probs')

    return blob_out"
268,"    def isancestor(self, a, b):
        """"""Test if a (in node) is an ancestor of b (in node)""""""","        if a == nullid or b == nullid:
            return False
        return self.dag.isancestor(a, b)"
269,"    def _isgooddelta(self, d, textlen):
        """"""Returns True if the given delta is good. Good means that it is within
        the disk span, disk size, and chain length bounds that we know to be
        performant.""""""","        if d is None:
            return False

        # - 'compresseddeltalen' is the sum of the total size of deltas we need
        #   to apply -- bounding it limits the amount of CPU we consume.
        _dist, l, data, base, chainbase, chainlen, compresseddeltalen = d

        # Criteria:
        # 1. the delta is not larger than the full text
        # 2. the delta chain cumulative size is not greater than twice the
        #    fulltext
        # 3. The chain length is less than the maximum
        #
        # This differs from upstream Mercurial's criteria. They prevent the
        # total ondisk span from chain base to rev from being greater than 4x
        # the full text len. This isn't good enough in our world since if we
        # have 10+ branches going on at once, we can easily exceed the 4x limit
        # and cause full texts to be written over and over again.
        if (
            l > textlen
            or compresseddeltalen > textlen * 2
            or (self._maxchainlen and chainlen > self._maxchainlen)
        ):
            return False

        return True"
270,"    def _build_image_features_dict(self, data_path, dt, store_dict_path):
        """"""
        Build resne(x)t image features with ImageLoader.

        (Or anything handleable by ImageLoader) and save to path. Only called if we
        haven't already built the dict before.
        """"""","        image_features_dict = {}
        total = len(self.data)
        import tqdm

        pbar = tqdm.tqdm(
            total=total,
            unit='cand',
            unit_scale=True,
            desc='Building image features dict for %s with ImageLoader.'
            % self.image_mode,
        )
        num = 0
        for ex in self.data:
            img_id = ex[self.image_id_key]
            img_path = self.image_id_to_image_path(img_id)
            image = self.image_loader.load(img_path).detach()
            # spatial features are [1, image_dim, spatial_dim, spatial_dim] tensors.
            # reduce non-spatial features to one-dimensional feature prior to saving.
            if not self.image_loader.is_spatial(self.image_mode):
                image = image[0, :, 0, 0]
            image_features_dict[img_id] = image
            num += 1
            pbar.update(1)
            if num % 1000 == 0:
                logging.debug(f'Processing image index: {num}')
        torch_utils.atomic_save(image_features_dict, store_dict_path)
        return image_features_dict"
271,"def registercommand():
    """"""register the fastannotate command""""""","    name = ""fastannotate|fastblame|fa""
    command(name, **fastannotatecommandargs)(fastannotate)"
272,"    def testSplit(self):
        """"""Test FramedTransport and BinaryProtocolAccelerated

        Tests that TBinaryProtocolAccelerated and TFramedTransport
        play nicely together when a read spans a frame""""""","
        protocol_factory = TBinaryProtocol.TBinaryProtocolAcceleratedFactory()
        bigstring = """".join(chr(byte) for byte in range(ord(""a""), ord(""z"") + 1))

        databuf = TTransport.TMemoryBuffer()
        prot = protocol_factory.getProtocol(databuf)
        prot.writeI32(42)
        prot.writeString(bigstring)
        prot.writeI16(24)
        data = databuf.getvalue()
        cutpoint = len(data) // 2
        parts = [data[:cutpoint], data[cutpoint:]]

        framed_buffer = TTransport.TMemoryBuffer()
        framed_writer = TTransport.TFramedTransport(framed_buffer)
        for part in parts:
            framed_writer.write(part)
            framed_writer.flush()
        self.assertEquals(len(framed_buffer.getvalue()), len(data) + 8)

        # Recreate framed_buffer so we can read from it.
        framed_buffer = TTransport.TMemoryBuffer(framed_buffer.getvalue())
        framed_reader = TTransport.TFramedTransport(framed_buffer)
        prot = protocol_factory.getProtocol(framed_reader)
        self.assertEqual(prot.readI32(), 42)
        bytes_comp(self, prot.readString(), bigstring)
        self.assertEqual(prot.readI16(), 24)"
273,"    def __init__(self, **kwargs):
        """"""
        NOTE: this interface is experimental.
        """"""","        super().__init__(**kwargs)
        assert (
            not self.mask_on and not self.keypoint_on
        ), ""Mask/Keypoints not supported in Rotated ROIHeads.""
        assert not self.train_on_pred_boxes, ""train_on_pred_boxes not implemented for RROIHeads!"""
274,"def _sendmail(ui, sender: str, recipients, msg) -> None:
    """"""send mail using sendmail.""""""","    program = ui.config(""email"", ""method"")
    cmdline = ""%s -f %s %s"" % (
        program,
        util.email(sender),
        "" "".join(map(util.email, recipients)),
    )
    ui.note(_(""sending mail: %s\n"") % cmdline)
    fp = util.popen(cmdline, ""w"")
    fp.write(msg)
    ret = fp.close()
    if ret:
        raise error.Abort(
            ""%s %s""
            % (os.path.basename(program.split(None, 1)[0]), util.explainexit(ret)[0])
        )"
275,"    def _queue_action(self, action, act_id, act_data=None):
        """"""
        Add an action to the queue with given id and info if it hasn't already been
        seen.

        :param action:
            action to be added to message queue
        :param act_id:
            an identifier to check if the action has been seen or to
            mark the action as seen
        :param act_data:
            any data about the given action you may want to record when
            marking it as seen
        """"""","        if act_id not in self.acted_packets:
            self.acted_packets[act_id] = act_data
            self.msg_queue.put(action)"
276,"def requirements_from_file(filename, builder=None):
  """"""Return a list of :class:`Resolvable` objects from a requirements.txt file.

  :param filename: The filename of the requirements.txt
  :keyword builder: (optional) The ResolverOptionsBuilder from which we should inherit
    default resolver options.
  :type builder: :class:`ResolverOptionsBuilder`
  """"""","
  relpath = os.path.dirname(filename)
  with open(filename, 'r') as fp:
    return requirements_from_lines(fp.readlines(), builder=builder, relpath=relpath)"
277,"def unpack_into(snapshot_path: Path, output_path: Path) -> BaseSnapshot:
    """"""Unpack a snapshot into the specified output directory.

    Returns the appropriate BaseSnapshot subclass for this snapshot.
    """"""","    # GNU tar is smart enough to automatically figure out the correct
    # decompression method.
    untar_cmd = [""tar"", ""-xf"", str(snapshot_path.absolute())]
    subprocess.check_call(untar_cmd, cwd=output_path)

    data_dir = output_path / ""data""
    try:
        with (data_dir / ""info.json"").open(""r"") as info_file:
            info = json.load(info_file)

        type_name = info[""type""]
        snapshot_type = snapshot_types.get(type_name)
        if snapshot_type is None:
            raise UnknownSnapshotTypeError(type_name)

        # pyre-fixme[45]: Cannot instantiate abstract class `BaseSnapshot`.
        snapshot = snapshot_type(output_path)
        snapshot.resume()
        return snapshot
    except Exception:
        cleanup_tmp_dir(data_dir)
        raise"
278,"    def __init__(self, cfg: CfgNode):
        """"""
        Initialize embedding loss from config
        """"""",        self.embdist_gauss_sigma = cfg.MODEL.ROI_DENSEPOSE_HEAD.CSE.EMBEDDING_DIST_GAUSS_SIGMA
279,"    def _genCounterCallbacks(self):
        """"""Yields this item's (names, value) counter tuple(s).""""""",        raise NotImplementedError()
280,"    def load(self, filename):
        """"""
        Load pre-existing dictionary in 'token[<TAB>count]' format.

        Initialize counts from other dictionary, or 0 if they aren't included.
        """"""","        print('Dictionary: loading dictionary from {}'.format(filename))
        with PathManager.open(filename) as read:
            for line in read:
                split = line.strip().split('\t')
                token = unescape(split[0])
                cnt = int(split[1]) if len(split) > 1 else 0
                self.freq[token] = cnt
                if token not in self.tok2ind:
                    index = len(self.tok2ind)
                    self.tok2ind[token] = index
                    self.ind2tok[index] = token
        print('[ num ques words =  %d ]' % len(self.ind2tok))

        with PathManager.open(filename[:-5] + ""_ans.dict"") as read:
            for line in read:
                split = line.strip().split('\t')
                token = unescape(split[0])
                cnt = int(split[1]) if len(split) > 1 else 0
                self.ansfreq[token] = cnt
                if token not in self.ans2ind:
                    index = len(self.ans2ind)
                    self.ans2ind[token] = index
                    self.ind2ans[index] = token

        print('[ num ans words =  %d ]' % len(self.ind2ans))"
281,"    def _set_label_cands_vec(self, *args, **kwargs):
        """"""
        Set the 'label_candidates_vec' field in the observation.

        Useful to override to change vectorization behavior.
        """"""","        obs = args[0]
        if 'labels' in obs:
            cands_key = 'candidates'
        else:
            cands_key = 'eval_candidates'
        if self.opt[cands_key] not in ['inline', 'batch-all-cands']:
            # vectorize label candidates if and only if we are using inline
            # candidates
            return obs
        return super()._set_label_cands_vec(*args, **kwargs)"
282,"    def _lookup_converter(cls, from_type: Type) -> Any:
        """"""
        Perform recursive lookup for the given type
        to find registered converter. If a converter was found for some base
        class, it gets registered for this class to save on further lookups.

        Args:
            from_type: type for which to find a converter
        Return:
            callable or None - registered converter or None
                if no suitable entry was found in the registry
        """"""","        if from_type in cls.registry:  # pyre-ignore[16]
            return cls.registry[from_type]
        for base in from_type.__bases__:
            converter = cls._lookup_converter(base)
            if converter is not None:
                cls._do_register(from_type, converter)
                return converter
        return None"
283,"    def get_sint8(self, fail_value=0):
        """"""Extract a int8_t from the current file position.""""""","        s = self.read_size(1)
        return self._unpack(""b"", s) if s else fail_value"
284,"def CheckBraces(filename, clean_lines, linenum, error):
  """"""Looks for misplaced braces (e.g. at the end of line).

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    error: The function to call with any errors found.
  """"""","
  line = clean_lines.elided[linenum]        # get rid of comments and strings

  if Match(r'\s+{\s*$', line):
    # We allow an open brace to start a line in the case where someone
    # is using braces in a block to explicitly create a new scope,
    # which is commonly used to control the lifetime of
    # stack-allocated variables.  We don't detect this perfectly: we
    # just don't complain if the last non-whitespace character on the
    # previous non-blank line is ';', ':', '{', or '}', or if the previous
    # line starts a preprocessor block.
    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]
    if (not Search(r'[;:}{]\s*$', prevline) and
        not Match(r'static\s+', prevline) and
        not Match(r'\s*#', prevline)):
      error(filename, linenum, 'whitespace/braces', 4,
            '{ should almost always be at the end of the previous line')

  # An else clause should be on the same line as the preceding closing brace.
  if Match(r'\s*else\s*', line):
    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]
    if Match(r'\s*}\s*$', prevline):
      error(filename, linenum, 'whitespace/newline', 4,
            'An else should appear on the same line as the preceding }')

  # Don't use a redundant ""return;"" at the bottom of a void function
  if line == '}':
    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]
    if Match(r'\s*return\s*;', prevline):
      error(filename, linenum, 'readability/braces', 4,
          ""Remove this redundant return statement"")

  # If braces come on one side of an else, they should be on both.
  # However, we have to worry about ""else if"" that spans multiple lines!
  if Search(r'}\s*else[^{]*$', line) or Match(r'[^}]*else\s*{', line):
    if Search(r'}\s*else if([^{]*)$', line):       # could be multi-line if
      # find the ( after the if
      pos = line.find('else if')
      pos = line.find('(', pos)
      if pos > 0:
        (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos)
        if endline[endpos:].find('{') == -1:    # must be brace after if
          error(filename, linenum, 'readability/braces', 5,
                'If an else has a brace on one side, it should have it on both')
    else:            # common case: else not followed by a multi-line if
      error(filename, linenum, 'readability/braces', 5,
            'If an else has a brace on one side, it should have it on both')

  # Likewise, an else should never have the else clause on the same line
  if Search(r'\belse [^\s{]', line) and not Search(r'\belse if\b', line):
    error(filename, linenum, 'whitespace/newline', 4,
          'Else clause should never be on same line as else (use 2 lines)')

  # In the same way, a do/while should never be on one line
  if Match(r'\s*do [^\s{]', line):
    error(filename, linenum, 'whitespace/newline', 4,
          'do/while clauses should not be on a single line')

  # Braces shouldn't be followed by a ; unless they're defining a struct
  # or initializing an array.
  # We can't tell in general, but we can for some common cases.
  prevlinenum = linenum
  while True:
    (prevline, prevlinenum) = GetPreviousNonBlankLine(clean_lines, prevlinenum)
    if Match(r'\s+{.*}\s*;', line) and not prevline.count(';'):
      line = prevline + line
    else:
      break
  if (Search(r'{.*}\s*;', line) and
      line.count('{') == line.count('}') and
      not Search(r'struct|class|enum|\s*=\s*{', line)):
    error(filename, linenum, 'readability/braces', 4,
          ""You don't need a ; after a }"")"
285,"    def checkout_path(self) -> Path:
        """"""Return the path to the checkout root.""""""","        return self.data_dir / ""checkout"""
286,"    def hit_point(self):
        """"""
        :return: XCUIElement hit point
        :rtype: lldb.SBValue
        """"""","        import_uikit()
        return fb.evaluateExpressionValue(
            ""(CGPoint)[{} hitPoint]"".format(self.element_value)
        )"
287,"    def create(self, *args, **kwargs):
        """"""Create all registered tasks.

        TODO: Handle SkipTask?
        """"""","        assert not self._did_create
        for task_cls in self._registered:
            task = task_cls(*args, **kwargs)
            self._created.append(task)
            self._created_names[task_cls.__name__] = task

        self._did_create = True"
288,"def push_tap(git_repository, tap_path, version, github_token):
    """"""
    Grab any working directory changes for the tap, clone a new tap repository,
    and push those changes upstream. The original tap path is in a clean state
    after this push. The clone is done with ssh, so ssh keys must be available

    Args:
        git_repository: The repo on github that needs to be cloned/pushed to
        tap_path: The directory that the tap (with changes) exists in
        version: The version to use in commit messages
    """"""","    logging.info(""Gathering git diff from {}"".format(tap_path))
    git_diff = run([""git"", ""diff""], tap_path, True).stdout
    user = get_current_user(github_token)
    git_url = ""https://{}:{}@github.com/{}.git"".format(user[""login""], github_token, git_repository)

    with tempfile.TemporaryDirectory() as temp_dir:
        logging.info(""Cloning {} into {}"".format(git_url, temp_dir))
        run([""git"", ""clone"", git_url, temp_dir])

        logging.info(""Cloned into {}. Applying patch"".format(temp_dir))
        run([""git"", ""apply"", ""-""], temp_dir, input=git_diff)

        logging.info(""Committing..."")
        with tempfile.NamedTemporaryFile() as fout:
            commit_message = (
                ""Bump buck to version {}\n\nThis commit was generated by ""
                ""release automation\n""
            ).format(version)
            fout.write(commit_message.encode(""utf-8""))
            fout.flush()
            run([""git"", ""commit"", ""-F"", fout.name, ""buck.rb""], temp_dir)

        logging.info(""Pushing commit upstream"")
        run([""git"", ""push"", ""origin""], temp_dir)
        logging.info(""Pushed commit upstream!"")

    logging.info(""Resetting state of {}, and updating it after push"".format(tap_path))
    run([""git"", ""checkout"", ""buck.rb""], tap_path)
    run([""git"", ""checkout"", ""master""], tap_path)
    run([""git"", ""pull""], tap_path)
    logging.info(""Reset state of {}, and updating it after push"".format(tap_path))"
289,"    def log(self, fname, lineno, line, msg, blame):
        """"""print error related a to given line of a given file.

        The faulty line will also be printed but only once in the case
        of multiple errors.

        :fname: filename
        :lineno: line number
        :line: actual content of the line
        :msg: error message
        """"""","        msgid = fname, lineno, line
        if msgid != self._lastseen:
            if blame:
                print(
                    ""%s:%d (%s): %s --> %s"" % (fname, lineno, blame, msg, line.strip())
                )
            else:
                print(""%s:%d: %s --> %s"" % (fname, lineno, msg, line.strip()))
            self._lastseen = msgid"
290,"def cmd(name, help=None, cmd_table=CmdTable):
    """"""
    @cmd() is a decorator that can be used to help define Subcmd instances

    Example usage:

        @subcmd('list', 'Show the result list')
        class ListCmd(Subcmd):
            def run(self, args):
                # Perform the command actions here...
                pass
    """"""","
    def wrapper(cls):
        class SubclassedCmd(cls):
            NAME = name
            HELP = help

        cmd_table.append(SubclassedCmd)
        return SubclassedCmd

    return wrapper"
291,"def ResetNolintSuppressions():
  ""Resets the set of NOLINT suppressions to empty.""",  _error_suppressions.clear()
292,"    def _set_model_opt(self):
        """"""
        read and save model.opt as an attribute Also updates the model with the
        overridden sections for easier access later on.

        Also updates the --model if it's not updated
        """"""","        # reading model.opt
        fopt = self.opt['model_file'] + '.opt'
        if not os.path.isfile(fopt):
            raise RuntimeError(f""The {fopt} can't be found"")
        try:
            with open(fopt, 'rb') as f:
                model_opt = json.load(f)
            self.model_opt = Opt(model_opt)
        except UnicodeDecodeError:
            raise RuntimeError(fopt + "" isn't in the expected format"")

        # override with the override field
        self.model_opt.update(self.model_opt.get('override', {}))

        # make sure that if there's a classes, then there should be a classes_from_file
        if 'classes' in self.model_opt and 'classes_from_file' not in self.model_opt:
            self.model_opt['classes_from_file'] = None

        if self.verbose:
            extra_special_print('model.opt')
            self.model_opt.log()
        # for cases where the model wasn't updated, like transformer_classifier
        if self.opt['model']:
            self.model_opt['model'] = self.opt['model']"
293,"    def __unicode__(self):
        """"""
        Returns the unicode representation of the file.
        """"""","        ret = []
        entries = [self.metadata_as_entry()] + [e for e in self if not e.obsolete]
        for entry in entries:
            ret.append(entry.__unicode__(self.wrapwidth))
        for entry in self.obsolete_entries():
            ret.append(entry.__unicode__(self.wrapwidth))
        ret = ""\n"".join(ret)

        if type(ret) != types.UnicodeType:
            return unicode(ret, self.encoding)  # noqa
        return ret"
294,"    def override_special_tokens(opt):
        """"""
        Override the special tokens for your tokenizer.
        """"""",        pass
295,"def gitignore(mctx, x):
    """"""File that matches the active .gitignore pattern.""""""","    # i18n: ""gitignore"" is a keyword
    getargs(x, 0, 0, _(""gitignore takes no arguments""))
    repo = mctx.ctx.repo()
    ignore = repo.dirstate._ignore
    return mctx.matches(ignore)"
296,"    def __len__(self):
        """"""Returns the number of created or registered tasks, as appropriate""""""",        return len(self.tasks)
297,"def shelvecmd(ui, repo, *pats, **opts):
    """"""save pending changes and revert working copy to a clean state

    Shelving takes files that :prog:`status` reports as not clean, saves
    the modifications to a bundle (a shelved change), and reverts the
    files to a clean state in the working copy.

    To restore the changes to the working copy, using :prog:`unshelve`,
    regardless of your current commit.

    When no files are specified, :prog:`shelve` saves all not-clean
    files. If specific files or directories are named, only changes to
    those files are shelved.

    Each shelved change has a name that makes it easier to find later.
    The name of a shelved change by default is based on the active
    bookmark. To specify a different name, use ``--name``.

    To see a list of existing shelved changes, use the ``--list``
    option. For each shelved change, this will print its name, age,
    and description. Use ``--patch`` or ``--stat`` for more details.

    To delete specific shelved changes, use ``--delete``. To delete
    all shelved changes, use ``--cleanup``.

    Returns 0 on success.
    """"""","    allowables = [
        (""addremove"", {""create""}),  # 'create' is pseudo action
        (""unknown"", {""create""}),
        (""cleanup"", {""cleanup""}),
        #       ('date', {'create'}), # ignored for passing '--date ""0 0""' in tests
        (""delete"", {""delete""}),
        (""edit"", {""create""}),
        (""list"", {""list""}),
        (""message"", {""create""}),
        (""name"", {""create""}),
        (""patch"", {""patch"", ""list""}),
        (""stat"", {""stat"", ""list""}),
    ]

    def checkopt(opt):
        if opts.get(opt):
            for i, allowable in allowables:
                if opts[i] and opt not in allowable:
                    raise error.Abort(
                        _(""options '--%s' and '--%s' may not be "" ""used together"")
                        % (opt, i)
                    )
            return True

    if checkopt(""cleanup""):
        if pats:
            raise error.Abort(_(""cannot specify names when using '--cleanup'""))
        return cleanupcmd(ui, repo)
    elif checkopt(""delete""):
        return deletecmd(ui, repo, pats)
    elif checkopt(""list""):
        return listcmd(ui, repo, pats, opts)
    elif checkopt(""patch""):
        return patchcmds(ui, repo, pats, opts, subcommand=""patch"")
    elif checkopt(""stat""):
        return patchcmds(ui, repo, pats, opts, subcommand=""stat"")
    else:
        return createcmd(ui, repo, pats, opts)"
298,"    def url(self):
        """"""Returns a URL string representing this peer.

        Currently, implementations expose the raw URL used to construct the
        instance. It may contain credentials as part of the URL. The
        expectations of the value aren't well-defined and this could lead to
        data leakage.

        TODO audit/clean consumers and more clearly define the contents of this
        value.
        """"""",
299,"    def add_additional_special_tokens(self, additional_special_tokens: List[str]):
        """"""
        Add additional special tokens to the dictionary.
        """"""","        self.additional_special_tokens = additional_special_tokens
        self.hf_tokenizer.add_special_tokens(
            {'additional_special_tokens': additional_special_tokens}
        )
        for tok in self.additional_special_tokens:
            self.add_token(tok)"
300,"def debuglfsdownload(ui, repo, *pats, **opts):
    """"""calculate the LFS download size when updating between REV1 and REV2

    If --no-sparse is provided, this operation would ignore any sparse
    profile that might be present and report data for the full checkout.

    With -v also prints which files are to be downloaded and the size of
    each file.""""""","    revs = opts.get(""rev"")

    node1, node2 = scmutil.revpair(repo, revs)
    match = lambda s: True
    if not opts.get(""sparse""):
        ui.debug(""will ignore sparse profile in this repo\n"")
    else:
        if not util.safehasattr(repo, ""sparsematch""):
            raise error.Abort(
                _(""--ignore-sparse makes no sense in a non-sparse"" "" repository"")
            )
        match = repo.sparsematch(node2)

    with ui.configoverride({(""remotefilelog"", ""dolfsprefetch""): False}):
        ctx1, ctx2 = repo[node1], repo[node2]
        mfdiff = ctx2.manifest().diff(ctx1.manifest())
        lfsflogs = util.sortdict()  # LFS filelogs
        for fname in mfdiff:
            if not match(fname):
                continue
            flog = repo.file(fname)
            try:
                node = ctx2.filenode(fname)
            except error.ManifestLookupError:
                continue
            if wrapper._islfs(flog, node=node):
                lfsflogs[fname] = flog

        totalsize = 0
        presentsize = 0
        for fname, flog in lfsflogs.items():
            rawtext = flog.revision(ctx2.filenode(fname), raw=True)
            p = pointer.deserialize(rawtext)
            present = repo.svfs.lfslocalblobstore.has(p.oid())
            lfssize = int(p[""size""])
            ui.note(_(""%s: %i (present=%r)\n"") % (fname, lfssize, present))
            totalsize += lfssize
            presentsize += lfssize if present else 0
        ui.status(
            _(""Total size: %i, to download: %i, already exists: %r\n"")
            % (totalsize, totalsize - presentsize, presentsize)
        )"
301,"def add_fpn_onto_conv_body(
    model, conv_body_func, fpn_level_info_func, P2only=False
):
    """"""Add the specified conv body to the model and then add FPN levels to it.
    """"""","    # Note: blobs_conv is in revsersed order: [fpn5, fpn4, fpn3, fpn2]
    # similarly for dims_conv: [2048, 1024, 512, 256]
    # similarly for spatial_scales_fpn: [1/32, 1/16, 1/8, 1/4]

    conv_body_func(model)
    blobs_fpn, dim_fpn, spatial_scales_fpn = add_fpn(
        model, fpn_level_info_func()
    )

    if P2only:
        # use only the finest level
        return blobs_fpn[-1], dim_fpn, spatial_scales_fpn[-1]
    else:
        # use all levels
        return blobs_fpn, dim_fpn, spatial_scales_fpn"
302,"def parse(version):
    """"""
    Parse the given version string and return either a :class:`Version` object
    or a :class:`LegacyVersion` object depending on if the given version is
    a valid PEP 440 version or a legacy version.
    """"""","    try:
        return Version(version)
    except InvalidVersion:
        return LegacyVersion(version)"
303,"    def get_frontend_args(self) -> Dict[str, Any]:
        """"""
        Specifies what options within a task_config should be forwarded to the client
        for use by the task's frontend.
        """"""","        return {
            ""task_description"": ""Placeholder Task Description - Javascript failed to load"",
            ""frame_height"": 650,
            ""num_subtasks"": self.args.blueprint.subtasks_per_unit,
            ""question"": self.args.blueprint.eval_question,
            ""block_mobile"": True,
            ""get_task_feedback"": False,  # TODO(#95) make option
            ""additional_task_description"": self.args.blueprint.additional_task_description,
        }"
304,"    def replacement_decode(self, codes):
        """"""Reconstruct an approximation of vectors given their codes.

        Parameters
        ----------
        codes : array_like
            Codes to decode, shape (n, self.code_size). `dtype` must be uint8.

        Returns
        -------
            Reconstructed vectors for each code, shape `(n, d)` and `dtype` float32.
        """"""","        n, cs = codes.shape
        codes = _check_dtype_uint8(codes)
        assert cs == self.code_size
        x = np.empty((n, self.d), dtype='float32')
        self.decode_c(swig_ptr(codes), swig_ptr(x), n)
        return x"
305,"    def busy(self):
        """"""Returns True if this connection object is currently in use.

        If a response is still pending, this will return True, even if
        the request has finished sending. In the future,
        HTTPConnection may transparently juggle multiple connections
        to the server, in which case this will be useful to detect if
        any of those connections is ready for use.
        """"""","        cr = self._current_response
        if cr is not None:
            if self._current_response_taken:
                if cr.will_close:
                    self.sock = None
                    self._current_response = None
                    return False
                elif cr.complete():
                    self._current_response = None
                    return False
            return True
        return False"
306,"def transform_proposals(dataset_dict, image_shape, transforms, *, proposal_topk, min_box_size=0):
    """"""
    Apply transformations to the proposals in dataset_dict, if any.

    Args:
        dataset_dict (dict): a dict read from the dataset, possibly
            contains fields ""proposal_boxes"", ""proposal_objectness_logits"", ""proposal_bbox_mode""
        image_shape (tuple): height, width
        transforms (TransformList):
        proposal_topk (int): only keep top-K scoring proposals
        min_box_size (int): proposals with either side smaller than this
            threshold are removed

    The input dict is modified in-place, with abovementioned keys removed. A new
    key ""proposals"" will be added. Its value is an `Instances`
    object which contains the transformed proposals in its field
    ""proposal_boxes"" and ""objectness_logits"".
    """"""","    if ""proposal_boxes"" in dataset_dict:
        # Transform proposal boxes
        boxes = transforms.apply_box(
            BoxMode.convert(
                dataset_dict.pop(""proposal_boxes""),
                dataset_dict.pop(""proposal_bbox_mode""),
                BoxMode.XYXY_ABS,
            )
        )
        boxes = Boxes(boxes)
        objectness_logits = torch.as_tensor(
            dataset_dict.pop(""proposal_objectness_logits"").astype(""float32"")
        )

        boxes.clip(image_shape)
        keep = boxes.nonempty(threshold=min_box_size)
        boxes = boxes[keep]
        objectness_logits = objectness_logits[keep]

        proposals = Instances(image_shape)
        proposals.proposal_boxes = boxes[:proposal_topk]
        proposals.objectness_logits = objectness_logits[:proposal_topk]
        dataset_dict[""proposals""] = proposals"
307,"    def get_ping_cmd(self):
        """"""Return the command string that will 
           ping / check if the server is alive 

        """"""","
        return ""%s --ping --port=%d --user=root"" % (self.drizzle_client_path, self.master_port)"
308,"def next_(ui, repo, *args, **opts):
    """"""check out a child commit""""""","    _moverelative(ui, repo, args, opts, reverse=False)"
309,"    def build_table(self, entry):
        """"""
        Packs an entry into an action-observation dictionary.

        :param entry: a tuple in the form described in the class docstring.
        """"""","        if isinstance(entry, (dict, Message)):
            # user is already provided things
            if 'eval_labels' in entry or 'eval_label' in entry:
                raise KeyError(
                    'Labels are converted to eval_labels automatically. Please do not '
                    'set them in setup_data.'
                )
            if 'episode_done' in entry:
                raise KeyError(
                    ""episode_done is set automatically for you. Please don't set it ""
                    ""in setup_data.""
                )
            if 'label' in entry:
                # for convenience, rename to the labels convention automatically
                label = entry.pop('label')
                assert isinstance(label, str)
                entry['labels'] = (label,)
            if 'labels' in entry and isinstance(entry['labels'], str):
                entry['labels'] = (entry['labels'],)
            table = entry.copy()
        elif isinstance(entry, (Tuple, List)):
            table = {}
            if entry[0] is not None:
                table['text'] = entry[0]
            if len(entry) > 1 and entry[1] is not None:
                l = entry[1]
                if isinstance(l, str):
                    l = (l,)
                table['labels'] = l
            if len(entry) > 2 and entry[2] is not None:
                table['reward'] = entry[2]
            if len(entry) > 3 and entry[3] is not None:
                table['label_candidates'] = entry[3]
            if len(entry) > 4 and entry[4] is not None:
                img = self.image_loader.load(entry[4])
                if img is not None:
                    table['image'] = img
        else:
            raise TypeError(
                f""items out of setup_data should be dict, Message, list, or tuple. ""
                f""Got {type(entry)})""
            )

        if table.get('labels', None) is not None and self.cands is not None:
            if self.addedCands:
                # remove elements in addedCands
                self.cands.difference_update(self.addedCands)
                self.addedCands.clear()
            for label in table['labels']:
                if label not in self.cands:
                    # add labels, queue them for removal next time
                    if not self.copied_cands:
                        self.cands = self.cands.copy()
                        self.copied_cands = True
                    self.cands.add(label)
                    self.addedCands.append(label)
            table['label_candidates'] = self.cands

        if 'labels' in table and 'label_candidates' in table:
            if table['labels'][0] not in table['label_candidates']:
                raise RuntimeError('true label missing from candidate labels')

        # go ahead and make it a message
        if isinstance(table, dict):
            table = Message(table)

        return table"
310,"    def remove(self, name):
        """"""
        Alias of ``pop``.
        """"""",        self.pop(name)
311,"def stop_aux_processes(client: EdenClient) -> None:
    """"""Tear down processes that will hold onto file handles and prevent shutdown
    for all mounts""""""","
    active_mount_points: Set[Optional[str]] = {
        os.fsdecode(mount.mountPoint) for mount in client.listMounts()
    }

    for repo in active_mount_points:
        if repo is not None:
            stop_aux_processes_for_path(repo)"
312,"def obsmarkersversion(caps):
    # type (Dict[str, Tuple[str, ...]]) -> Iterable[int]
    """"""extract the list of supported obsmarkers versions from a bundle2caps dict""""""","    obscaps = caps.get(""obsmarkers"", ())
    return [int(c[1:]) for c in obscaps if c.startswith(""V"")]"
313,"def gc(ui, repo, *args, **opts):
    """"""garbage collect the client caches""""""","    ui.warn(_(""@prog@ gc is no longer supported.""))

    if not sysplatform.startswith(""win""):
        cachepath = ui.config(""remotefilelog"", ""cachepath"")

        if cachepath:
            command = ""`rm -rf {}/*`"".format(cachepath)

            ui.warn(
                _(
                    """"""
To reclaim space from the hgcache directory, run:

%s

NOTE: The hgcache should manage its size itself. You should only run the command
above if you are completely out of space and quickly need to reclaim some space
temporarily. This will affect other users if you run this command on a shared machine.
""""""
                )
                % command
            )"
314,"def intersectmatchers(m1, m2):
    """"""Composes two matchers by matching if both of them match.

    The second matcher's non-matching-attributes (root, cwd, bad, traversedir)
    are ignored.
    """"""","    if m1 is None or m2 is None:
        return m1 or m2
    if m1.always():
        m = copy.copy(m2)
        # TODO: Consider encapsulating these things in a class so there's only
        # one thing to copy from m1.
        m.bad = m1.bad
        m.traversedir = m1.traversedir
        m.abs = m1.abs
        m.rel = m1.rel
        m._relativeuipath |= m1._relativeuipath
        return m
    if m2.always():
        m = copy.copy(m1)
        m._relativeuipath |= m2._relativeuipath
        return m
    return intersectionmatcher(m1, m2)"
315,"    def eq(self, other):
        """"""
        Automatically created by attrs.
        """"""","        if other.__class__ is self.__class__:
            return attrs_to_tuple(self) == attrs_to_tuple(other)
        else:
            return NotImplemented"
316,"def getflogheads(ui, repo, path):
    """"""
    Extension printing a remotefilelog's heads

    Used for testing purpose
    """"""","
    dest = repo.ui.expandpath(""default"")
    peer = hg.peer(repo, {}, dest)

    flogheads = peer.getflogheads(path)

    if flogheads:
        for head in flogheads:
            ui.write(head + ""\n"")
    else:
        ui.write(_(""EMPTY\n""))"
317,"    def explore_type(name, datatype, is_child):
        """"""Function to explore struct/class and union types.
        See Explorer.explore_type for more information.
        """"""","        type_code = datatype.code
        type_desc = """"
        if type_code == gdb.TYPE_CODE_STRUCT:
            type_desc = ""struct/class""
        else:
            type_desc = ""union""

        fields = datatype.fields()
        if CompoundExplorer._get_real_field_count(fields) == 0:
            if is_child:
                print (""%s is a %s of type '%s' with no fields."" %
                       (name, type_desc, str(datatype)))
                Explorer.return_to_enclosing_type_prompt()
            else:
                print (""'%s' is a %s with no fields."" % (name, type_desc))
            return False

        if is_child:
            print (""%s is a %s of type '%s' ""
                   ""with the following fields:\n"" %
                   (name, type_desc, str(datatype)))
        else:
            print (""'%s' is a %s with the following ""
                   ""fields:\n"" %
                   (name, type_desc))

        has_explorable_fields = False
        current_choice = 0
        choice_to_compound_field_map = { }
        print_list = [ ]
        for field in fields:
            if field.artificial:
                continue
            if field.is_base_class:
                field_desc = ""base class""
            else:
                field_desc = ""field""
            rhs = (""<Enter %d to explore this %s of type '%s'>"" %
                   (current_choice, field_desc, str(field.type)))
            print_list.append((field.name, rhs))
            choice_to_compound_field_map[str(current_choice)] = (
                field.name, field.type, field_desc)
            current_choice = current_choice + 1

        CompoundExplorer._print_fields(print_list)
        print ("""")

        if len(choice_to_compound_field_map) > 0:
            choice = raw_input(""Enter the field number of choice: "")
            if choice in choice_to_compound_field_map:
                if is_child:
                    new_name = (""%s '%s' of %s"" % 
                                (choice_to_compound_field_map[choice][2],
                                 choice_to_compound_field_map[choice][0],
                                 name))
                else:
                    new_name = (""%s '%s' of '%s'"" % 
                                (choice_to_compound_field_map[choice][2],
                                 choice_to_compound_field_map[choice][0],
                                 name))
                Explorer.explore_type(new_name,
                    choice_to_compound_field_map[choice][1], True)
                return True
            else:
                if is_child:
                    Explorer.return_to_enclosing_type()
        else:
            if is_child:
                Explorer.return_to_enclosing_type_prompt()

        return False"
318,"    def status(
        self, match: ""Callable[[str], bool]"", ignored: bool, clean: bool, unknown: bool
    ) -> ""scmutil.status"":
        """"""Determine the status of the working copy relative to the
        dirstate and return a scmutil.status.
        """"""","        if self._ui.configbool(""workingcopy"", ""ruststatus""):
            try:
                return self._ruststatus(match, ignored, clean, unknown)
            except self.FallbackToPythonStatus:
                pass

        wctx = self._repo[None]
        # Prime the wctx._parents cache so the parent doesn't change out from
        # under us if a checkout happens in another process.
        pctx = wctx.p1()

        listignored, listclean, listunknown = ignored, clean, unknown
        modified: ""List[str]"" = []
        added: ""List[str]"" = []
        unknownpaths: ""List[str]"" = []
        ignoredpaths: ""List[str]"" = []
        removed: ""List[str]"" = []
        deleted: ""List[str]"" = []
        cleanpaths: ""List[str]"" = []

        dmap = self._map
        dmap.preload()
        dget = dmap.__getitem__
        madd = modified.append
        aadd = added.append
        uadd = unknownpaths.append
        iadd = ignoredpaths.append
        radd = removed.append
        dadd = deleted.append
        cadd = cleanpaths.append
        ignore = self._ignore
        copymap = self._map.copymap

        # We have seen some rare issues that a few ""M"" or ""R"" files show up
        # while the files are expected to be clean. Log the reason of first few
        # ""M"" files.
        mtolog = self._ui.configint(""experimental"", ""samplestatus"") or 0

        oldid = self.identity()

        # Step 1: Get the files that are different from the clean checkedout p1 tree.
        pendingchanges = self._fs.pendingchanges(match, listignored=listignored)

        for fn, exists in pendingchanges:
            assert isinstance(fn, str)
            try:
                t = dget(fn)
                # This ""?"" state is only tracked by treestate, emulate the old
                # behavior - KeyError.
                if t[0] == ""?"":
                    raise KeyError
            except KeyError:
                isignored = ignore(fn)
                if listignored and isignored:
                    iadd(fn)
                elif listunknown and not isignored:
                    uadd(fn)
                continue

            state = t[0]
            if not exists and state in ""nma"":
                dadd(fn)
            elif state == ""n"":
                madd(fn)
            else:
                # All other states will be handled by the logic below, and we
                # don't care that it's a pending change.
                pass

        # Fetch the nonnormalset after iterating over pendingchanges, since the
        # iteration may change the nonnormalset as lookup states are resolved.
        if util.safehasattr(dmap, ""nonnormalsetfiltered""):
            # treestate has a fast path to filter out ignored directories.
            ignorevisitdir: ""Callable[[str], Union[str, bool]]"" = ignore.visitdir

            def dirfilter(path: str) -> bool:
                result = ignorevisitdir(path.rstrip(""/""))
                return result == ""all""

            tsmap = cast(treestate.treestatemap, dmap)
            nonnormalset = tsmap.nonnormalsetfiltered(dirfilter)
        else:
            nonnormalset = dmap.nonnormalset

        otherparentset = dmap.otherparentset

        # The seen set is used to prevent steps 2 and 3 from processing things
        # we saw in step 1.
        seenset = set(deleted + modified)

        # audit_path is used to verify that nonnormal files still exist and are
        # not behind symlinks.
        auditpath: ""pathutil.pathauditor"" = pathutil.pathauditor(
            self._root, cached=True
        )

        def fileexists(fn: str) -> bool:
            # So let's double check for the existence of that file.
            st = list(util.statfiles([self._join(fn)]))[0]

            # auditpath checks to see if the file is under a symlink directory.
            # If it is, we treat it the same as if it didn't exist.
            return st is not None and auditpath.check(fn)

        # Step 2: Handle status results that are not simply pending filesystem
        # changes on top of the pristine tree.
        for fn in otherparentset:
            assert isinstance(fn, str)
            if not match(fn) or fn in seenset:
                continue
            t = dget(fn)
            state = t[0]
            # We only need to handle 'n' here, since all other states will be
            # covered by the nonnormal loop below.
            if state in ""n"":
                # pendingchanges() above only checks for changes against p1.
                # For things from p2, we need to manually check for
                # existence. We don't have to check if they're modified,
                # since them coming from p2 indicates they are considered
                # modified.
                if fileexists(fn):
                    if mtolog > 0:
                        mtolog -= 1
                        self._ui.log(""status"", ""M %s: exists in p2"" % fn)
                    madd(fn)
                else:
                    dadd(fn)
                seenset.add(fn)

        for fn in nonnormalset:
            assert isinstance(fn, str)
            if not match(fn) or fn in seenset:
                continue
            t = dget(fn)
            state = t[0]
            if state == ""m"":
                madd(fn)
                seenset.add(fn)
                if mtolog > 0:
                    mtolog -= 1
                    self._ui.log(""status"", ""M %s: state is 'm' (merge)"" % fn)
            elif state == ""a"":
                if fileexists(fn):
                    aadd(fn)
                else:
                    # If an added file is deleted, report it as missing
                    dadd(fn)
                seenset.add(fn)
            elif state == ""r"":
                radd(fn)
                seenset.add(fn)
            elif state == ""n"":
                # This can happen if the file is in a lookup state, but all 'n'
                # files should've been checked in fs.pendingchanges, so we can
                # ignore it here.
                pass
            elif state == ""?"":
                # I'm pretty sure this is a bug if nonnormalset contains unknown
                # files, but the tests say it can happen so let's just ignore
                # it.
                pass
            else:
                raise error.ProgrammingError(
                    ""unexpected nonnormal state '%s' "" ""for '%s'"" % (t, fn)
                )

        # Most copies should be handled above, as modifies or adds, but there
        # can be cases where a file is clean and already committed and a commit
        # is just retroactively marking it as copied. In that case we need to
        # mark is as modified.
        for fn in copymap:
            assert isinstance(fn, str)
            if not match(fn) or fn in seenset:
                continue
            # It seems like a bug, but the tests show that copymap can contain
            # files that aren't in the dirstate. I believe this is caused by
            # using treestate, which leaves the copymap as partially maintained.
            if fn not in dmap:
                continue
            madd(fn)
            seenset.add(fn)

        status = scmutil.status(
            modified, added, removed, deleted, unknownpaths, ignoredpaths, cleanpaths
        )

        # Step 3: If clean files were requested, add those to the results
        seenset = set()
        for files in status:
            seenset.update(files)
            seenset.update(util.dirs(files))

        if listclean:
            for fn in pctx.manifest().matches(match):
                assert isinstance(fn, str)
                if fn not in seenset:
                    cadd(fn)
            seenset.update(cleanpaths)

        # Step 4: Report any explicitly requested files that don't exist
        # pyre-fixme[16]: Anonymous callable has no attribute `files`.
        for path in sorted(match.files()):
            try:
                if path in seenset:
                    continue
                os.lstat(os.path.join(self._root, path))
            except OSError as ex:
                # pyre-fixme[16]: Anonymous callable has no attribute `bad`.
                match.bad(path, encoding.strtolocal(ex.strerror))

        # TODO: fire this inside filesystem. fixup is a list of files that
        # checklookup says are clean
        if not getattr(self._repo, ""_insidepoststatusfixup"", False):
            self._poststatusfixup(status, wctx, oldid)

        perftrace.tracevalue(""A/M/R Files"", len(modified) + len(added) + len(removed))
        if len(unknownpaths) > 0:
            perftrace.tracevalue(""Unknown Files"", len(unknownpaths))
        if len(ignoredpaths) > 0:
            perftrace.tracevalue(""Ignored Files"", len(ignoredpaths))
        return status"
319,"    def observe(self, observation: Union[Dict, Message]) -> Message:
        """"""
        Overrides TA.observe to tokenize the retriever query.
        """"""","        observation = Message(observation)
        observation = self._generation_agent.observe(self, observation)
        if observation.is_padding():
            return observation
        if 'query_vec' not in observation and self._query_key in observation:
            self._set_query_vec(observation)
        if 'input_turn_cnt_vec' not in observation and self._query_key in observation:
            self._set_input_turn_cnt_vec(observation)
        return observation"
320,"    def decodeddata(self):
        """"""Returns `data()` after running repository decoding filters.

        This is often equivalent to how the data would be expressed on disk.
        """"""","        return self._repo.wwritedata(self.path(), self.data())"
321,"def should_sync_gradnorm(opt):
    """"""
    Indicates whether fp16 optimizer wrappers should accumulate over workers.

    FP16 overflow detection and gradient clipping both require accumulating gradients
    across all workers when using FSDP, as workers only store a fraction of the
    gradients.
    """"""","    return (
        FSDP_AVAILABLE
        and opt['fp16']
        and opt.get('ddp_backend', DEFAULT_DDP_BACKEND) in ('zero2', 'zero3')
    )"
322,"def getcurrentprocstarttime():
    """"""Get current process start time

    See _getprocstarttime docstring for more info""""""","    pid = _kernel32.GetCurrentProcessId()
    return getprocstarttime(pid)"
323,"    def upgrade_opt(cls, opt_on_disk):
        """"""
        Upgrade opts from older model files.
        """"""","        super(BertClassifierAgent, cls).upgrade_opt(opt_on_disk)

        # 2019-06-25: previous versions of the model did not add a CLS token
        # to the beginning of text_vec.
        if ""add_cls_token"" not in opt_on_disk:
            warn_once(""Old model: overriding `add_cls_token` to False."")
            opt_on_disk[""add_cls_token""] = False

        return opt_on_disk"
324,"    def custom_evaluation(
        self, teacher_action: Message, labels, model_response: Message
    ):
        """"""
        for dialog state tracking, we compute the joint goal accuracy, which is the
        percentage of the turns where the model correctly and precisely predicts all
        slots(domain, slot_type, slot_value).
        """"""","        resp = model_response.get(""text"")
        if not resp:
            return

        # extract ground truth from labels
        (
            slots_truth,
            slots_truth_per_domain,
            slots_truth_named_entity,
        ) = self._extract_slot_from_string(labels[0])

        # extract generated slots from model_response
        (
            slots_pred,
            slots_pred_per_domain,
            slots_pred_named_entity,
        ) = self._extract_slot_from_string(resp)

        for gt_slot in slots_truth:
            self.metrics.add(""all/slot_r"", AverageMetric(gt_slot in slots_pred))
            curr_domain = gt_slot.split(""--"")[0]
            self.metrics.add(
                f""{curr_domain}/slot_r"", AverageMetric(gt_slot in slots_pred)
            )

        for gt_slot in slots_pred_named_entity:
            self.metrics.add(
                ""hallucination"", AverageMetric(gt_slot not in slots_truth_named_entity)
            )

        for predicted_slot in slots_pred:
            self.metrics.add(""all/slot_p"", AverageMetric(predicted_slot in slots_truth))
            curr_domain = predicted_slot.split(""--"")[0]
            self.metrics.add(
                f""{curr_domain}/slot_p"", AverageMetric(predicted_slot in slots_truth)
            )

        self.metrics.add(""jga"", AverageMetric(set(slots_truth) == set(slots_pred)))
        self.metrics.add(
            ""named_entities/jga"",
            AverageMetric(
                set(slots_truth_named_entity) == set(slots_pred_named_entity)
            ),
        )
        for gt_slot in slots_truth_named_entity:
            self.metrics.add(""all_ne/slot_r"", AverageMetric(gt_slot in slots_pred))
            curr_domain = gt_slot.split(""--"")[0]
            self.metrics.add(
                f""{curr_domain}_ne/slot_r"", AverageMetric(gt_slot in slots_pred)
            )
        for predicted_slot in slots_pred_named_entity:
            self.metrics.add(
                ""all_ne/slot_p"", AverageMetric(predicted_slot in slots_truth)
            )
            curr_domain = predicted_slot.split(""--"")[0]
            self.metrics.add(
                f""{curr_domain}_ne/slot_p"", AverageMetric(predicted_slot in slots_truth)
            )

        for domain in slots_truth_per_domain:
            if domain in slots_pred_per_domain:
                self.metrics.add(
                    f""{domain}/jga"",
                    AverageMetric(
                        slots_truth_per_domain[domain] == slots_pred_per_domain[domain]
                    ),
                )"
325,"    def get_method_id(self, method_ref):
        """"""method_ref can be one of:
        - a encoded_method object
        - integer method index""""""","        method_ids = self.get_method_ids()
        if method_ids:
            if isinstance(method_ref, encoded_method):
                if method_ref.method_idx < len(method_ids):
                    return method_ids[method_ref.method_idx]
            elif isinstance(method_ref, numbers.Integral):
                if method_ref < len(method_ids):
                    return method_ids[method_ref]
            else:
                raise ValueError(""invalid method_ref type %s"" % (type(method_ref)))
        return None"
326,"def load_task_script(template_id_or_path: str) -> Tuple[str, str, TaskScript]:
    """"""Loads task script given either template_id or full path to the scripts.

    Args:
       template_id_or_path: str, either template_id or full path to the
        script. In the former case a script will be searched in
        TASK_SCRIPTS_DIR.

    Returns:
        tuple, (template_id, task_path, task_script_module).
    """"""","    if '/' in template_id_or_path:
        task_dir, task_script_name = template_id_or_path.rsplit('/', 1)
        assert task_script_name.startswith('task'), template_id_or_path
        assert task_script_name.endswith('.py'), template_id_or_path
        template_id = task_script_name[4:-3]
    else:
        task_dir = str(phyre.settings.TASK_SCRIPTS_DIR)
        template_id = template_id_or_path
    loaded = load_task_scripts_from_folder(task_dir, [template_id])
    if len(loaded) != 1:
        raise RuntimeError('Failed to load task script %s from %s' %
                           (template_id, task_dir))
    return loaded[0]"
327,"def im_detect_keypoints_hflip(model, im, target_scale, target_max_size, boxes):
    """"""Computes keypoint predictions on the horizontally flipped image.
    Function signature is the same as for im_detect_keypoints_aug.
    """"""","    # Compute keypoints for the flipped image
    im_hf = im[:, ::-1, :]
    boxes_hf = box_utils.flip_boxes(boxes, im.shape[1])

    im_scale = im_conv_body_only(model, im_hf, target_scale, target_max_size)
    heatmaps_hf = im_detect_keypoints(model, im_scale, boxes_hf)

    # Invert the predicted keypoints
    heatmaps_inv = keypoint_utils.flip_heatmaps(heatmaps_hf)

    return heatmaps_inv"
328,"def rolling_median_by_h(x, h, w, name):
    """"""Compute a rolling median of x, after first aggregating by h.

    Right-aligned. Computes a single median for each unique value of h. Each
    median is over at least w samples.

    For each h where there are fewer than w samples, we take samples from the previous h,
    moving backwards. (In other words, we ~ assume that the x's are shuffled within each h.)

    Parameters
    ----------
    x: Array.
    h: Array of horizon for each value in x.
    w: Integer window size (number of elements).
    name: Name for metric in result dataframe

    Returns
    -------
    Dataframe with columns horizon and name, the rolling median of x.
    """"""","    # Aggregate over h
    df = pd.DataFrame({'x': x, 'h': h})
    grouped = df.groupby('h')
    df2 = grouped.size().reset_index().sort_values('h')
    hs = df2['h']

    res_h = []
    res_x = []
    # Start from the right and work backwards
    i = len(hs) - 1
    while i >= 0:
        h_i = hs[i]
        xs = grouped.get_group(h_i).x.tolist()

        # wrap in array so this works if h is pandas Series with custom index or numpy array
        next_idx_to_add = np.array(h == h_i).argmax() - 1
        while (len(xs) < w) and (next_idx_to_add >= 0):
            # Include points from the previous horizon. All of them if still
            # less than w, otherwise just enough to get to w.
            xs.append(x[next_idx_to_add])
            next_idx_to_add -= 1
        if len(xs) < w:
            # Ran out of points before getting enough.
            break
        res_h.append(hs[i])
        res_x.append(np.median(xs))
        i -= 1
    res_h.reverse()
    res_x.reverse()
    return pd.DataFrame({'horizon': res_h, name: res_x})"
329,"def get_options(cls):
    """"""Get argparse options for class, `cls`

    Look for class level attributes of type option and
    convert them into the arguments  necessary for calling
    parser.add_argument().

    Arguments:
        subclass of VTask or Vservice - `cls`

    Returns:
        list(namedtuple) -
            .opt - namedtuple
                   .args - list of argument string names
                          (e.g. ['--help', '-h'])
                   .kwargs - dict of kwargs for add_argument()
                          (e.g. default=False, action='store_true' etc)
            .regfunc - callable that takes the ArgumentParser as an argument
                       and adds the option to it.
                       (e.g. ""foo.regfunc(ap)"" registers the foo option on ap)
    """"""","    ret = []
    for k in dir(cls):
        v = getattr(cls, k)
        preparefunc = getattr(v, '_prepareForArgumentParser', None)
        if not preparefunc:
            continue
        opt = preparefunc(cls)
        regfunc = partial(getattr(v, '_addToArgumentParser'), opt)
        ret.append(_OptRegFunc(opt, regfunc))
    return ret"
330,"    def __init__(self, tko_responses = 1):
        """"""Simple server stub that initially responds to requests
        with server error, causing it to hard tko.
        The number of times to respond as error = tkoResponses.
        """"""","        super().__init__()
        self.tko_responses = tko_responses"
331,"    def write(self, bytes):
        """"""Decode bytes to the output stream.

        :raises ValueError: If the stream has already seen the end of file
            marker.
        :returns: None, or the excess bytes beyond the end of file marker.
        """"""","        if bytes:
            self.buffered_bytes.append(bytes)
        return self.state()"
332,"def ReverseCloseExpression(clean_lines, linenum, pos):
  """"""If input points to ) or } or ] or >, finds the position that opens it.

  If lines[linenum][pos] points to a ')' or '}' or ']' or '>', finds the
  linenum/pos that correspond to the opening of the expression.

  Args:
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    pos: A position on the line.

  Returns:
    A tuple (line, linenum, pos) pointer *at* the opening brace, or
    (line, 0, -1) if we never find the matching opening brace.  Note
    we ignore strings and comments when matching; and the line we
    return is the 'cleansed' line at linenum.
  """"""","  line = clean_lines.elided[linenum]
  endchar = line[pos]
  if endchar not in ')}]>':
    return (line, 0, -1)
  if endchar == ')': startchar = '('
  if endchar == ']': startchar = '['
  if endchar == '}': startchar = '{'
  if endchar == '>': startchar = '<'

  # Check last line
  (start_pos, num_open) = FindStartOfExpressionInLine(
      line, pos, 0, startchar, endchar)
  if start_pos > -1:
    return (line, linenum, start_pos)

  # Continue scanning backward
  while linenum > 0:
    linenum -= 1
    line = clean_lines.elided[linenum]
    (start_pos, num_open) = FindStartOfExpressionInLine(
        line, len(line) - 1, num_open, startchar, endchar)
    if start_pos > -1:
      return (line, linenum, start_pos)

  # Did not find startchar before beginning of file, give up
  return (line, 0, -1)"
333,"    def __init__(self, opt: Opt, shared: TShared = None):
        """"""
        Override init to build the data.
        """"""","        super().__init__(opt, shared)
        if self.lower:
            warn_once('Are you sure you want to lower case your BPE dictionary?')

        if self.maxtokens > 0 or self.minfreq > 0:
            raise ValueError(
                'You should not filter vocabulary with using --dict-tokenizer bytelevelbpe'
                ' (no --dict-minfreq or --dict-maxtokens).'
            )

        self.bpe_data, self.json_path, self.merge_path = self._build_data()

        # build encoder & decoder
        self.encoder: Dict[str, str] = self._build_encoder(self.json_path)
        self.decoder: Dict[str, str] = {v: k for k, v in self.encoder.items()}

        bpe_merges = [
            tuple(merge_str.split()) for merge_str in self.bpe_data.split('\n')[1:-1]
        ]
        self.byte_encoder = self.bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))

        if regex is None:
            raise ImportError('Please install regex with: pip install regex')
        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = regex.compile(self.PATTERN)"
334,"    def close(self):
        """"""Close the underlying transport.""""""",        self.transport.close()
335,"  def PEX_INHERIT_PATH(self):
    """"""Boolean

    Allow inheriting packages from site-packages.  By default, PEX scrubs any packages and
    namespace packages from sys.path prior to invoking the application.  This is generally not
    advised, but can be used in situations when certain dependencies do not conform to standard
    packaging practices and thus cannot be bundled into PEX files.  Default: false.
    """"""","    return self._get_bool('PEX_INHERIT_PATH', default=False)"
336,"    def step(self, closure=None):
        """"""
        Performs a single optimization step.
        """"""","        self._unscale_grads()
        self.optimizer.step(closure)"
337,"    def _manifest(self):
        """"""generate a manifest based on the return values of filectxfn""""""","
        # keep this simple for now; just worry about p1
        pctx = self._parents[0]
        man = pctx.manifest().copy()

        for f in self._status.modified:
            if git.isgitformat(self._repo):
                man[f] = git.hashobj(b""blob"", self[f].data())
            else:
                p1node = nullid
                p2node = nullid
                p = pctx[f].parents()  # if file isn't in pctx, check p2?
                if len(p) > 0:
                    p1node = p[0].filenode()
                    if len(p) > 1:
                        p2node = p[1].filenode()
                man[f] = revlog.hash(self[f].data(), p1node, p2node)

        for f in self._status.added:
            if git.isgitformat(self._repo):
                man[f] = git.hashobj(b""blob"", self[f].data())
            else:
                man[f] = revlog.hash(self[f].data(), nullid, nullid)

        for f in self._status.removed:
            if f in man:
                del man[f]

        return man"
338,"def _fixsys():
    """"""Fix sys.path so core edenscm modules (edenscmnative, and 3rd party
    libraries) are in sys.path

    Fix sys.stdin if it's None.
    """"""","    import os

    # Do not expose those modules to edenscm.__dict__
    import sys

    dirname = os.path.dirname

    # __file__ is ""hg/edenscm/__init__.py""
    # libdir is ""hg/""
    # Do not follow symlinks (ex. do not use ""realpath""). It breaks buck build.
    libdir = dirname(dirname(os.path.abspath(__file__)))

    # Make ""edenscmdeps.zip"" available in sys.path. It includes 3rd party
    # pure-Python libraries like IPython, thrift runtime, etc.
    #
    # Note: On Windows, the released version of hg uses python27.zip for all
    # pure Python modules including edenscm and everything in edenscmdeps.zip,
    # so not being able to locate edenscmdeps.zip is not fatal.
    if sys.version_info[0] == 3:
        name = ""edenscmdeps3.zip""
    else:
        name = ""edenscmdeps.zip""
    for candidate in [libdir, os.path.join(libdir, ""build"")]:
        depspath = os.path.join(candidate, name)
        if os.path.exists(depspath) and depspath not in sys.path:
            sys.path.insert(0, depspath)

    # Make sure ""edenscmnative"" can be imported. Error early.
    import edenscmnative

    edenscmnative.__name__

    # stdin can be None if the parent process unset the stdin file descriptor.
    # Replace it early, since it may be read in layer modules, like pycompat.
    if sys.stdin is None:
        sys.stdin = open(os.devnull, ""r"")

    # On Windows, the system time zone setting, if does not match the time
    # zone from the package building machine, can cause pyc to be invalidated
    # in a zip file. Workaround it by bypassing the mtime check.
    if os.name == ""nt"" and sys.version_info[0] == 3:
        import zipimport

        zipimport._get_mtime_and_size_of_source = lambda _s, _p: (0, 0)"
339,"def get_default_cache():
    """"""Determine the default cache location

    This returns the ``PYTHON_EGG_CACHE`` environment variable, if set.
    Otherwise, on Windows, it returns a ""Python-Eggs"" subdirectory of the
    ""Application Data"" directory.  On all other systems, it's ""~/.python-eggs"".
    """"""","    try:
        return os.environ['PYTHON_EGG_CACHE']
    except KeyError:
        pass

    if os.name!='nt':
        return os.path.expanduser('~/.python-eggs')

    # XXX this may be locale-specific!
    app_data = 'Application Data'
    app_homes = [
        # best option, should be locale-safe
        (('APPDATA',), None),
        (('USERPROFILE',), app_data),
        (('HOMEDRIVE','HOMEPATH'), app_data),
        (('HOMEPATH',), app_data),
        (('HOME',), None),
        # 95/98/ME
        (('WINDIR',), app_data),
    ]

    for keys, subdir in app_homes:
        dirname = ''
        for key in keys:
            if key in os.environ:
                dirname = os.path.join(dirname, os.environ[key])
            else:
                break
        else:
            if subdir:
                dirname = os.path.join(dirname, subdir)
            return os.path.join(dirname, 'Python-Eggs')
    else:
        raise RuntimeError(
            ""Please set the PYTHON_EGG_CACHE enviroment variable""
        )"
340,"    def __init__(
        self,
        *,
        stem_class,
        stem_width,
        block_class,
        depths,
        widths,
        group_widths,
        strides,
        bottleneck_ratios,
        se_ratio,
        activation_class,
        freeze_at=0,
        norm=""BN"",
        out_features=None,
    ):
        """"""
        Args:
            stem_class (callable): A callable taking 4 arguments (channels in, channels out,
                normalization, callable returning an activation function) that returns another
                callable implementing the stem module.
            stem_width (int): The number of output channels that the stem produces.
            block_class (callable): A callable taking 6 arguments (channels in, channels out,
                stride, normalization, callable returning an activation function, a dict of
                block-specific parameters) that returns another callable implementing the repeated
                block module.
            depths (list[int]): Number of blocks in each stage.
            widths (list[int]): For each stage, the number of output channels of each block.
            group_widths (list[int]): For each stage, the number of channels per group in group
                convolution, if the block uses group convolution.
            strides (list[int]): The stride that each network stage applies to its input.
            bottleneck_ratios (list[float]): For each stage, the ratio of the number of bottleneck
                channels to the number of block input channels (or, equivalently, output channels),
                if the block uses a bottleneck.
            se_ratio (float): The ratio of the number of channels used inside the squeeze-excitation
                (SE) module to it number of input channels, if SE the block uses SE.
            activation_class (callable): A callable taking no arguments that returns another
                callable implementing an activation function.
            freeze_at (int): The number of stages at the beginning to freeze.
                see :meth:`freeze` for detailed explanation.
            norm (str or callable): normalization for all conv layers.
                See :func:`layers.get_norm` for supported format.
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. RegNet's use ""stem"" and ""s1"", ""s2"", etc for the stages after
                the stem. If None, will return the output of the last layer.
        """"""","        super().__init__()
        self.stem = stem_class(3, stem_width, norm, activation_class)

        current_stride = self.stem.stride
        self._out_feature_strides = {""stem"": current_stride}
        self._out_feature_channels = {""stem"": self.stem.out_channels}
        self.stages_and_names = []
        prev_w = stem_width

        for i, (d, w, s, b, g) in enumerate(
            zip(depths, widths, strides, bottleneck_ratios, group_widths)
        ):
            params = {""bot_mul"": b, ""group_w"": g, ""se_r"": se_ratio}
            stage = AnyStage(prev_w, w, s, d, block_class, norm, activation_class, params)
            name = ""s{}"".format(i + 1)
            self.add_module(name, stage)
            self.stages_and_names.append((stage, name))
            self._out_feature_strides[name] = current_stride = int(
                current_stride * np.prod([k.stride for k in stage.children()])
            )
            self._out_feature_channels[name] = list(stage.children())[-1].out_channels
            prev_w = w

        self.apply(init_weights)

        if out_features is None:
            out_features = [name]
        self._out_features = out_features
        assert len(self._out_features)
        children = [x[0] for x in self.named_children()]
        for out_feature in self._out_features:
            assert out_feature in children, ""Available children: {} does not include {}"".format(
                "", "".join(children), out_feature
            )
        self.freeze(freeze_at)"
341,"def abs_glob(directory: str, pattern: str = ""*"") -> typing.Generator[str, None, None]:
    """"""
    Returns all files that match the specified glob inside a directory.
    Returns absolute paths. Does not return files that start with '.'
    """"""","    for result in glob.glob(join(directory, pattern)):
        yield join(directory, result)"
342,"    def generation_model(self, model: str):
        """"""
        Override to always be GPT2.
        """"""","        self._generation_model = model
        self._generation_agent = GPT2WithRetrieverAgentBase"
343,"    def summary(self):
        """"""
        :return: CGPoint summary
        :rtype: str
        """"""","        x = self.element.GetChildMemberWithName(""x"")
        y = self.element.GetChildMemberWithName(""y"")

        x_value = float(x.GetValue())
        y_value = float(y.GetValue())
        return ""{{{}, {}}}"".format(x_value, y_value)"
344,"def RemoveMultiLineCommentsFromRange(lines, begin, end):
  """"""Clears a range of lines for multi-line comments.""""""","  # Having // dummy comments makes the lines non-empty, so we will not get
  # unnecessary blank line warnings later in the code.
  for i in range(begin, end):
    lines[i] = '// dummy'"
345,"def samedevice(path1, path2):
    """"""Returns whether path1 and path2 are on the same device.""""""","    res1 = getfileinfo(path1)
    res2 = getfileinfo(path2)
    return res1.dwVolumeSerialNumber == res2.dwVolumeSerialNumber"
346,"def debugmovescratchbookmark(ui, repo, *args, **opts):
    """"""move existing scratch bookmark to the specified revision""""""","    if not common.isserver(ui):
        raise error.Abort(
            _(""scratch bookmarks can only be moved on an infinitepush server"")
        )

    scratchbookmarkname = _resolvescratchbookmark(ui, opts.get(""bookmark""))
    index = repo.bundlestore.index
    with index:
        currentnode = index.getnode(scratchbookmarkname)
        if not currentnode:
            raise error.Abort(
                _(""scratch bookmark '%s' does not exist"") % scratchbookmarkname
            )

        targetnode = _resolvetargetnode(repo, opts.get(""rev""))
        index.deletebookmarks([scratchbookmarkname])
        index.addbookmark(scratchbookmarkname, targetnode, False)"
347,"    def __init__(self, intensity_min, intensity_max):
        """"""
        Args:
            intensity_min (float): Minimum augmentation
            intensity_max (float): Maximum augmentation
        """"""","        super().__init__()
        self._init(locals())"
348,"def _filesindirs(repo, manifest, dirs):
    """"""
    Generator that yields pairs of all the files in the manifest that are found
    inside the directories listed in dirs, and which directory they are found
    in.
    """"""","    for dir in dirs:
        dirmatch = matchmod.match(repo.root, """", include=[dir + ""/**""])
        for f in manifest.matches(dirmatch):
            yield f, dir"
349,"def _evaluate_box_proposals(dataset_predictions, lvis_api, thresholds=None, area=""all"", limit=None):
    """"""
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official LVIS API recall evaluation code. However,
    it produces slightly different results.
    """"""","    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0**2, 1e5**2],  # all
        [0**2, 32**2],  # small
        [32**2, 96**2],  # medium
        [96**2, 1e5**2],  # large
        [96**2, 128**2],  # 96-128
        [128**2, 256**2],  # 128-256
        [256**2, 512**2],  # 256-512
        [512**2, 1e5**2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict[""proposals""]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = lvis_api.get_ann_ids(img_ids=[prediction_dict[""image_id""]])
        anno = lvis_api.load_anns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj[""bbox""], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS) for obj in anno
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = (
        torch.cat(gt_overlaps, dim=0) if len(gt_overlaps) else torch.zeros(0, dtype=torch.float32)
    )
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }"
350,"def check_for_occlusions(task, user_input, keep_space_around_bodies=True):
    """"""Returns true if user_input occludes scene objects.""""""","    if not isinstance(task, bytes):
        task = serialize(task)
    if isinstance(user_input, scene_if.UserInput):
        return simulator_bindings.check_for_occlusions_general(
            task, serialize(user_input), keep_space_around_bodies)
    else:
        points, rectangulars, balls = _prepare_user_input(*user_input)
        return simulator_bindings.check_for_occlusions(
            task, points, rectangulars, balls, keep_space_around_bodies)"
351,"    def _get_subagent_opt(
        self,
        filename: str,
        specific_override_args: Dict[str, Any],
        general_override_args: Dict[str, Any],
    ) -> Opt:
        """"""
        Return the specific subagent opt parameters.
        """"""","        return super().get_subagent_opt(
            self.opt['datapath'],
            filename,
            specific_override_args,
            general_override_args,
        )"
352,"    def forward(self, x, H, W):
        """"""Forward function.
        Args:
            x: Input feature, tensor size (B, H*W, C).
            H, W: Spatial resolution of the input feature.
        """"""","        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""

        x = x.view(B, H, W, C)

        # padding
        pad_input = (H % 2 == 1) or (W % 2 == 1)
        if pad_input:
            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x"
353,"def ProcessFile(filename, vlevel, extra_check_functions=[]):
  """"""Does google-lint on a single file.

  Args:
    filename: The name of the file to parse.

    vlevel: The level of errors to report.  Every error of confidence
    >= verbose_level will be reported.  0 is a good default.

    extra_check_functions: An array of additional check functions that will be
                           run on each source line. Each function takes 4
                           arguments: filename, clean_lines, line, error
  """"""","
  _SetVerboseLevel(vlevel)

  try:
    # Support the UNIX convention of using ""-"" for stdin.  Note that
    # we are not opening the file with universal newline support
    # (which codecs doesn't support anyway), so the resulting lines do
    # contain trailing '\r' characters if we are reading a file that
    # has CRLF endings.
    # If after the split a trailing '\r' is present, it is removed
    # below. If it is not expected to be present (i.e. os.linesep !=
    # '\r\n' as in Windows), a warning is issued below if this file
    # is processed.

    if filename == '-':
      lines = codecs.StreamReaderWriter(sys.stdin,
                                        codecs.getreader('utf8'),
                                        codecs.getwriter('utf8'),
                                        'replace').read().split('\n')
    else:
      lines = codecs.open(filename, 'r', 'utf8', 'replace').read().split('\n')

    carriage_return_found = False
    global_nolint_found = False
    is_generated = False
    # Remove trailing '\r'.
    for linenum in range(len(lines)):
      if Search(r'@nolint', lines[linenum]):
        global_nolint_found = True
        break
      if Search(r'Generated from .* by', lines[linenum]):
        is_generated = True
        break
      if lines[linenum].endswith('\r'):
        lines[linenum] = lines[linenum].rstrip('\r')
        carriage_return_found = True

  except IOError:
    sys.stderr.write(
        ""Skipping input '%s': Can't open for reading\n"" % filename)
    return

  if is_generated:
    #sys.stderr.write('Ignoring %s; generated file\n' % filename)
    return
  if global_nolint_found:
    #sys.stderr.write('Ignoring %s; @nolint found\n' % filename)
    return

  # Note, if no dot is found, this will give the entire filename as the ext.
  file_extension = filename[filename.rfind('.') + 1:]

  # When reading from stdin, the extension is unknown, so no cpplint tests
  # should rely on the extension.
  if (filename != '-' and file_extension != 'cc' and file_extension != 'h'
      and file_extension != 'cpp' and file_extension != 'c'):
    sys.stderr.write('Ignoring %s; not a .cc or .h file\n' % filename)
  else:
    ProcessFileData(filename, file_extension, lines, Error,
                    extra_check_functions)
    if carriage_return_found and os.linesep != '\r\n':
      # Use 0 for linenum since outputting only one error for potentially
      # several lines.
      Error(filename, 0, 'whitespace/newline', 1,
            'One or more unexpected \\r (^M) found;'
            'better to use only a \\n')"
354,"  def CheckCompletedBlocks(self, filename, error):
    """"""Checks that all classes and namespaces have been completely parsed.

    Call this when all lines in a file have been processed.
    Args:
      filename: The name of the current file.
      error: The function to call with any errors found.
    """"""","    # Note: This test can result in false positives if #ifdef constructs
    # get in the way of brace matching. See the testBuildClass test in
    # cpplint_unittest.py for an example of this.
    for obj in self.stack:
      if isinstance(obj, _ClassInfo):
        error(filename, obj.starting_linenum, 'build/class', 5,
              'Failed to find complete declaration of class %s' %
              obj.name)
      elif isinstance(obj, _NamespaceInfo):
        error(filename, obj.starting_linenum, 'build/namespaces', 5,
              'Failed to find complete declaration of namespace %s' %
              obj.name)"
355,"def unorderable_list_difference(expected, actual, ignore_duplicate=False):
    """"""Same behavior as sorted_list_difference but
    for lists of unorderable items (like dicts).

    As it does a linear search per item (remove) it
    has O(n*n) performance.
    """"""","    missing = []
    unexpected = []
    while expected:
        item = expected.pop()
        try:
            actual.remove(item)
        except ValueError:
            missing.append(item)
        if ignore_duplicate:
            for lst in expected, actual:
                try:
                    while True:
                        lst.remove(item)
                except ValueError:
                    pass
    if ignore_duplicate:
        while actual:
            item = actual.pop()
            unexpected.append(item)
            try:
                while True:
                    actual.remove(item)
            except ValueError:
                pass
        return missing, unexpected

    # anything left in actual is unexpected
    return missing, actual"
356,"    def iterremoveditems(self):
        """"""
        Returns an iterator over (filename, (state, mode, size, mtime)) for
        files that have been marked as removed.
        """"""","        return treedirstatemapiterator(self._rmap, removed=True)"
357,"def _changesrange(fctx1, fctx2, linerange2, diffopts):
    """"""Return `(diffinrange, linerange1)` where `diffinrange` is True
    if diff from fctx2 to fctx1 has changes in linerange2 and
    `linerange1` is the new line range for fctx1.
    """"""","    blocks = mdiff.allblocks(fctx1.data(), fctx2.data(), diffopts)
    filteredblocks, linerange1 = mdiff.blocksinrange(blocks, linerange2)
    diffinrange = any(stype == ""!"" for _, stype in filteredblocks)
    return diffinrange, linerange1"
358,"def _buildfuncargs(exp, context, curmethods, funcname, argspec):
    """"""Compile parsed tree of function arguments into list or dict of
    (func, data) pairs

    >>> context = engine(lambda t: (runsymbol, t))
    >>> def fargs(expr, argspec):
    ...     x = _parseexpr(expr)
    ...     n = getsymbol(x[1])
    ...     return _buildfuncargs(x[2], context, exprmethods, n, argspec)
    >>> list(fargs('a(l=1, k=2)', 'k l m').keys())
    ['l', 'k']
    >>> args = fargs('a(opts=1, k=2)', '**opts')
    >>> list(args.keys()), list(args['opts'].keys())
    (['opts'], ['opts', 'k'])
    """"""","
    def compiledict(xs):
        return util.sortdict(
            (k, compileexp(x, context, curmethods)) for k, x in pycompat.iteritems(xs)
        )

    def compilelist(xs):
        return [compileexp(x, context, curmethods) for x in xs]

    if not argspec:
        # filter or function with no argspec: return list of positional args
        return compilelist(getlist(exp))

    # function with argspec: return dict of named args
    _poskeys, varkey, _keys, optkey = argspec = parser.splitargspec(argspec)
    treeargs = parser.buildargsdict(
        getlist(exp), funcname, argspec, keyvaluenode=""keyvalue"", keynode=""symbol""
    )
    compargs = util.sortdict()
    if varkey:
        compargs[varkey] = compilelist(treeargs.pop(varkey))
    if optkey:
        compargs[optkey] = compiledict(treeargs.pop(optkey))
    compargs.update(compiledict(treeargs))
    return compargs"
359,"    def set_strict_kwargs_checking(self, enable: bool) -> None:
        """"""
        Toggles strict kwargs checking. If enabled, a ValueError is thrown if any
        unused parameters are passed to a PathHandler function. If disabled, only
        a warning is given.

        With a centralized file API, there's a tradeoff of convenience and
        correctness delegating arguments to the proper I/O layers. An underlying
        `PathHandler` may support custom arguments which should not be statically
        exposed on the `PathManager` function. For example, a custom `HTTPURLHandler`
        may want to expose a `cache_timeout` argument for `open()` which specifies
        how old a locally cached resource can be before it's refetched from the
        remote server. This argument would not make sense for a `NativePathHandler`.
        If strict kwargs checking is disabled, `cache_timeout` can be passed to
        `PathManager.open` which will forward the arguments to the underlying
        handler. By default, checking is enabled since it is innately unsafe:
        multiple `PathHandler`s could reuse arguments with different semantic
        meanings or types.

        Args:
            enable (bool)
        """"""","        self._native_path_handler._strict_kwargs_check = enable
        for handler in self._path_handlers.values():
            handler._strict_kwargs_check = enable"
360,"    def save(self, dir_name: str, file_name: str):
        """"""
        Save appropriate files.

        :param dir_name:
            directory to save.
        :param file_name:
            file to save.
        """"""","        self.tokenizer.save_model(dir_name, file_name)"
361,"def forget(ui, repo, *pats, **opts):
    """"""stop tracking the specified files

    Mark the specified files so they will no longer be tracked
    after the next commit.

    Forget does not delete the files from the working copy. To delete
    the file from the working copy, see :prog:`remove`.

    Forget does not remove files from the repository history. The files
    will only be removed in the next commit and its descendants.

    To undo a forget before the next commit, see :prog:`add`.

    .. container:: verbose

      Examples:

      - forget newly-added binary files::

          @prog@ forget ""set:added() and binary()""

      - forget files that would be excluded by .gitignore::

          @prog@ forget ""set:gitignore()""

    Returns 0 on success.
    """"""","
    if not pats:
        raise error.Abort(_(""no files specified""))

    m = scmutil.match(repo[None], pats, opts)
    rejected = cmdutil.forget(ui, repo, m, prefix="""", explicitonly=False)[0]
    return rejected and 1 or 0"
362,"    def joinpath(self, *args):
        """"""Combine this path with one or several arguments, and return a
        new path representing either a subpath (if all arguments are relative
        paths) or a totally different path (if one of the arguments is
        anchored).
        """"""",        return self._make_child(args)
363,"    def width(self):
        """"""Return the total width of the operation.""""""","        if not self._tasks:
            return 0
        task = self._tasks[-1]
        if len(self._tasks) > 1:
            # scale up the overall width by the current task or preserve it if
            # no current width is known.
            return task[3] * (task[1] or 1)
        else:
            return task[1]"
364,"def currentworkspacewithusernamecheck(repo):
    """"""
    Returns the currently connected workspace, or None if the repo is not
    connected to a workspace and the flag if it requires username migration
    because the workspace name doesn't match the current username anymore.
    """"""","
    (current_workspace, locally_owned) = currentworkspacewithlocallyownedinfo(repo)
    migrationrequired = locally_owned and not current_workspace.startswith(
        userworkspaceprefix(repo.ui)
    )
    return (current_workspace, migrationrequired)"
365,"    def get_task_agent(self):
        """"""
        Not possible/well-defined in this setting.
        """"""",        return self.worlds[self.world_idx].get_task_agent()
366,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Override to add personality-override option.
        """"""","        TransresnetMultimodalModel.add_cmdline_args(parser, partial_opt=partial_opt)
        super().add_cmdline_args(parser, partial_opt=partial_opt)
        arg_group = parser.add_argument_group(""TransresnetMultimodal Arguments"")
        parser.add_argument(
            ""--personality-override"",
            type=str,
            default=None,
            help=""for use in other tasks where no personality ""
            ""is given. This will give the model a personality ""
            ""(whichever is specified)."",
        )
        parser.add_argument(
            ""--personalities-path"",
            type=str,
            default=None,
            help=""Path to personalities list"",
        )
        return arg_group"
367,"    def get_code_offset(self):
        """"""Get the code offset for this method.""""""",        return self.encoded_method.code_off
368,"    def hsh2rev(self, hsh):
        """"""convert hg hash to linelog revision. return None if not found.""""""",        return self._hsh2rev.get(hsh)
369,"    def test_compare_opts(self):
        """"""
        Compare opts by loading them with Opt.load().

        Will not compare the override field.
        """"""","
        with testing_utils.tempdir() as tmpdir:
            # Write test opts
            opt_dir = tmpdir
            opt_path_1 = os.path.join(opt_dir, '1.opt')
            opt_path_2 = os.path.join(opt_dir, '2.opt')
            with open(opt_path_1, 'w') as f1:
                json.dump(self.compare_opt_1, f1)
            with open(opt_path_2, 'w') as f2:
                json.dump(self.compare_opt_2, f2)

            # Compare opts
            output = compare_opts(opt_path_1=opt_path_1, opt_path_2=opt_path_2)
            desired_output = """"""
Args only found in opt 1:
key2: a

Args only found in opt 2:
key3: b

Args that are different in both opts:
key1:
\tIn opt 1: 0
\tIn opt 2: 1""""""
            self.assertEqual(output, desired_output)"
370,"def capture_output():
    """"""
    Suppress all logging output into a single buffer.

    Use as a context manager.

    >>> with capture_output() as output:
    ...     print('hello')
    >>> output.getvalue()
    'hello'
    """"""","    sio = io.StringIO()
    with contextlib.redirect_stdout(sio), contextlib.redirect_stderr(sio):
        yield sio"
371,"def get_rank():
    """"""
    Returns the rank of the current worker.

    Returns 0 if not in distributed.
    """"""","    if not is_distributed():
        return 0
    else:
        return dist.get_rank()"
372,"def get_output_dir(datasets, training=True):
    """"""Get the output directory determined by the current global config.""""""","    assert isinstance(datasets, tuple([tuple, list] + list(six.string_types))), \
        'datasets argument must be of type tuple, list or string'
    is_string = isinstance(datasets, six.string_types)
    dataset_name = datasets if is_string else ':'.join(datasets)
    tag = 'train' if training else 'test'
    # <output-dir>/<train|test>/<dataset-name>/<model-type>/
    outdir = osp.join(__C.OUTPUT_DIR, tag, dataset_name, __C.MODEL.TYPE)
    if not osp.exists(outdir):
        os.makedirs(outdir)
    return outdir"
373,"    def _recv_to_buffer_with_copy(self, num_bytes, buf):
        """"""
        Receives num_bytes from the nailgun socket and writes them into the specified buffer.
        """"""","        bytes_read = 0
        while bytes_read < num_bytes:
            recv_buf = self.transport.recv(num_bytes - bytes_read)
            if not len(recv_buf):
                raise NailgunException(
                    ""Server unexpectedly disconnected in recv()"",
                    NailgunException.CONNECTION_BROKEN,
                )
            buf[bytes_read : bytes_read + len(recv_buf)] = recv_buf
            bytes_read += len(recv_buf)"
374,"def wait_for_shutdown(
    pid: int, timeout: float, kill_timeout: float = DEFAULT_SIGKILL_TIMEOUT
) -> bool:
    """"""Wait for a process to exit.

    If it does not exit within `timeout` seconds kill it with SIGKILL.
    Returns True if the process exited on its own or False if it only exited
    after SIGKILL.

    Throws a ShutdownError if we failed to kill the process with SIGKILL
    (either because we failed to send the signal, or if the process still did
    not exit within kill_timeout seconds after sending SIGKILL).
    """"""","    # Wait until the process exits on its own.
    if wait_for_process_exit(pid, timeout):
        return True

    # client.shutdown() failed to terminate the process within the specified
    # timeout.  Take a more aggressive approach by sending SIGKILL.
    print_stderr(
        ""error: sent shutdown request, but edenfs did not exit ""
        ""within {} seconds. Attempting SIGKILL."",
        timeout,
    )
    sigkill_process(pid, timeout=kill_timeout)
    return False"
375,"def filter_images_with_few_keypoints(dataset_dicts, min_keypoints_per_image):
    """"""
    Filter out images with too few number of keypoints.

    Args:
        dataset_dicts (list[dict]): annotations in Detectron2 Dataset format.

    Returns:
        list[dict]: the same format as dataset_dicts, but filtered.
    """"""","    num_before = len(dataset_dicts)

    def visible_keypoints_in_image(dic):
        # Each keypoints field has the format [x1, y1, v1, ...], where v is visibility
        annotations = dic[""annotations""]
        return sum(
            (np.array(ann[""keypoints""][2::3]) > 0).sum()
            for ann in annotations
            if ""keypoints"" in ann
        )

    dataset_dicts = [
        x for x in dataset_dicts if visible_keypoints_in_image(x) >= min_keypoints_per_image
    ]
    num_after = len(dataset_dicts)
    logger = logging.getLogger(__name__)
    logger.info(
        ""Removed {} images with fewer than {} keypoints."".format(
            num_before - num_after, min_keypoints_per_image
        )
    )
    return dataset_dicts"
376,"def parsemeta(text):
    """"""return (metadatadict, metadatasize)""""""","    # text can be buffer, so we can't use .startswith or .index
    if bytes(text[:2]) != b""\1\n"":
        return None, None
    s = _mdre.search(text, 2).start()
    mtext = decodeutf8(text[2:s])
    meta = {}
    for l in mtext.splitlines():
        k, v = l.split("": "", 1)
        meta[k] = v
    return meta, (s + 2)"
377,"def mimeencode(
    ui,
    s: Union[List[email.message.Message], bytes, str],
    charsets=None,
    display: bool = False,
) -> email.message.Message:
    """"""creates mime text object, encodes it if needed, and sets
    charset and transfer-encoding accordingly.""""""","    cs = ""us-ascii""
    if not display:
        s, cs = _encode(ui, s, charsets)
    return mimetextqp(s, ""plain"", cs)"
378,"def foreground(repo, nodes):
    """"""Returns all nodes in the ""foreground"" of the given nodes.

    The foreground of a commit is the transitive closure of all descendants
    and successors of the commit.
    """"""","    unfi = repo
    nm = unfi.changelog.nodemap
    foreground = set(nodes)
    newnodes = set(nodes)
    while newnodes:
        newnodes.update(n for n in allsuccessors(repo, newnodes) if n in nm)
        newnodes.update(unfi.nodes(""%ln::"", newnodes))
        newnodes.difference_update(foreground)
        foreground.update(newnodes)
    return foreground"
379,"    def map(self, items, timeout=None):
        """"""Enqueues `items` into the queue""""""","        futures = map(self.submit, items)
        return [f.result(timeout) for f in futures]"
380,"    def reset(self):
        """"""
        Preparation for a new round of evaluation.
        Should be called before starting a round of evaluation.
        """"""",        pass
381,"def findblocks(text):
    """"""Find continuous blocks of lines in text.

    Returns a list of dictionaries representing the blocks. Each block
    has an 'indent' field and a 'lines' field.
    """"""","    blocks = []
    for b in _blockre.split(text.lstrip(""\n"").rstrip()):
        lines = b.splitlines()
        if lines:
            indent = min((len(l) - len(l.lstrip())) for l in lines)
            lines = [l[indent:] for l in lines]
            blocks.append({""indent"": indent, ""lines"": lines})
    return blocks"
382,"    def setup_data(self, datafile: str):
        """"""
        The core method which the user should override.

        Yields the data, one message at a time, as well as markers indicating
        new episodes.

        :param str datafile:
            If the initializer set a 'datafile' field within the initialization,
            this will be provided here. Otherwise, datafile will be the fold:
            either ""train"", ""valid"", or ""test"".

        :return:
            Yields pairs (message, new_episode) containing a Message object
            and whether the message marks the beginning of a totally new
            episode.
        """"""",        pass
383,"def ruleeditor(repo, ui, actions, editcomment=""""):
    """"""open an editor to edit rules

    rules are in the format [ [act, ctx], ...] like in state.rules
    """"""","    if repo.ui.configbool(""experimental"", ""histedit.autoverb""):
        newact = util.sortdict()
        for act in actions:
            ctx = repo[act.node]
            summary = _getsummary(ctx)
            fword = summary.split("" "", 1)[0].lower()
            added = False

            # if it doesn't end with the special character '!' just skip this
            if fword.endswith(""!""):
                fword = fword[:-1]
                if fword in primaryactions | secondaryactions | tertiaryactions:
                    act.verb = fword
                    # get the target summary
                    tsum = summary[len(fword) + 1 :].lstrip()
                    # safe but slow: reverse iterate over the actions so we
                    # don't clash on two commits having the same summary
                    for na, l in reversed(list(pycompat.iteritems(newact))):
                        actx = repo[na.node]
                        asum = _getsummary(actx)
                        if asum == tsum:
                            added = True
                            l.append(act)
                            break

            if not added:
                newact[act] = []

        # copy over and flatten the new list
        actions = []
        for na, l in pycompat.iteritems(newact):
            actions.append(na)
            actions += l

    rules = ""\n"".join([act.torule() for act in actions])
    rules += ""\n\n""
    rules += editcomment
    rules = ui.edit(
        rules,
        ui.username(),
        {""prefix"": ""histedit""},
        repopath=repo.path,
        action=""histedit"",
    )

    # Save edit rules in .hg/histedit-last-edit.txt in case
    # the user needs to ask for help after something
    # surprising happens.
    f = open(repo.localvfs.join(""histedit-last-edit.txt""), ""w"")
    f.write(rules)
    f.close()

    return rules"
384,"def gen_init_net_from_blobs(blobs, blobs_to_use=None, excluded_blobs=None):
    ''' Generate an initialization net based on a blob dict '''","    ret = caffe2_pb2.NetDef()
    if blobs_to_use is None:
        blobs_to_use = {x for x in blobs}
    else:
        blobs_to_use = copy.deepcopy(blobs_to_use)
    if excluded_blobs is not None:
        blobs_to_use = [x for x in blobs_to_use if x not in excluded_blobs]
    for name in blobs_to_use:
        blob = blobs[name]
        if isinstance(blob, str):
            print('Blob {} with type {} is not supported in generating init net,'
                  ' skipped.'.format(name, type(blob)))
            continue
        add_tensor(ret, name, blob)

    return ret"
385,"    def train_step(self, batch):
        """"""
        Null output.
        """"""",        return Output()
386,"def fix_incremental_state(
    generation_model: str, incremental_state: Dict[int, Any]
) -> Dict[int, Any]:
    """"""
    Fix incremental state. Essentially takes BART into account.

    :param generation_model:
        generation model
    :param incremental_state:
        incremental decoded state
    """"""","    if generation_model == 'bart':
        for incr_state_l in incremental_state.values():
            assert 'self_attn' in incr_state_l
            assert 'prev_mask' in incr_state_l['self_attn']
            self_attn_mask = incr_state_l['self_attn']['prev_mask']
            # check this is on the very first run with incremental state
            if self_attn_mask.ndim == 3 and tuple(self_attn_mask.shape[1:]) == (2, 2):
                # cut off the inappropriate incremental state
                incr_state_l['self_attn']['prev_mask'] = self_attn_mask[:, -1:, :]
    elif generation_model == 't5':
        # No current solution for t5 exists.
        incremental_state = {}

    return incremental_state"
387,"    def _processparam(self, name: str, value: ""Optional[str]"") -> None:
        """"""process a parameter, applying its effect if needed

        Parameter starting with a lower case letter are advisory and will be
        ignored when unknown.  Those starting with an upper case letter are
        mandatory and will this function will raise a KeyError when unknown.

        Note: no option are currently supported. Any input will be either
              ignored or failing.
        """"""","        if not name:
            raise ValueError(r""empty parameter name"")
        if name[0:1] not in pycompat.bytestr(string.ascii_letters):
            raise ValueError(r""non letter first character: %s"" % name)
        try:
            handler = b2streamparamsmap[name.lower()]
        except KeyError:
            if name[0:1].islower():
                indebug(self.ui, ""ignoring unknown parameter %s"" % name)
            else:
                raise error.BundleUnknownFeatureError(params=(name,))
        else:
            handler(self, name, value)"
388,"    def get_reasons(self) -> List[str]:
        """"""
        Return dataframe reasons.
        """"""",        return self.dataframe['reason'].values.tolist()
389,"    def test_rollout_no_archive(self, env_name, rollout_len):
        """"""Tests rollout_len steps (or until termination) of random policy.""""""","        env = gym.make(env_name, savedir=None)
        assert env.savedir is None
        assert env._stats_file is None
        assert env._stats_logger is None
        rollout_env(env, rollout_len)"
390,"    def inference_single_image(
        self,
        anchors: List[Boxes],
        box_cls: List[torch.Tensor],
        box_delta: List[torch.Tensor],
        image_size: Tuple[int, int],
    ):
        """"""
        Identical to :meth:`RetinaNet.inference_single_image.
        """"""","        pred = self._decode_multi_level_predictions(
            anchors,
            box_cls,
            box_delta,
            self.test_score_thresh,
            self.test_topk_candidates,
            image_size,
        )
        keep = batched_nms(
            pred.pred_boxes.tensor, pred.scores, pred.pred_classes, self.test_nms_thresh
        )
        return pred[keep[: self.max_detections_per_image]]"
391,"    def test_text_task(self):
        """"""
        Test that model correctly handles text task.

        Random chance is 10%, so this should be able to get much better than that very
        quickly.
        """"""","        args = Opt({**self.base_args, **self.text_args})
        valid, test = testing_utils.train_model(args)
        assert (
            valid['accuracy'] > 0.1
        ), f'ImagePolyencoderAgent val-set accuracy on a simple task was {valid[""accuracy""].value():0.2f}.'"
392,"def random_color(rgb=False, maximum=255):
    """"""
    Args:
        rgb (bool): whether to return RGB colors or BGR colors.
        maximum (int): either 255 or 1

    Returns:
        ndarray: a vector of 3 numbers
    """"""","    idx = np.random.randint(0, len(_COLORS))
    ret = _COLORS[idx] * maximum
    if not rgb:
        ret = ret[::-1]
    return ret"
393,"    def _run_user(self, fn, *args, **kwargs):
        """"""Run a user supplied function.

        Exceptions are processed by `_got_user_exception`.

        :return: Either whatever 'fn' returns or ``exception_caught`` if
            'fn' raised an exception.
        """"""","        try:
            return fn(*args, **kwargs)
        except KeyboardInterrupt:
            raise
        except:
            return self._got_user_exception(sys.exc_info())"
394,"    def get_frontend_args(self) -> Dict[str, Any]:
        """"""
        Specifies what options within a task_config should be forwarded to the client
        for use by the task's frontend.
        """"""","        return {
            ""task_description"": ""Placeholder Task Description - Javascript failed to load"",
            ""frame_height"": 650,
            ""num_subtasks"": self.args.blueprint.subtasks_per_unit,
            ""question"": self.args.blueprint.eval_question,
            ""block_mobile"": True,
            ""get_task_feedback"": False,  # TODO(#95) make option
            ""additional_task_description"": self.args.blueprint.additional_task_description,
        }"
395,"    def remove_tail(self, min_freq):
        """"""
        Remove elements below the frequency cutoff from the dictionary.
        """"""","        to_remove = []
        for token, freq in self.freq.items():
            if freq < min_freq:
                # queue up removals since can't mutate dict during iteration
                to_remove.append(token)

        for token in to_remove:
            del self.freq[token]
            idx = self.tok2ind.pop(token)
            del self.ind2tok[idx]"
396,"def load_passage_reader(
    ctx_file: str, return_dict: bool = True
) -> Union[Dict[str, Tuple[str, str]], List[Tuple[str, str, str]]]:
    """"""
    Load passages from file, corresponding to a FAISS index.

    We attempt to read the passages with a csv reader.

    If passage files are not saved correctly with a csv reader,
    reads can fail.

    :param ctxt_file:
        file to read

    :return reader:
        return a reader over the passages
    """"""","    logging.info(f'Reading data from: {ctx_file}')
    f_open = gzip.open if ctx_file.endswith("".gz"") else open
    try:
        passages = {} if return_dict else []
        with f_open(ctx_file) as tsvfile:
            _reader = csv.reader(tsvfile, delimiter='\t')  # type: ignore
            ids = []
            for idx, row in tqdm(enumerate(_reader)):
                if idx == 0:
                    assert row[0] == 'id'
                    ids.append(-1)
                elif idx <= 1:
                    ids.append(row[0])
                    if return_dict:
                        passages[row[0]] = (row[1], row[2])  # type: ignore
                    else:
                        passages.append((row[0], row[1], row[2]))  # type: ignore
                    continue
                else:
                    assert int(row[0]) == int(ids[idx - 1]) + 1, ""invalid load""
                    if return_dict:
                        passages[row[0]] = (row[1], row[2])  # type: ignore
                    else:
                        passages.append((row[0], row[1], row[2]))  # type: ignore
                    ids.append(row[0])

        del ids
    except (csv.Error, AssertionError) as e:
        passages = {} if return_dict else []
        logging.error(f'Exception: {e}')
        logging.warning('Error in loading csv; loading via readlines')
        with f_open(ctx_file) as tsvfile:
            for idx, l in tqdm(enumerate(tsvfile.readlines())):
                line = l.replace('\n', '').split('\t')  # type: ignore
                assert len(line) == 3
                if idx == 0:
                    assert line[0] == 'id'
                if line[0] != 'id':
                    if return_dict:
                        passages[line[0]] = (line[1], line[2])  # type: ignore
                    else:
                        passages.append((line[0], line[1], line[2]))  # type: ignore
    return passages"
397,"    def setup_episodes(self, fold):
        """"""
        Parses into TodStructuredEpisode.
        """"""","        domains = self.opt.get(""msre2e_domains"", DOMAINS)
        chunks = self._load_data(fold, domains)
        domains_cnt = Counter()
        episodes = []
        for utterances in chunks:
            if len(utterances) < 1:
                continue
            domain = utterances[0][""domain""]
            domains_cnt[domain] += 1
            idx = 0
            rounds = []
            goal_calls = []
            if len(utterances) > 0 and utterances[0][""speaker""] == ""agent"":
                idx, sys_utt, api_resp = self._get_utterance_and_api_call_for_speaker(
                    ""agent"", utterances, idx
                )
                r = tod.TodStructuredRound(
                    user_utt=tod.CONST_SILENCE,
                    api_resp_machine=api_resp,
                    sys_utt=sys_utt,
                )
                rounds.append(r)

            cum_api_call = {}
            while idx < len(utterances):
                idx, user_utt, api_call = self._get_utterance_and_api_call_for_speaker(
                    ""user"", utterances, idx
                )
                idx, sys_utt, api_resp = self._get_utterance_and_api_call_for_speaker(
                    ""agent"", utterances, idx
                )
                if not self.opt[""use_cumulative_api_calls""]:
                    r = tod.TodStructuredRound(
                        user_utt=user_utt,
                        api_call_machine=api_call,
                        api_resp_machine=api_resp,
                        sys_utt=sys_utt,
                    )
                else:
                    cum_api_call.update(api_call)
                    r = tod.TodStructuredRound(
                        user_utt=user_utt,
                        api_call_machine=copy.deepcopy(cum_api_call)
                        if len(api_resp) > 0
                        else {},
                        api_resp_machine=api_resp if len(api_resp) > 0 else {},
                        sys_utt=sys_utt,
                    )

                rounds.append(r)
                if len(api_call) > 0:
                    goal_calls.append(api_call)

            episode = tod.TodStructuredEpisode(
                domain=domain,
                api_schemas_machine=SLOT_NAMES[domain],
                goal_calls_machine=goal_calls,
                rounds=rounds,
                delex=self.opt.get(""delex"", False),
            )
            episodes.append(episode)
        return episodes"
398,"    def default_sizes(cls, scale):
        """"""Convert single scale parameter to a dict of arguments for build.""""""","        raise RuntimeError('Using ""scale"" is not supported for %s' %
                           cls.__name__)"
399,"def _hostsettings(ui, hostname) -> Dict[str, Any]:
    """"""Obtain security settings for a hostname.

    Returns a dict of settings relevant to that hostname.
    """"""","    s = {
        # Whether we should attempt to load default/available CA certs
        # if an explicit ``cafile`` is not defined.
        ""allowloaddefaultcerts"": True,
        # List of 2-tuple of (hash algorithm, hash).
        ""certfingerprints"": [],
        # Path to file containing concatenated CA certs. Used by
        # SSLContext.load_verify_locations().
        ""cafile"": None,
        # Whether certificate verification should be disabled.
        ""disablecertverification"": False,
        # Whether the legacy [hostfingerprints] section has data for this host.
        ""legacyfingerprint"": False,
        # PROTOCOL_* constant to use for SSLContext.__init__.
        ""protocol"": None,
        # String representation of minimum protocol to be used for UI
        # presentation.
        ""protocolui"": None,
        # ssl.CERT_* constant used by SSLContext.verify_mode.
        ""verifymode"": None,
        # Defines extra ssl.OP* bitwise options to set.
        ""ctxoptions"": None,
        # OpenSSL Cipher List to use (instead of default).
        ""ciphers"": None,
    }

    # Allow minimum TLS protocol to be specified in the config.
    def validateprotocol(protocol, key):
        if protocol not in configprotocols:
            raise error.Abort(
                _(""unsupported protocol from hostsecurity.%s: %s"") % (key, protocol),
                hint=_(""valid protocols: %s"") % "" "".join(sorted(configprotocols)),
            )

    # We default to TLS 1.1+ where we can because TLS 1.0 has known
    # vulnerabilities (like BEAST and POODLE). We allow users to downgrade to
    # TLS 1.0+ via config options in case a legacy server is encountered.
    if ""tls1.1"" in supportedprotocols:
        defaultprotocol = ""tls1.1""
    else:
        # Let people know they are borderline secure.
        # We don't document this config option because we want people to see
        # the bold warnings on the web site.
        # internal config: hostsecurity.disabletls10warning
        if not ui.configbool(""hostsecurity"", ""disabletls10warning""):
            ui.warn(
                _(
                    ""warning: connecting to %s using legacy security ""
                    ""technology (TLS 1.0); see ""
                    ""https://mercurial-scm.org/wiki/SecureConnections for ""
                    ""more info\n""
                )
                % hostname
            )
        defaultprotocol = ""tls1.0""

    key = ""minimumprotocol""
    protocol = ui.config(""hostsecurity"", key, defaultprotocol)
    validateprotocol(protocol, key)

    key = ""%s:minimumprotocol"" % hostname
    protocol = ui.config(""hostsecurity"", key, protocol)
    validateprotocol(protocol, key)

    # If --insecure is used, we allow the use of TLS 1.0 despite config options.
    # We always print a ""connection security to %s is disabled..."" message when
    # --insecure is used. So no need to print anything more here.
    if ui.insecureconnections:
        protocol = ""tls1.0""

    s[""protocol""], s[""ctxoptions""], s[""protocolui""] = protocolsettings(protocol)

    ciphers = ui.config(""hostsecurity"", ""ciphers"")
    ciphers = ui.config(""hostsecurity"", ""%s:ciphers"" % hostname, ciphers)
    s[""ciphers""] = ciphers

    # Look for fingerprints in [hostsecurity] section. Value is a list
    # of <alg>:<fingerprint> strings.
    fingerprints = ui.configlist(""hostsecurity"", ""%s:fingerprints"" % hostname)
    for fingerprint in fingerprints:
        if not (fingerprint.startswith((""sha1:"", ""sha256:"", ""sha512:""))):
            raise error.Abort(
                _(""invalid fingerprint for %s: %s"") % (hostname, fingerprint),
                hint=_('must begin with ""sha1:"", ""sha256:"", ' 'or ""sha512:""'),
            )

        alg, fingerprint = fingerprint.split("":"", 1)
        fingerprint = fingerprint.replace("":"", """").lower()
        # pyre-fixme[16]: Item `None` of `Union[None, List[typing.Any], bool]` has
        #  no attribute `append`.
        s[""certfingerprints""].append((alg, fingerprint))

    # Fingerprints from [hostfingerprints] are always SHA-1.
    for fingerprint in ui.configlist(""hostfingerprints"", hostname):
        fingerprint = fingerprint.replace("":"", """").lower()
        # pyre-fixme[16]: Item `None` of `Union[None, List[typing.Any], bool]` has
        #  no attribute `append`.
        s[""certfingerprints""].append((""sha1"", fingerprint))
        s[""legacyfingerprint""] = True

    # If a host cert fingerprint is defined, it is the only thing that
    # matters. No need to validate CA certs.
    if s[""certfingerprints""]:
        # pyre-fixme[6]: For 2nd param expected `Union[None, List[typing.Any],
        #  bool]` but got `VerifyMode`.
        s[""verifymode""] = ssl.CERT_NONE
        s[""allowloaddefaultcerts""] = False

    # If --insecure is used, don't take CAs into consideration.
    elif ui.insecureconnections:
        s[""disablecertverification""] = True
        # pyre-fixme[6]: For 2nd param expected `Union[None, List[typing.Any],
        #  bool]` but got `VerifyMode`.
        s[""verifymode""] = ssl.CERT_NONE
        s[""allowloaddefaultcerts""] = False

    if ui.configbool(""devel"", ""disableloaddefaultcerts""):
        s[""allowloaddefaultcerts""] = False

    # If both fingerprints and a per-host ca file are specified, issue a warning
    # because users should not be surprised about what security is or isn't
    # being performed.
    cafile = ui.config(""hostsecurity"", ""%s:verifycertsfile"" % hostname)
    if s[""certfingerprints""] and cafile:
        ui.warn(
            _(
                ""(hostsecurity.%s:verifycertsfile ignored when host ""
                ""fingerprints defined; using host fingerprints for ""
                ""verification)\n""
            )
            % hostname
        )

    # Try to hook up CA certificate validation unless something above
    # makes it not necessary.
    if s[""verifymode""] is None:
        # Look at per-host ca file first.
        if cafile:
            cafile = util.expandpath(cafile)
            if not os.path.exists(cafile):
                raise error.Abort(
                    _(""path specified by %s does not exist: %s"")
                    % (""hostsecurity.%s:verifycertsfile"" % hostname, cafile)
                )
            s[""cafile""] = cafile
        else:
            # Find global certificates file in config.
            cafile = ui.config(""web"", ""cacerts"")

            if cafile:
                cafile = util.expandpath(cafile)
                if not os.path.exists(cafile):
                    raise error.Abort(_(""could not find web.cacerts: %s"") % cafile)
            elif s[""allowloaddefaultcerts""]:
                # CAs not defined in config. Try to find system bundles.
                cafile = _defaultcacerts(ui)
                if cafile:
                    ui.debug(""using %s for CA file\n"" % cafile)

            s[""cafile""] = cafile

        # Require certificate validation if CA certs are being loaded and
        # verification hasn't been disabled above.
        if cafile or (_canloaddefaultcerts and s[""allowloaddefaultcerts""]):
            # pyre-fixme[6]: For 2nd param expected `Union[None, List[typing.Any],
            #  bool]` but got `VerifyMode`.
            s[""verifymode""] = ssl.CERT_REQUIRED
        else:
            # At this point we don't have a fingerprint, aren't being
            # explicitly insecure, and can't load CA certs. Connecting
            # is insecure. We allow the connection and abort during
            # validation (once we have the fingerprint to print to the
            # user).
            # pyre-fixme[6]: For 2nd param expected `Union[None, List[typing.Any],
            #  bool]` but got `VerifyMode`.
            s[""verifymode""] = ssl.CERT_NONE

    assert s[""protocol""] is not None
    assert s[""ctxoptions""] is not None
    assert s[""verifymode""] is not None

    return s"
400,"    def enable_bpe_dropout(self, enabled: bool):
        """"""
        Used to toggle BPE dropout on (True) or off (False).
        """"""",        self._bpe_dropout_enabled = enabled
401,"    def __init__(self, opt: Opt, dictionary: DictionaryAgent, shared: TShared = None):
        """"""
        Initialize DPR model.

        It is up to subclasses to initialize rerankers.
        """"""","        RagRetrieverReranker.__init__(self, opt, dictionary, shared=shared)
        self.dpr_num_docs = opt['dpr_num_docs']
        assert self.dpr_num_docs
        dpr_opt = copy.deepcopy(opt)
        dpr_opt['n_docs'] = self.dpr_num_docs
        DPRRetriever.__init__(self, dpr_opt, dictionary, shared=shared)"
402,"def perfdiffwd(ui, repo, **opts):
    """"""Profile diff of working directory changes""""""","    timer, fm = gettimer(ui, opts)
    options = {
        ""w"": ""ignore_all_space"",
        ""b"": ""ignore_space_change"",
        ""B"": ""ignore_blank_lines"",
    }

    for diffopt in ("""", ""w"", ""b"", ""B"", ""wB""):
        opts = dict((options[c], ""1"") for c in diffopt)

        def d():
            ui.pushbuffer()
            commands.diff(ui, repo, **opts)
            ui.popbuffer()

        title = ""diffopts: %s"" % (diffopt and (""-"" + diffopt) or ""none"")
        timer(d, title)
    fm.end()"
403,"def patch(obj, attribute, value):
    """"""Set 'obj.attribute' to 'value' and return a callable to restore 'obj'.

    If 'attribute' is not set on 'obj' already, then the returned callable
    will delete the attribute when called.

    :param obj: An object to monkey-patch.
    :param attribute: The name of the attribute to patch.
    :param value: The value to set 'obj.attribute' to.
    :return: A nullary callable that, when run, will restore 'obj' to its
        original state.
    """"""","    patcher = MonkeyPatcher((obj, attribute, value))
    patcher.patch()
    return patcher.restore"
404,"    def has_metadata(name):
        """"""Does the package's distribution contain the named metadata?""""""",
405,"def unwrapfunction(container, funcname, wrapper=None):
    """"""undo wrapfunction

    If wrappers is None, undo the last wrap. Otherwise removes the wrapper
    from the chain of wrappers.

    Return the removed wrapper.
    Raise IndexError if wrapper is None and nothing to unwrap; ValueError if
    wrapper is not None but is not found in the wrapper chain.
    """"""","    chain = getwrapperchain(container, funcname)
    origfn = chain.pop()
    if wrapper is None:
        wrapper = chain[0]
    chain.remove(wrapper)
    setattr(container, funcname, origfn)
    for w in reversed(chain):
        wrapfunction(container, funcname, w)
    return wrapper"
406,"def extdatasource(repo, source):
    """"""Gather a map of rev -> value dict from the specified source

    A source spec is treated as a URL, with a special case shell: type
    for parsing the output from a shell command.

    The data is parsed as a series of newline-separated records where
    each record is a revision specifier optionally followed by a space
    and a freeform string value. If the revision is known locally, it
    is converted to a rev, otherwise the record is skipped.

    Note that both key and value are treated as UTF-8 and converted to
    the local encoding. This allows uniformity between local and
    remote data sources.
    """"""","
    spec = repo.ui.config(""extdata"", source)
    if not spec:
        raise error.Abort(_(""unknown extdata source '%s'"") % source)

    data = {}
    src = proc = None
    try:
        if spec.startswith(""shell:""):
            # external commands should be run relative to the repo root
            cmd = spec[6:]
            proc = subprocess.Popen(
                cmd,
                shell=True,
                bufsize=-1,
                close_fds=util.closefds,
                stdout=subprocess.PIPE,
                cwd=repo.root,
            )
            src = proc.stdout
        else:
            # treat as a URL or file
            src = url.open(repo.ui, spec)
        for l in src:
            if b"" "" in l:
                k, v = l.strip().split(b"" "", 1)
            else:
                k, v = l.strip(), b""""

            k = k.decode(""utf8"")
            try:
                data[repo[k].rev()] = v.decode(""utf8"")
            except (error.LookupError, error.RepoLookupError):
                pass  # we ignore data for nodes that don't exist locally
    finally:
        if proc:
            proc.communicate()
        if src:
            src.close()
    if proc and proc.returncode != 0:
        raise error.Abort(
            _(""extdata command '%s' failed: %s"")
            % (cmd, util.explainexit(proc.returncode)[0])
        )

    return data"
407,"    def __init__(self, embeddings, hidden_size):
        """"""
        Initialization.

        Arguments here can be used to provide hyperparameters.
        """"""","        super().__init__()
        self.embeddings = embeddings
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
        )"
408,"def get_multi_run_analyzer(opt) -> MultiRunAcuteAnalyzer:
    """"""
    Return an object to analyze the results of multiple runs simultaneously.

    Load HITs from each run into a separate dataframe, and then pass all dataframes into
    a separate analyzer class that will concatenate them.
    """"""","
    run_ids = opt['run_ids'].split(',')

    # Define paths
    assert (
        opt['outdir'] is not None
    ), '--outdir must be specified when combining results of multiple runs!'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    opt['outdir'] = os.path.join(opt['outdir'], f'combined_runs_{timestamp}')
    os.makedirs(opt['outdir'], exist_ok=True)
    run_id_list_path = os.path.join(opt['outdir'], 'run_ids.txt')

    # Save a simple list of all run IDs stitched together
    with open(run_id_list_path, 'w') as f:
        for run_id in run_ids:
            f.write(run_id + '\n')

    # Loop loading HITs over all run ids into dataframes
    dataframes = {}
    for run_id in run_ids:
        print(f'\nStarting to load HITs for run ID {run_id}.')
        opt_copy = deepcopy(opt)
        opt_copy['run_ids'] = run_id
        dataframes[run_id] = AcuteAnalyzer(opt_copy).dataframe

    return MultiRunAcuteAnalyzer(opt=opt, dataframes=dataframes)"
409,"    def _consumemanifest(self):
        """"""Consumes the manifest portion of the bundle, setting filestart so the
        file portion can be read.""""""","        self._cgunpacker.seek(self.manstart)
        self._cgunpacker.manifestheader()
        for delta in self._cgunpacker.deltaiter():
            pass

        # Changegroup v3 supports additional manifest entries that we need to
        # skip.
        if self._cgunpacker.version == ""03"":
            for chunkdata in iter(self._cgunpacker.filelogheader, {}):
                # If we get here, there are directory manifests in the changegroup
                for delta in self._cgunpacker.deltaiter():
                    pass

        self.filestart = self._cgunpacker.tell()"
410,"    def connection_made(self, transport):
        """"""Implements asyncio.Protocol.connection_made.""""""","        assert self.transport is None, ""Thrift transport already instantiated here.""
        assert self.client is None, ""Client already instantiated here.""
        self.transport = self.wrapAsyncioTransport(transport)
        thrift_protocol = self.THEADER_PROTOCOL_FACTORY(
            client_type=self.client_type,
        ).getProtocol(self.transport)
        thrift_protocol.trans.set_header_flag(HEADER_FLAG.SUPPORT_OUT_OF_ORDER)
        self.client = self.client_class(thrift_protocol, self.loop)"
411,"    def resolve(self):
        """"""
        Resolve the entry point from its module and attrs.
        """"""","        module = __import__(self.module_name, fromlist=['__name__'], level=0)
        try:
            return functools.reduce(getattr, self.attrs, module)
        except AttributeError as exc:
            raise ImportError(str(exc))"
412,"def synchronize():
    """"""
    Helper function to synchronize (barrier) among all processes when
    using distributed training
    """"""","    if not dist.is_available():
        return
    if not dist.is_initialized():
        return
    world_size = dist.get_world_size()
    if world_size == 1:
        return
    if dist.get_backend() == dist.Backend.NCCL:
        # This argument is needed to avoid warnings.
        # It's valid only for NCCL backend.
        dist.barrier(device_ids=[torch.cuda.current_device()])
    else:
        dist.barrier()"
413,"def evalstringliteral(context, mapping, arg):
    """"""Evaluate given argument as string template, but returns symbol name
    if it is unknown""""""","    func, data = arg
    if func is runsymbol:
        thing = func(context, mapping, data, default=data)
    else:
        thing = func(context, mapping, data)
    return stringify(thing)"
414,"    def _get_local_path(
        self,
        path: str,
        force: bool = False,
        cache_dir: Optional[str] = None,
        **kwargs: Any,
    ) -> str:
        """"""
        This implementation downloads the remote resource and caches it locally.
        The resource will only be downloaded if not previously requested.
        """"""","        self._check_kwargs(kwargs)
        if (
            force
            or path not in self.cache_map
            or not os.path.exists(self.cache_map[path])
        ):
            logger = logging.getLogger(__name__)
            parsed_url = urlparse(path)
            dirname = os.path.join(
                get_cache_dir(cache_dir), os.path.dirname(parsed_url.path.lstrip(""/""))
            )
            filename = path.split(""/"")[-1]
            if len(filename) > self.MAX_FILENAME_LEN:
                filename = filename[:100] + ""_"" + uuid.uuid4().hex

            cached = os.path.join(dirname, filename)
            with file_lock(cached):
                if not os.path.isfile(cached):
                    logger.info(""Downloading {} ..."".format(path))
                    cached = download(path, dirname, filename=filename)
            logger.info(""URL {} cached in {}"".format(path, cached))
            self.cache_map[path] = cached
        return self.cache_map[path]"
415,"    def get_start_cmd(self):
        """""" Return the command string that will start up the server 
            as desired / intended
 
        """"""","
        # We make ourselves bug-compatible with MTR / xtrabackup
        # test runners
        if platform.system() != 'Windows' and os.geteuid() == 0:
            self.server_options.append(""--user=root"")
        server_args = [ ""--no-defaults"" 
                      , self.process_server_options()
                      , ""--open-files-limit=1024""
                      , ""--local-infile""
                      , ""--character-set-server=latin1""
                      , ""--connect-timeout=60""
                      , ""--log-bin-trust-function-creators=1""
                      , ""--key_buffer_size=1M""
                      , ""--sort_buffer=256K""
                      , ""--max_heap_table_size=1M""
                      , ""--loose-innodb_data_file_path=ibdata1:10M:autoextend""
                      , ""--loose-innodb_buffer_pool_size=8M""
                      , ""--loose-innodb_write_io_threads=2""
                      , ""--loose-innodb_read_io_threads=2""
                      , ""--loose-innodb_log_buffer_size=1M""
                      , ""--loose-innodb_log_file_size=5M""
                      , ""--loose-innodb_additional_mem_pool_size=1M""
                      , ""--loose-innodb_log_files_in_group=2""
                      , ""--slave-net-timeout=120""
                      , ""--log-bin=%s"" %(os.path.join(self.logdir,""mysqld-bin""))
                      , ""--binlog-format=ROW""
                      , ""--loose-enable-performance-schema""
                      , ""--loose-performance-schema-max-mutex-instances=10000""
                      , ""--loose-performance-schema-max-rwlock-instances=10000""
                      , ""--loose-performance-schema-max-table-instances=500""
                      , ""--loose-performance-schema-max-table-handles=1000""
                      , ""--loose-enable-performance-schema""
                      , ""--basedir=%s"" %(self.code_tree.basedir)
                      , ""--datadir=%s"" %(self.datadir)
                      , ""--tmpdir=%s""  %(self.tmpdir)
                      , ""--character-sets-dir=%s"" %(self.charsetdir)
                      , self.langstring
                      , ""--ssl-ca=%s"" %(os.path.join(self.std_data,'cacert.pem'))
                      , ""--ssl-cert=%s"" %(os.path.join(self.std_data,'server-cert.pem'))
                      , ""--ssl-key=%s"" %(os.path.join(self.std_data,'server-key.pem'))
                      , ""--port=%d"" %(self.master_port)
                      , ""--socket=%s"" %(self.socket_file)
                      , ""--pid-file=%s"" %(self.pid_file)
                      , ""--default-storage-engine=%s"" %(self.default_storage_engine)
                      # server-id maybe needs fixing, but we want to avoid
                      # the server-id=0 and no replication thing...
                      , ""--server-id=%d"" %(self.get_numeric_server_id()+1)
                      , self.secure_file_string
                      ]

        if not self.version.startswith('5.0'):
            server_args += [ ""--binlog-direct-non-transactional-updates""
                           , ""--general_log=1""
                           , ""--general_log_file=%s"" %(self.general_log_file)
                           , ""--slow_query_log=1""
                           , ""--slow_query_log_file=%s"" %(self.slow_query_log_file)
                           ]
        self.gen_cnf_file(server_args)

        if self.gdb:
            server_args.append('--gdb')
            return self.system_manager.handle_gdb_reqs(self, server_args)
        else:
            return ""%s %s %s & "" % ( self.cmd_prefix
                                   , self.server_path
                                   , "" "".join(server_args)
                                   )"
416,"    def clip_main_grads(self, max_norm):
        """"""
        Clips gradient norm and updates dynamic loss scaler.
        """"""","        self._sync_fp16_grads_to_fp32()
        grad_norm = clip_grad_norm(
            self.fp32_params, max_norm, sync=self._aggregate_gnorms
        )

        # detect overflow and adjust loss scale
        if self.scaler is not None:
            overflow = has_overflow(grad_norm)
            prev_scale = self.scaler.loss_scale
            self.scaler.update_scale(overflow)
            if overflow:
                self.zero_grad()
                if self.scaler.loss_scale <= self.min_loss_scale:
                    # Use FloatingPointError as an uncommon error that parent
                    # functions can safely catch to stop training.
                    self.scaler.loss_scale = prev_scale
                    raise FloatingPointError(
                        (
                            'Minimum loss scale reached ({}). Your loss is probably exploding. '
                            'Try lowering the learning rate, using gradient clipping or '
                            'increasing the batch size.'
                        ).format(self.min_loss_scale)
                    )
                    logging.info(
                        f'Overflow: setting loss scale to {self.scaler.loss_scale}'
                    )

        return grad_norm"
417,"def substitute_prompt(prompt):
    ""Perform substitutions on PROMPT.""","
    result = ''
    plen = len(prompt)
    i = 0
    while i < plen:
        if prompt[i] == '\\':
            i = i + 1
            if i >= plen:
                break
            cmdch = prompt[i]

            if cmdch in prompt_substitutions:
                cmd = prompt_substitutions[cmdch]

                if i + 1 < plen and prompt[i + 1] == '{':
                    j = i + 1
                    while j < plen and prompt[j] != '}':
                        j = j + 1
                    # Just ignore formatting errors.
                    if j >= plen or prompt[j] != '}':
                        arg = None
                    else:
                        arg = prompt[i + 2 : j]
                        i = j
                else:
                    arg = None
                result += str(cmd(arg))
            else:
                # Unrecognized escapes are turned into the escaped
                # character itself.
                result += prompt[i]
        else:
            result += prompt[i]

        i = i + 1

    return result"
418,"def _lines(
    fs: ShellFS,
    paths: List[str],
    stdin: Optional[BinaryIO] = None,
    reporterror: Optional[Callable[[str, str], None]] = None,
) -> Iterator[bytes]:
    """"""yield lines in paths and stdin""""""","    if not paths:
        paths = [""-""]
    for path in paths:
        if path == ""-"":
            if stdin:
                yield from stdin
        else:
            try:
                with fs.open(path, ""rb"") as f:
                    yield from f
            except FileNotFoundError:
                if reporterror is None:
                    raise
                reporterror(path, ""No such file or directory"")
            except NotADirectoryError:
                if reporterror is None:
                    raise
                reporterror(path, ""Not a directory"")"
419,"    def addfilegenerator(self, genid, filenames, genfunc, order=0, location=""""):
        """"""add a function to generates some files at transaction commit

        The `genfunc` argument is a function capable of generating proper
        content of each entry in the `filename` tuple.

        At transaction close time, `genfunc` will be called with one file
        object argument per entries in `filenames`.

        The transaction itself is responsible for the backup, creation and
        final write of such file.

        The `genid` argument is used to ensure the same set of file is only
        generated once. Call to `addfilegenerator` for a `genid` already
        present will overwrite the old entry.

        The `order` argument may be used to control the order in which multiple
        generator will be executed.

        The `location` arguments may be used to indicate the files are located
        outside of the the standard directory for transaction. It should match
        one of the key of the `transaction.vfsmap` dictionary.
        """"""","        # For now, we are unable to do proper backup and restore of custom vfs
        # but for bookmarks that are handled outside this mechanism.
        self._filegenerators[genid] = (order, filenames, genfunc, location)"
420,"def finddate(ui, repo, date):
    """"""Find the tipmost changeset that matches the given date spec""""""","
    df = util.matchdate(date)
    m = scmutil.matchall(repo)
    results = {}

    def prep(ctx, fns):
        d = ctx.date()
        if df(d[0]):
            results[ctx.node()] = d

    for ctx in walkchangerevs(repo, m, {""rev"": None}, prep):
        node = ctx.node()
        if node in results:
            ui.status(
                _(""found revision %s from %s\n"")
                % (hex(node), util.datestr(results[node]))
            )
            return node

    raise error.Abort(_(""revision matching date not found""))"
421,"    def get_cache_path(self, archive_name, names=()):
        """"""Return absolute location in cache for `archive_name` and `names`

        The parent directory of the resulting path will be created if it does
        not already exist.  `archive_name` should be the base filename of the
        enclosing egg (which may not be the name of the enclosing zipfile!),
        including its "".egg"" extension.  `names`, if provided, should be a
        sequence of path name parts ""under"" the egg's extraction location.

        This method should only be called by resource providers that need to
        obtain an extraction location, and only for names they intend to
        extract, as it tracks the generated names for possible cleanup later.
        """"""","        extract_path = self.extraction_path or get_default_cache()
        target_path = os.path.join(extract_path, archive_name+'-tmp', *names)
        try:
            _bypass_ensure_directory(target_path)
        except:
            self.extraction_error()

        self._warn_unsafe_extraction_path(extract_path)

        self.cached_files[target_path] = 1
        return target_path"
422,"    def report(self):
        """"""
        Report metrics for all subagents.
        """"""","        metrics = {}
        for a in self.agents:
            if hasattr(a, 'report'):
                m = a.report()
                for k, v in m.items():
                    if k not in metrics:
                        # first agent gets priority in settings values for keys
                        # this way model can't e.g. override accuracy to 100%
                        metrics[k] = v
        if metrics and 'exs' in metrics:
            self.total_exs += metrics['exs'].value()
        return metrics"
423,"def validatesocket(sock) -> None:
    """"""Validate a socket meets security requirements.

    The passed socket must have been created with ``wrapsocket()``.
    """"""","    host = sock._hgstate[""hostname""]
    ui = sock._hgstate[""ui""]
    settings = sock._hgstate[""settings""]

    try:
        peercert = sock.getpeercert(True)
        peercert2 = sock.getpeercert()
    except AttributeError:
        raise error.Abort(_(""%s ssl connection error"") % host)

    if not peercert:
        raise error.Abort(_(""%s certificate error: "" ""no certificate received"") % host)

    if settings[""disablecertverification""]:
        # We don't print the certificate fingerprint because it shouldn't
        # be necessary: if the user requested certificate verification be
        # disabled, they presumably already saw a message about the inability
        # to verify the certificate and this message would have printed the
        # fingerprint. So printing the fingerprint here adds little to no
        # value.
        ui.warn(
            _(
                ""warning: connection security to %s is disabled per current ""
                ""settings; communication is susceptible to eavesdropping ""
                ""and tampering\n""
            )
            % host
        )
        return

    # If a certificate fingerprint is pinned, use it and only it to
    # validate the remote cert.
    peerfingerprints = {
        ""sha1"": hashlib.sha1(peercert).hexdigest(),
        ""sha256"": hashlib.sha256(peercert).hexdigest(),
        ""sha512"": hashlib.sha512(peercert).hexdigest(),
    }

    def fmtfingerprint(s):
        return "":"".join([s[x : x + 2] for x in range(0, len(s), 2)])

    nicefingerprint = ""sha256:%s"" % fmtfingerprint(peerfingerprints[""sha256""])

    if settings[""certfingerprints""]:
        for hash, fingerprint in settings[""certfingerprints""]:
            if peerfingerprints[hash].lower() == fingerprint:
                ui.debug(
                    ""%s certificate matched fingerprint %s:%s\n""
                    % (host, hash, fmtfingerprint(fingerprint))
                )
                if settings[""legacyfingerprint""]:
                    ui.warn(
                        _(
                            ""(SHA-1 fingerprint for %s found in legacy ""
                            ""[hostfingerprints] section; ""
                            ""if you trust this fingerprint, remove the old ""
                            ""SHA-1 fingerprint from [hostfingerprints] and ""
                            ""add the following entry to the new ""
                            ""[hostsecurity] section: %s:fingerprints=%s)\n""
                        )
                        % (host, host, nicefingerprint)
                    )
                return

        # Pinned fingerprint didn't match. This is a fatal error.
        if settings[""legacyfingerprint""]:
            section = ""hostfingerprint""
            nice = fmtfingerprint(peerfingerprints[""sha1""])
        else:
            section = ""hostsecurity""
            # pyre-fixme[61]: `hash` is undefined, or not always defined.
            nice = ""%s:%s"" % (hash, fmtfingerprint(peerfingerprints[hash]))
        raise error.Abort(
            _(""certificate for %s has unexpected "" ""fingerprint %s"") % (host, nice),
            hint=_(""check %s configuration"") % section,
        )

    # Security is enabled but no CAs are loaded. We can't establish trust
    # for the cert so abort.
    if not sock._hgstate[""caloaded""]:
        raise error.Abort(
            _(
                ""unable to verify security of %s (no loaded CA certificates); ""
                ""refusing to connect""
            )
            % host,
            hint=_(
                ""see https://mercurial-scm.org/wiki/SecureConnections for ""
                ""how to configure Mercurial to avoid this error or set ""
                ""hostsecurity.%s:fingerprints=%s to trust this server""
            )
            % (host, nicefingerprint),
        )

    msg = _verifycert(peercert2, host)
    if msg:
        raise error.Abort(
            _(""%s certificate error: %s"") % (host, msg),
            hint=_(
                ""set hostsecurity.%s:certfingerprints=%s ""
                ""config setting or use --insecure to connect ""
                ""insecurely""
            )
            % (host, nicefingerprint),
        )"
424,"    def _attributes_by_value(cls):
        """"""
        :return: Hash of all attributes and their values
        :rtype: dict[int, str]
        """"""","        class_attributes = set(dir(cls)) - set(dir(object))
        return dict(
            [
                (getattr(cls, n), n)
                for n in class_attributes
                if not callable(getattr(cls, n)) and not n.startswith(""__"")
            ]
        )"
425,"def get_fold(eval_setup: str, seed: int
            ) -> Tuple[Tuple[str, ...], Tuple[str, ...], Tuple[str, ...]]:
    """"""Get seed'th fold for specified evaluation setup.

    Args:
        eval_setup: The name of the evaluation setup to use. E.g.,
            ball_cross_template.
        seed: The random seed to create the fold.

    Returns:
        Tuple (train_ids, dev_ids, test_ids)
            Contains task ids to use for each split.

    Raises:
        ValueError: Eval_setup is not valid evaluation setup.
    """"""","    try:
        builder = EVAL_SETUP_BUILDERS[eval_setup]
    except KeyError:
        raise ValueError(f'Unknown eval setup: {eval_setup}. Chose one of'
                         f' {"","".join(EVAL_SETUP_BUILDERS)}')
    _, test_ids = _flatten_eval_setup(builder(seed))
    train_ids, dev_ids = _flatten_eval_setup(builder(seed=seed, dev_seed=seed))
    return train_ids, dev_ids, test_ids"
426,"    def getdoc(obj):
        """"""Get docstring as bytes; may be None so gettext() won't confuse it
        with _('')""""""","        if isinstance(obj, str):
            return obj
        doc = getattr(obj, ""__doc__"", None)
        return doc"
427,"    def readline(self):
        # print(""readline!"")
        """"""may be optimized...""""""","        s=bytes()
        while True:
            c=self.read(1)
            s+=c
        if len(c)==0 or chr(c[0])=='\n':
            return s"
428,"def list_eval_setups() -> Tuple[str, ...]:
    """"""Get a list of names for all known eval setups.""""""",    return tuple(sorted(EVAL_SETUP_BUILDERS))
429,"    def _get_new_data(self, opt):
        """"""
        Load extra positive dialogue data IFF datatype==train.
        """"""","        dt = opt['datatype'].split(':')[0]
        if dt == 'train':
            with PathManager.open(os.path.join(_path(opt), NEW_DATA), 'r') as f:
                data = json.load(f)
            new_data = []
            for ep in data:
                new_ep = []
                for ex in ep:
                    ex['new_data'] = True
                    new_ep.append(Message(ex))
                new_data.append(new_ep)
            return new_data

        return []"
430,"    def _runloop(self):
        """"""Overridden to execute TNonblockingServer's main loop""""""","        while not self.server._stop:
            self.server.serve()
        while not self._stopped:
            time.sleep(0.1)"
431,"def _transform_to_aug(tfm_or_aug):
    """"""
    Wrap Transform into Augmentation.
    Private, used internally to implement augmentations.
    """"""","    assert isinstance(tfm_or_aug, (Transform, Augmentation)), tfm_or_aug
    if isinstance(tfm_or_aug, Augmentation):
        return tfm_or_aug
    else:
        return _TransformToAug(tfm_or_aug)"
432,"    def init_search_query_generator(self, opt):
        """"""
        If a generator model file is specified, we use it.
        """"""","        if opt['search_query_generator_model_file']:
            return super().init_search_query_generator(opt)  # type: ignore
        else:
            logging.warning('Not initializing SQ Generator *within* retriever')"
433,"def allsuccessors(repo, nodes, startdepth=None, stopdepth=None):
    """"""Yields all the nodes that are successors of the given nodes.

    Successors that are not known locally may be omitted.""""""","    depth = 0
    thislevel = set(nodes)
    nextlevel = set()
    seen = set()
    ispublic = getispublicfunc(repo)
    islocal = getislocal(repo)
    while thislevel and (stopdepth is None or depth < stopdepth):
        for current in thislevel:
            if current in seen:
                continue
            seen.add(current)
            if startdepth is None or depth >= startdepth:
                yield current
            if islocal(current) and ispublic(current):
                continue
            succsets = lookupsuccessors(repo, current)
            if succsets:
                nextlevel = nextlevel.union(*succsets)
        depth += 1
        thislevel = nextlevel
        nextlevel = set()"
434,"def _move_valid_files_from_dev_to_valid(dpath):
    """"""
    Files from Google are stored at `nq-dev-##.jsonl.gz` and get untar'd to `nq-
    dev-##.jsonl`.

    The agent expects them to be stored at `nq-valid-00.jsonl`. This moves them over if
    need be.
    """"""","    valid_path = os.path.join(dpath, 'valid')
    for f in os.listdir(valid_path):
        if ""dev"" in f:
            new = f.replace('dev', 'valid')
            os.rename(os.path.join(valid_path, f), os.path.join(valid_path, new))"
435,"def propagate_failure(status):
    """"""
    Propagate the failure mode of a subprocess to the current process.
    """"""","
    # The subprocess died with via a signal, so re-raise it.
    if status < 0:
        os.kill(os.getpid(), -status)

    # The subprocess died with an error code, propagate it.
    if status > 0:
        sys.exit(status)"
436,"    def __init__(
        self,
        num_features,
        embeddingsize,
        hiddensize,
        padding_idx=0,
        rnn_class=""lstm"",
        numlayers=2,
        dropout=0.1,
        bidir_input=False,
        attn_length=-1,
        sparse=False,
    ):
        """"""
        Initialize recurrent decoder.
        """"""","        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.layers = numlayers
        self.hsz = hiddensize
        self.esz = embeddingsize

        self.lt = nn.Embedding(
            num_features, embeddingsize, padding_idx=padding_idx, sparse=sparse
        )
        self.rnn = rnn_class(
            embeddingsize + hiddensize,
            hiddensize,
            numlayers,
            dropout=dropout if numlayers > 1 else 0,
            batch_first=True,
        )"
437,"def distutils_dir_name(dname):
    """"""Returns the name of a distutils build directory""""""","    if dname == ""scripts"":
        # ""scripts"" dir in distutils builds does not contain
        # any platform info, just the Python version
        f = ""{dirname}-{version}""
    else:
        f = ""{dirname}.{platform}-{version}""
    return f.format(
        dirname=dname, platform=distutils.util.get_platform(), version=sys.version[:3]
    )"
438,"    def build_feedforward(
        self,
        dim: int = None,
        dim_hidden: int = None,
        relu_dropout: float = 0,
        activation: str = 'relu',
    ) -> TransformerFFN:
        """"""
        Overridden to allow swapping out of the feedforward class at instantiation.
        """"""","        return self.swappables.feedforward(  # type: ignore
            opt=self.opt,
            dim=dim,
            dim_hidden=dim_hidden,
            relu_dropout=relu_dropout,
            activation=activation,
        )"
439,"def random_colors(N, rgb=False, maximum=255):
    """"""
    Args:
        N (int): number of unique colors needed
        rgb (bool): whether to return RGB colors or BGR colors.
        maximum (int): either 255 or 1

    Returns:
        ndarray: a list of random_color
    """"""","    indices = random.sample(range(len(_COLORS)), N)
    ret = [_COLORS[i] * maximum for i in indices]
    if not rgb:
        ret = [x[::-1] for x in ret]
    return ret"
440,"  def __init__(self, predicate=None, output=sys.stderr, clock=time, prefix=''):
    """"""
      If predicate specified, it should take a ""verbosity"" integer and determine whether
      or not to log, e.g.

        def predicate(verbosity):
          try:
            return verbosity < int(os.environ.get('APP_VERBOSITY', 0))
          except ValueError:
            return False

      output defaults to sys.stderr, but can take any file-like object.
    """"""","    self._predicate = predicate or (lambda verbosity: True)
    self._length = None
    self._output = output
    self._isatty = getattr(output, 'isatty', False) and output.isatty()
    self._lock = threading.RLock()
    self._local = threading.local()
    self._clock = clock
    self._prefix = prefix"
441,"    def _init_mutators(self, opt: Opt):
        """"""
        Initialize mutator objects for sub agents.
        """"""","        self.krm_mutators = None
        self.drm_mutators = None
        if opt['krm_message_mutators']:
            logging.warning(
                'WARNING: If specifying KRM Mutators, they MUST be message mutators'
            )
            mutator_types = Mutator.load_mutator_types(opt.get('krm_message_mutators'))
            self.krm_mutators = [mutator(opt) for mutator in mutator_types]
        if opt['drm_message_mutators']:
            logging.warning(
                'WARNING: If specifying DRM Mutators, they MUST be message mutators'
            )
            mutator_types = Mutator.load_mutator_types(opt.get('drm_message_mutators'))
            self.drm_mutators = [mutator(opt) for mutator in mutator_types]"
442,"    def selected_summary(self):
        """"""
        :return: XCUIElement is selected summary
        :rtype: str | None
        """"""","        if self.selected_value:
            return ""selected: {}"".format(self.selected_value)
        return None"
443,"def GpuNameScope(gpu_id):
    """"""Create a name scope for GPU device `gpu_id`.""""""","    with core.NameScope('gpu_{:d}'.format(gpu_id)):
        yield"
444,"    def abortpending(self):
        """"""Used in alternative manifestlog implementations to throw out pending
        additions.""""""",
445,"    def check_args(name, arg_str):
        """"""Utility to check if adequate number of arguments are passed to an
        explore command.

        Arguments:
            name: The name of the explore command.
            arg_str: The argument string passed to the explore command.

        Returns:
            True if adequate arguments are passed, false otherwise.

        Raises:
            gdb.GdbError if adequate arguments are not passed.
        """"""","        if len(arg_str) < 1:
            raise gdb.GdbError(""ERROR: '%s' requires an argument.""
                               % name)
            return False
        else:
            return True"
446,"def CheckAccess(filename, clean_lines, linenum, nesting_state, error):
  """"""Checks for improper use of DISALLOW* macros.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    nesting_state: A _NestingState instance which maintains information about
                   the current stack of nested blocks being parsed.
    error: The function to call with any errors found.
  """"""","  line = clean_lines.elided[linenum]  # get rid of comments and strings

  matched = Match((r'\s*(DISALLOW_COPY_AND_ASSIGN|'
                   r'DISALLOW_EVIL_CONSTRUCTORS|'
                   r'DISALLOW_IMPLICIT_CONSTRUCTORS)'), line)
  if not matched:
    return
  if nesting_state.stack and isinstance(nesting_state.stack[-1], _ClassInfo):
    if nesting_state.stack[-1].access != 'private':
      error(filename, linenum, 'readability/constructors', 3,
            '%s must be in the private: section' % matched.group(1))

  else:
    # Found DISALLOW* macro outside a class declaration, or perhaps it
    # was used inside a function when it should have been part of the
    # class declaration.  We could issue a warning here, but it
    # probably resulted in a compiler error already.
    pass"
447,"    def score_candidates(self, batch, cand_vecs, cand_encs=None):
        """"""
        Score candidates.

        Override to extract weights appropriately, if needed.
        """"""","        original_dim = cand_vecs.dim()
        if original_dim == 2:
            cand_vecs = cand_vecs.unsqueeze(1)
        ctxt_rep, ret_weights, ctxt_rep_mask, cand_rep = self.model(
            **self._model_context_input(batch), cand_tokens=cand_vecs
        )
        if original_dim == 2:
            num_cands = cand_rep.size(0)  # will be bsz if using batch cands
            cand_rep = (
                cand_rep.expand(num_cands, batch.text_vec.size(0), -1)
                .transpose(0, 1)
                .contiguous()
            )
        ctxt_code_weights, ctxt_rep_mask = self.model(
            ctxt_rep=ctxt_rep, ctxt_rep_mask=ctxt_rep_mask, cand_rep=cand_rep
        )
        character_weights = torch.bmm(ctxt_code_weights, ret_weights)
        return character_weights, ctxt_rep_mask"
448,"    def __iter__(self):
        """"""Iterate over the storage

        Yields journalentry instances for each contained journal record.

        """"""","        local = self._open(self.localvfs)

        if self.sharedvfs is None:
            return local

        # iterate over both local and shared entries, but only those
        # shared entries that are among the currently shared features
        shared = (
            e
            for e in self._open(self.sharedvfs)
            if sharednamespaces.get(e.namespace) in self.sharedfeatures
        )
        return _mergeentriesiter(local, shared)"
449,"    def get_word_id(self, word):
        """"""
        Given a word, get the word id within the dictionary.
        Returns -1 if word is not in the dictionary.
        """"""",        return self.f.getWordId(word)
450,"    def __bytes__(self):
        """"""Return the bytes representation of the path.  This is only
        recommended to use under Unix.""""""","        if sys.version_info < (3, 2):
            raise NotImplementedError(""needs Python 3.2 or later"")
        return os.fsencode(str(self))"
451,"def register_rmtree(directory):
  """"""Register an existing directory to be cleaned up at process exit.""""""",  return _MKDTEMP_SINGLETON.register(directory)
452,"    def add_mapping(self, fbsource_dir, target_dir) -> None:
        """"""Add a posix path or pattern.  We cannot normpath the input
        here because that would change the paths from posix to windows
        form and break the logic throughout this class.""""""","        self.roots.append(fbsource_dir)
        self.mapping.append((fbsource_dir, target_dir))"
453,"def _expand_bbox_targets(bbox_target_data):
    """"""Bounding-box regression targets are stored in a compact form in the
    roidb.

    This function expands those targets into the 4-of-4*K representation used
    by the network (i.e. only one class has non-zero targets). The loss weights
    are similarly expanded.

    Returns:
        bbox_target_data (ndarray): N x 4K blob of regression targets
        bbox_inside_weights (ndarray): N x 4K blob of loss weights
    """"""","    num_bbox_reg_classes = cfg.MODEL.NUM_CLASSES
    if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:
        num_bbox_reg_classes = 2  # bg and fg

    clss = bbox_target_data[:, 0]
    bbox_targets = blob_utils.zeros((clss.size, 4 * num_bbox_reg_classes))
    bbox_inside_weights = blob_utils.zeros(bbox_targets.shape)
    inds = np.where(clss > 0)[0]
    for ind in inds:
        cls = int(clss[ind])
        start = 4 * cls
        end = start + 4
        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]
        bbox_inside_weights[ind, start:end] = (1.0, 1.0, 1.0, 1.0)
    return bbox_targets, bbox_inside_weights"
454,"    def importThriftStr(self, payload, **kwargs):
        """"""Compiles a thrift file from string `payload`""""""","        with tempfile.NamedTemporaryFile(suffix='.thrift', mode='w') as f:
            if self.debug:
                f.delete = False
            f.write(payload)
            f.flush()
            return self.importThrift(f.name, **kwargs)"
455,"    def read(self) -> List[MountInfo]:
        ""Returns the list of system mounts.""",
456,"def _validaterevset(repo, revset, bookmark) -> None:
    """"""Abort if the revs to be pushed aren't valid for a scratch branch.""""""","    if not bookmark and not repo.revs(revset):
        raise error.Abort(_(""nothing to push""))
    if bookmark:
        # Allow bundle with many heads only if no bookmark is specified
        heads = repo.revs(""heads(%r)"", revset)
        if len(heads) > 1:
            raise error.Abort(_(""cannot push more than one head to a scratch branch""))"
457,"    def run_on_video(self, video):
        """"""
        Visualizes predictions on frames of the input video.

        Args:
            video (cv2.VideoCapture): a :class:`VideoCapture` object, whose source can be
                either a webcam or a video file.

        Yields:
            ndarray: BGR visualizations of each video frame.
        """"""","        video_visualizer = VideoVisualizer(self.metadata, self.instance_mode)

        def process_predictions(frame, predictions):
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if ""panoptic_seg"" in predictions:
                panoptic_seg, segments_info = predictions[""panoptic_seg""]
                vis_frame = video_visualizer.draw_panoptic_seg_predictions(
                    frame, panoptic_seg.to(self.cpu_device), segments_info
                )
            elif ""instances"" in predictions:
                predictions = predictions[""instances""].to(self.cpu_device)
                vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)
            elif ""sem_seg"" in predictions:
                vis_frame = video_visualizer.draw_sem_seg(
                    frame, predictions[""sem_seg""].argmax(dim=0).to(self.cpu_device)
                )

            # Converts Matplotlib RGB format to OpenCV BGR format
            vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)
            return vis_frame

        frame_gen = self._frame_from_video(video)
        if self.parallel:
            buffer_size = self.predictor.default_buffer_size

            frame_data = deque()

            for cnt, frame in enumerate(frame_gen):
                frame_data.append(frame)
                self.predictor.put(frame)

                if cnt >= buffer_size:
                    frame = frame_data.popleft()
                    predictions = self.predictor.get()
                    yield process_predictions(frame, predictions)

            while len(frame_data):
                frame = frame_data.popleft()
                predictions = self.predictor.get()
                yield process_predictions(frame, predictions)
        else:
            for frame in frame_gen:
                yield process_predictions(frame, self.predictor(frame))"
458,"def parse_extras(extras_file):
    """"""Parse a JSON file for ""exceptions"" and ""additions"".

    Returns a dict containing a list of both.
    """"""","    parsed = ''
    extras = {'exceptions': [], 'additions': []}
    try:
        with open(extras_file, 'rb') as thefile:
            print(""Parsing extras file..."")
            parsed = json.load(thefile)
    except IOError as err:
        print(""Error parsing extras file: %s"" % err)
    # Check for exceptions
    extras['exceptions'] = parsed.get(""exceptions_list"", [])
    if extras['exceptions']:
        print(""Found exceptions."")
    # Check for additions
    extras['additions'] = parsed.get(""additions_list"", [])
    if extras['additions']:
        print(""Found additions."")
    return extras"
459,"    def _get_context_batch_size(self, **ctxt_inputs: torch.Tensor) -> int:
        """"""
        Return the batch size of the context.
        """"""","        if ctxt_inputs['ctxt_tokens'] is not None:
            return ctxt_inputs['ctxt_tokens'].size(0)
        else:
            return ctxt_inputs['ctxt_image'].size(0)"
460,"    def _makeQueue(self):
        """"""Override this if you need a custom Queue implementation""""""",        return queue.Queue(maxsize=self.max_items)
461,"    def convert(box: _RawBoxType, from_mode: ""BoxMode"", to_mode: ""BoxMode"") -> _RawBoxType:
        """"""
        Args:
            box: can be a k-tuple, k-list or an Nxk array/tensor, where k = 4 or 5
            from_mode, to_mode (BoxMode)

        Returns:
            The converted box of the same type.
        """"""","        if from_mode == to_mode:
            return box

        original_type = type(box)
        is_numpy = isinstance(box, np.ndarray)
        single_box = isinstance(box, (list, tuple))
        if single_box:
            assert len(box) == 4 or len(box) == 5, (
                ""BoxMode.convert takes either a k-tuple/list or an Nxk array/tensor,""
                "" where k == 4 or 5""
            )
            arr = torch.tensor(box)[None, :]
        else:
            # avoid modifying the input box
            if is_numpy:
                arr = torch.from_numpy(np.asarray(box)).clone()
            else:
                arr = box.clone()

        assert to_mode not in [BoxMode.XYXY_REL, BoxMode.XYWH_REL] and from_mode not in [
            BoxMode.XYXY_REL,
            BoxMode.XYWH_REL,
        ], ""Relative mode not yet supported!""

        if from_mode == BoxMode.XYWHA_ABS and to_mode == BoxMode.XYXY_ABS:
            assert (
                arr.shape[-1] == 5
            ), ""The last dimension of input shape must be 5 for XYWHA format""
            original_dtype = arr.dtype
            arr = arr.double()

            w = arr[:, 2]
            h = arr[:, 3]
            a = arr[:, 4]
            c = torch.abs(torch.cos(a * math.pi / 180.0))
            s = torch.abs(torch.sin(a * math.pi / 180.0))
            # This basically computes the horizontal bounding rectangle of the rotated box
            new_w = c * w + s * h
            new_h = c * h + s * w

            # convert center to top-left corner
            arr[:, 0] -= new_w / 2.0
            arr[:, 1] -= new_h / 2.0
            # bottom-right corner
            arr[:, 2] = arr[:, 0] + new_w
            arr[:, 3] = arr[:, 1] + new_h

            arr = arr[:, :4].to(dtype=original_dtype)
        elif from_mode == BoxMode.XYWH_ABS and to_mode == BoxMode.XYWHA_ABS:
            original_dtype = arr.dtype
            arr = arr.double()
            arr[:, 0] += arr[:, 2] / 2.0
            arr[:, 1] += arr[:, 3] / 2.0
            angles = torch.zeros((arr.shape[0], 1), dtype=arr.dtype)
            arr = torch.cat((arr, angles), axis=1).to(dtype=original_dtype)
        else:
            if to_mode == BoxMode.XYXY_ABS and from_mode == BoxMode.XYWH_ABS:
                arr[:, 2] += arr[:, 0]
                arr[:, 3] += arr[:, 1]
            elif from_mode == BoxMode.XYXY_ABS and to_mode == BoxMode.XYWH_ABS:
                arr[:, 2] -= arr[:, 0]
                arr[:, 3] -= arr[:, 1]
            else:
                raise NotImplementedError(
                    ""Conversion from BoxMode {} to {} is not supported yet"".format(
                        from_mode, to_mode
                    )
                )

        if single_box:
            return original_type(arr.flatten().tolist())
        if is_numpy:
            return arr.numpy()
        else:
            return arr"
462,"    def incrementalmissingrevs(self, common=None):
        """"""Return an object that can be used to incrementally compute the
        revision numbers of the ancestors of arbitrary sets that are not
        ancestors of common. This is an ancestor.incrementalmissingancestors
        object.

        'common' is a list of revision numbers. If common is not supplied, uses
        nullrev.
        """"""","        if common is None:
            common = [nullrev]

        return ancestor.incrementalmissingancestors(self.parentrevs, common)"
463,"def _globpatsplit(pat) -> List[str]:
    """"""Split a glob pattern. Return a list.

    A naive version is ""path.split(""/"")"". This function handles more cases, like
    ""{*,{a,b}*/*}"".

    >>> _globpatsplit(""*/**/x/{a,b/c}"")
    ['*', '**', 'x', '{a,b/c}']
    """"""","    result = []
    buf = """"
    parentheses = 0
    for ch in pat:
        if ch == ""{"":
            parentheses += 1
        elif ch == ""}"":
            parentheses -= 1
        if parentheses == 0 and ch == ""/"":
            if buf:
                result.append(buf)
                buf = """"
        else:
            buf += ch
    if buf:
        result.append(buf)
    return result"
464,"def dirstatecopy(ui, repo, wctx, src, dst, dryrun=False, cwd=None):
    """"""Update the dirstate to reflect the intent of copying src to dst. For
    different reasons it might not end with dst being marked as copied from src.
    """"""","    origsrc = repo.dirstate.copied(src) or src
    if dst == origsrc:  # copying back a copy?
        if repo.dirstate[dst] not in ""mn"" and not dryrun:
            repo.dirstate.normallookup(dst)
    else:
        if repo.dirstate[origsrc] == ""a"" and origsrc == src:
            if not ui.quiet:
                ui.warn(
                    _(
                        ""%s has not been committed yet, so no copy ""
                        ""data will be stored for %s.\n""
                    )
                    % (repo.pathto(origsrc, cwd), repo.pathto(dst, cwd))
                )
            if repo.dirstate[dst] in ""?r"" and not dryrun:
                wctx.add([dst])
        elif not dryrun:
            wctx.copy(origsrc, dst)"
465,"    def mock_roi_heads(self, tensor_mode=True):
        """"""
        Patching several inference functions inside ROIHeads and its subclasses

        Args:
            tensor_mode (bool): whether the inputs/outputs are caffe2's tensor
                format or not. Default to True.
        """"""","        # NOTE: this requries the `keypoint_rcnn_inference` and `mask_rcnn_inference`
        # are called inside the same file as BaseXxxHead due to using mock.patch.
        kpt_heads_mod = keypoint_head.BaseKeypointRCNNHead.__module__
        mask_head_mod = mask_head.BaseMaskRCNNHead.__module__

        mock_ctx_managers = [
            mock_fastrcnn_outputs_inference(
                tensor_mode=tensor_mode,
                check=True,
                box_predictor_type=type(self.heads.box_predictor),
            )
        ]
        if getattr(self.heads, ""keypoint_on"", False):
            mock_ctx_managers += [
                mock_keypoint_rcnn_inference(
                    tensor_mode, kpt_heads_mod, self.use_heatmap_max_keypoint
                )
            ]
        if getattr(self.heads, ""mask_on"", False):
            mock_ctx_managers += [mock_mask_rcnn_inference(tensor_mode, mask_head_mod)]

        with contextlib.ExitStack() as stack:  # python 3.3+
            for mgr in mock_ctx_managers:
                stack.enter_context(mgr)
            yield"
466,"    def update_scroll_offset(self) -> None:
        """"""
        yay scrolling logic! we will start simple here
        and basically just center the viewport to current
        matched line
        """"""","        window_height = self.get_viewport_height()
        half_height = int(round(window_height / 2.0))

        # important, we need to get the real SCREEN position
        # of the hover index, not its index within our matches
        hovered = self.line_matches[self.hover_index]
        desired_top_row = hovered.get_screen_index() - half_height

        old_offset = self.scroll_offset
        desired_top_row = max(desired_top_row, 0)
        new_offset = -desired_top_row
        # lets add in some leeway -- don't bother repositioning
        # if the old offset is within 1/2 of the window height
        # of our desired (unless we absolutely have to)
        if (
            abs(new_offset - old_offset) > half_height / 2
            or self.hover_index + old_offset < 0
        ):
            # need to reassign now we have gone too far
            self.scroll_offset = new_offset
        if old_offset is not self.scroll_offset:
            self.dirty_all()

        # also update our scroll bar
        self.scroll_bar.calc_box_fractions()"
467,"    def _add_gt_annotations(self, entry):
        """"""Add ground truth annotation metadata to an roidb entry.""""""","        ann_ids = self.COCO.getAnnIds(imgIds=entry['id'], iscrowd=None)
        objs = self.COCO.loadAnns(ann_ids)
        # Sanitize bboxes -- some are invalid
        valid_objs = []
        valid_segms = []
        width = entry['width']
        height = entry['height']
        for obj in objs:
            # crowd regions are RLE encoded
            if segm_utils.is_poly(obj['segmentation']):
                # Valid polygons have >= 3 points, so require >= 6 coordinates
                obj['segmentation'] = [
                    p for p in obj['segmentation'] if len(p) >= 6
                ]
            if obj['area'] < cfg.TRAIN.GT_MIN_AREA:
                continue
            if 'ignore' in obj and obj['ignore'] == 1:
                continue
            # Convert form (x1, y1, w, h) to (x1, y1, x2, y2)
            x1, y1, x2, y2 = box_utils.xywh_to_xyxy(obj['bbox'])
            x1, y1, x2, y2 = box_utils.clip_xyxy_to_image(
                x1, y1, x2, y2, height, width
            )
            # Require non-zero seg area and more than 1x1 box size
            if obj['area'] > 0 and x2 > x1 and y2 > y1:
                obj['clean_bbox'] = [x1, y1, x2, y2]
                valid_objs.append(obj)
                valid_segms.append(obj['segmentation'])
        num_valid_objs = len(valid_objs)

        boxes = np.zeros((num_valid_objs, 4), dtype=entry['boxes'].dtype)
        gt_classes = np.zeros((num_valid_objs), dtype=entry['gt_classes'].dtype)
        gt_overlaps = np.zeros(
            (num_valid_objs, self.num_classes),
            dtype=entry['gt_overlaps'].dtype
        )
        seg_areas = np.zeros((num_valid_objs), dtype=entry['seg_areas'].dtype)
        is_crowd = np.zeros((num_valid_objs), dtype=entry['is_crowd'].dtype)
        box_to_gt_ind_map = np.zeros(
            (num_valid_objs), dtype=entry['box_to_gt_ind_map'].dtype
        )
        if self.keypoints is not None:
            gt_keypoints = np.zeros(
                (num_valid_objs, 3, self.num_keypoints),
                dtype=entry['gt_keypoints'].dtype
            )

        im_has_visible_keypoints = False
        for ix, obj in enumerate(valid_objs):
            cls = self.json_category_id_to_contiguous_id[obj['category_id']]
            boxes[ix, :] = obj['clean_bbox']
            gt_classes[ix] = cls
            seg_areas[ix] = obj['area']
            is_crowd[ix] = obj['iscrowd']
            box_to_gt_ind_map[ix] = ix
            if self.keypoints is not None:
                gt_keypoints[ix, :, :] = self._get_gt_keypoints(obj)
                if np.sum(gt_keypoints[ix, 2, :]) > 0:
                    im_has_visible_keypoints = True
            if obj['iscrowd']:
                # Set overlap to -1 for all classes for crowd objects
                # so they will be excluded during training
                gt_overlaps[ix, :] = -1.0
            else:
                gt_overlaps[ix, cls] = 1.0
        entry['boxes'] = np.append(entry['boxes'], boxes, axis=0)
        entry['segms'].extend(valid_segms)
        # To match the original implementation:
        # entry['boxes'] = np.append(
        #     entry['boxes'], boxes.astype(np.int).astype(np.float), axis=0)
        entry['gt_classes'] = np.append(entry['gt_classes'], gt_classes)
        entry['seg_areas'] = np.append(entry['seg_areas'], seg_areas)
        entry['gt_overlaps'] = np.append(
            entry['gt_overlaps'].toarray(), gt_overlaps, axis=0
        )
        entry['gt_overlaps'] = scipy.sparse.csr_matrix(entry['gt_overlaps'])
        entry['is_crowd'] = np.append(entry['is_crowd'], is_crowd)
        entry['box_to_gt_ind_map'] = np.append(
            entry['box_to_gt_ind_map'], box_to_gt_ind_map
        )
        if self.keypoints is not None:
            entry['gt_keypoints'] = np.append(
                entry['gt_keypoints'], gt_keypoints, axis=0
            )
            entry['has_visible_keypoints'] = im_has_visible_keypoints"
468,"    def parts(self):
        """"""An object providing sequence-like access to the
        components in the filesystem path.""""""","        # We cache the tuple to avoid building a new one each time .parts
        # is accessed.  XXX is this necessary?
        try:
            return self._pparts
        except AttributeError:
            self._pparts = tuple(self._parts)
            return self._pparts"
469,"def create_release(repository, github_token, message, version_tag, commit):
    """"""
    Creates a release on github and returns that data

    Args:
        repository: The name of the repository to work on
        github_token: The token to use for github operations
        message: The message to put in the body of the release
        version_tag: The tag to have github create. This is also used in the release
                     title
        commit: The commit to pin the release to
    """"""","    url = ""https://api.github.com/repos/{}/releases"".format(repository)
    data = {
        ""tag_name"": version_tag,
        ""target_commitish"": commit,
        ""name"": ""Release {}"".format(version_tag),
        ""body"": message.format(tag=version_tag),
        # Draft would be nice, but then the releases don't show up in the API,
        # and we can't get tarball/zipball urls until publishing :(
        ""draft"": False,
    }
    headers = get_headers(github_token)
    logging.info(""Creating a new release"")
    response = requests.post(url, json=data, headers=headers)
    response.raise_for_status()
    ret = response.json()
    logging.info(""Created new release at {}"".format(ret[""html_url""]))
    return ret"
470,"    def postprocess_output_generations(self, label: str) -> str:
        """"""
        Post-process the model output.

        Returns the model output by default, override to add custom logic
        """"""",        return label
471,"    def get_initial_forced_decoder_input(
        self,
        bsz: int,
        inputs: torch.LongTensor,
        n_docs: int,
        start_idx: int,
        end_idx: int,
        input_turns_cnt: Optional[torch.LongTensor] = None,
    ) -> torch.LongTensor:
        """"""
        Return the initial input to the decoder during training.

        Repeat each input n_docs times.

        :param bsz:
            batchsize
        :param inputs:
            inputs to decode
        :param n_docs:
            number of docs per input
        :param start_idx:
            start token idx
        :param end_idx:
            end token idx
        :param input_turns_cnt:
            an optional tensor containing the number of turns of each corresponding context.

        :return initial_input:
            initial input for the decoder.
        """"""","        inputs = get_forced_decoder_inputs(
            inputs, bsz, start_idx, end_idx, self.generation_model
        )
        inputs = inputs.repeat(1, n_docs).reshape(-1, inputs.size(1))  # type: ignore
        return inputs"
472,"def mod(context, mapping, args):
    """"""Calculate a mod b such that a / b + a mod b == a""""""","    if not len(args) == 2:
        # i18n: ""mod"" is a keyword
        raise error.ParseError(_(""mod expects two arguments""))

    func = lambda a, b: a % b
    return runarithmetic(context, mapping, (func, args[0], args[1]))"
473,"    def build_encoder(
        cls,
        opt: Opt,
        *args,
        dictionary: Optional[DictionaryAgent] = None,
        embedding: Optional[torch.nn.Embedding] = None,
        encoder_class: Optional[Type] = None,
        **kwargs,
    ):
        """"""
        Override to build with IdentityLayer as Encoder.
        """"""","        return FidModel.build_encoder(
            opt, dictionary, encoder_class=IdentityLayer, **kwargs
        )"
474,"    def _set_model_dict(self):
        """"""
        find model dictionary from model_list.py; should be run after
        `self._add_user_model_tasks`
        """"""","        mf, self.model_dict = (self.opt['model_file'], {})
        exp_path = to_zoo(self.opt, mf)
        if self.verbose:
            print('expected path in model list:', exp_path, 'or', mf)
        if all_models.get(exp_path):
            self.model_dict.update(all_models[exp_path])
        elif all_models.get(mf):
            self.model_dict.update(all_models[mf])"
475,"    def _process_conversations_needed(self, args: ""DictConfig"") -> Dict[str, int]:
        """"""
        Formats the pair of models and sets the number of conversations needed.
        """"""","
        conversations_needed_string = args.blueprint.conversations_needed_string
        conversations_needed = {}
        parts = conversations_needed_string.split(',')
        for part in parts:
            model1, model2, num_string = part.split(':')
            models_alphabetize = [model1, model2]
            models_alphabetize.sort()  # alphabetizing for consistency
            conversations_needed[
                f'{models_alphabetize[0]}:{models_alphabetize[1]}'
            ] = int(
                num_string
            )  # format is model_1_name:model_2_name

        return conversations_needed"
476,"def normalize_path_arg(path_arg: str, may_need_tilde_expansion: bool = False) -> str:
    """"""Normalizes a path by using os.path.realpath().

    Note that this function is expected to be used with command-line arguments.
    If the argument comes from a config file or GUI where tilde expansion is not
    done by the shell, then may_need_tilde_expansion=True should be specified.
    """"""","    if path_arg:
        if may_need_tilde_expansion:
            path_arg = os.path.expanduser(path_arg)

        # Use the canonical version of the path.
        path_arg = os.path.realpath(path_arg)
    return path_arg"
477,"    def set_labels(self, labels: torch.Tensor):
        """"""
        Cache the label vec.
        """"""",        self.label_vec = labels
478,"def unescape(s):
    """"""
    Revert escaped characters back to their special version.

    For example, \\n => newline and \\t => tab
    """"""","    return s.replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')"
479,"    def _wrap_file_access(self, wrap=True):
        """"""
        Wrap 'open' so that they it checks if accessed files are known dependencies.
        If 'wrap' is equal to False, restore original function instead.
        """"""","        return self._wrap_fun_for_file_access(builtins, ""open"", wrap)"
480,"def _get_defaults(func):
    """"""Internal helper to extract the default arguments, by name.""""""","    try:
        code = func.__code__
    except AttributeError:
        # Some built-in functions don't have __code__, __defaults__, etc.
        return {}
    pos_count = code.co_argcount
    arg_names = code.co_varnames
    arg_names = arg_names[:pos_count]
    defaults = func.__defaults__ or ()
    kwdefaults = func.__kwdefaults__
    res = dict(kwdefaults) if kwdefaults else {}
    pos_offset = pos_count - len(defaults)
    for name, value in zip(arg_names[pos_offset:], defaults):
        assert name not in res
        res[name] = value
    return res"
481,"    def _predict_character_from_context(
        self, context: str, characters: Dict[str, str], who: str
    ) -> str:
        """"""
        Given context, predict who the character is.

        :param context:
            dialogue context
        :param characters:
            available characters to choose from
        :param who:
            whether to predict self or partner

        :return whoareyou:
            return predicted self character
        """"""","        assert (
            not self.annotate_speaker
        ), ""if annotate speaker, characters would be in dialogue history""
        control_token = WHO_ARE_YOU if who == 'self' else WHO_AM_I
        utterances = self.get_utterances_from_full_context(
            context, include_context=self.include_context
        )

        if self.num_utterances > 0:
            utterances = utterances[: self.num_utterances]

        utterances.insert(-2, control_token)
        limited_context = self.delimiter.join(utterances)
        label_candidates = extract_characters(context)
        act = self.predict(limited_context, predictor_label_candidates=label_candidates)
        return act['text']"
482,"    def time(self, a_datetime):
        """"""Inform the client of the time.

        "":param datetime: A datetime.datetime object.
        """"""","        time = a_datetime.astimezone(iso8601.Utc())
        self._stream.write(_b(""time: %04d-%02d-%02d %02d:%02d:%02d.%06dZ\n"" % (
            time.year, time.month, time.day, time.hour, time.minute,
            time.second, time.microsecond)))"
483,"def showbisect(repo, ctx, templ, **args) -> Optional[str]:
    """"""String. The changeset bisection status.""""""","    return hbisect.label(repo, ctx.node())"
484,"    def translated_entries(self):
        """"""
        Convenience method to keep the same interface with POFile instances.
        """"""",        return self
485,"def add_topdown_lateral_module(
    model, fpn_top, fpn_lateral, fpn_bottom, dim_top, dim_lateral
):
    """"""Add a top-down lateral module.""""""","    # Lateral 1x1 conv
    if cfg.FPN.USE_GN:
        # use GroupNorm
        lat = model.ConvGN(
            fpn_lateral,
            fpn_bottom + '_lateral',
            dim_in=dim_lateral,
            dim_out=dim_top,
            group_gn=get_group_gn(dim_top),
            kernel=1,
            pad=0,
            stride=1,
            weight_init=(
                const_fill(0.0) if cfg.FPN.ZERO_INIT_LATERAL
                else ('XavierFill', {})),
            bias_init=const_fill(0.0)
        )
    else:
        lat = model.Conv(
            fpn_lateral,
            fpn_bottom + '_lateral',
            dim_in=dim_lateral,
            dim_out=dim_top,
            kernel=1,
            pad=0,
            stride=1,
            weight_init=(
                const_fill(0.0)
                if cfg.FPN.ZERO_INIT_LATERAL else ('XavierFill', {})
            ),
            bias_init=const_fill(0.0)
        )
    # Top-down 2x upsampling
    td = model.net.UpsampleNearest(fpn_top, fpn_bottom + '_topdown', scale=2)
    # Sum lateral and top-down
    model.net.Sum([lat, td], fpn_bottom)"
486,"def get_user_home_dir() -> str:
    """"""
    Grabs the home directory of the user's system
    We store our 'pantri_config.json' file here
    """"""","    standard_home = os.path.expanduser(""~"")

    if os.path.exists(standard_home):
        return standard_home

    logging.getLogger(""pantri"").error(
        ""Error: home path not found. Try running Pantri again.""
    )
    exit()"
487,"  def satisfies(self, requirement):
    """"""Determine whether this package matches the requirement.

    :param requirement: The requirement to compare this Package against
    :type requirement: string or :class:`pkg_resources.Requirement`
    :returns: True if the package matches the requirement, otherwise False
    """"""","    requirement = maybe_requirement(requirement)
    link_name = safe_name(self.name).lower()
    if link_name != requirement.key:
      return False
    return self.raw_version in requirement"
488,"    def _resolvedata(self, potentialentry):
        """"""Check that the node for potentialentry exists and return it""""""","        if not potentialentry in self.potentialentries:
            return None
        hexnode, nametype, remote, rname = self.potentialentries[potentialentry]
        repo = self._repo
        binnode = bin(hexnode)
        try:
            repo.changelog.rev(binnode)
        except LookupError as e:
            if rname not in selectivepullinitbookmarknames(repo):
                # Not a critical bookmark.
                return None
            raise error.RepoLookupError(
                _(""remotename entry %s (%s) cannot be found: %s"")
                % (potentialentry, hexnode, e),
                hint=_(""try '@prog@ doctor' to attempt to fix it""),
            )
        # Skip closed branches
        if nametype == ""branches"" and repo[binnode].closesbranch():
            return None
        return [binnode]"
489,"    def forward_layers(
        self,
        tensor: torch.Tensor,
        encoder_output: torch.Tensor,
        encoder_mask: torch.Tensor,
        incr_state: Dict[int, Dict[str, Dict[str, torch.Tensor]]],
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """"""
        Override of forward_layers of TransformerDecoder.
        """"""","        new_incr_state = {}
        for _s in range(0, self.opt['ladder_size']):
            tensor, new_incr_state = super().forward_layers(
                tensor=tensor,
                encoder_output=encoder_output,
                encoder_mask=encoder_mask,
                incr_state=incr_state,
                **kwargs,
            )
        return tensor, new_incr_state"
490,"def log_command(args: Sequence[str]) -> None:
    """"""
    Given a command, print it in a both machine and human readable way.

    Args:
        *args: the list of command line arguments you want to run
        env: the dictionary of environment variable settings for the command
    """"""","    cmd = ' '.join(shlex.quote(arg) for arg in args)
    logging.info(""$ "" + cmd)"
491,"    def globfind(self, pattern):
        """"""Returns multiple files matching a glob.""""""","        # pylint: disable=no-self-use

        glob_matches = glob(pattern)

        if len(glob_matches) < 1:
            raise ProcessorError(""No matching filename found"")

        glob_matches.sort()

        # return glob_matches
        new_glob = []
        for glob_item in glob_matches:
            new_string = os.path.basename(glob_item)
            if self.env[""remove_extension""]:
                new_string = os.path.splitext(new_string)[0]
            new_glob.append(new_string)
        return new_glob"
492,"    def simulate_action(self,
                        task_index: int,
                        action: ActionLike,
                        *,
                        need_images: bool = True,
                        need_featurized_objects: bool = False,
                        stride: int = phyre.simulator.DEFAULT_STRIDE,
                        stable: bool = False) -> phyre.simulation.Simulation:
        """"""Runs simluation for the action.

        Args:
            task_index: index of the task.
            action: numpy array or list of self.action_space_dim floats in
                [0, 1].
            need_images: whether simulation images are needed.
            need_featurized_objects: whether simulation featurized_objects are needed.
            stride: int, defines the striding for the simulation images
                array. Higher striding will result in less images and less
                compute. Note, that this parameter doesn't affect simulation
                FPS. Ignored if need_images is False.
            stable: if True, will simulate a few actions in the neigborhood
                of the actions and return STABLY_SOLVED status iff all
                neigbour actions are either SOLVED or INVALID. Otherwise
                UNSTABLY_SOLVED is returned. SOLVED is never returned if
                stable is set.

        Returns:
             * phyre.simulation.Simulation object containing the result of
                the simulation. 
             * SimulationStatus, images, and featurized_objects are easily
                 accesible with simulation.simulation_status, simulation.images,
                 and simulation.featurized_objects.
        """"""","        user_input, is_valid = self._get_user_input(action)
        if not is_valid:
            return phyre.simulation.Simulation(
                status=SimulationStatus.INVALID_INPUT)

        main_status, images, objects = self._simulate_user_input(
            task_index, user_input, need_images, need_featurized_objects,
            stride)
        if not stable or not main_status.is_solved():
            return phyre.simulation.Simulation(status=main_status,
                                               images=images,
                                               featurized_objects=objects)

        for modified_user_input in _yield_user_input_neighborhood(user_input):
            status, _, _ = self._simulate_user_input(
                task_index,
                modified_user_input,
                need_images=False,
                need_featurized_objects=False,
                stride=stride)
            if status.is_not_solved():
                return phyre.simulation.Simulation(
                    status=SimulationStatus.UNSTABLY_SOLVED,
                    images=images,
                    featurized_objects=objects)
        return phyre.simulation.Simulation(
            status=SimulationStatus.STABLY_SOLVED,
            images=images,
            featurized_objects=objects)"
493,"    def registertmp(self, tmpfile, location=""""):
        """"""register a temporary transaction file

        Such files will be deleted when the transaction exits (on both
        failure and success).
        """"""","        self._addbackupentry((location, """", tmpfile, False))"
494,"    def hierarchy_text(self, pointer=False, trait=False, frame=False, indent=0):
        """"""
        String representation of the hierarchy of elements

        :param bool pointer: Print pointers
        :param bool trait: Print traits
        :param bool frame: Print frames
        :param int indent: Indention
        :return: String representation of the hierarchy of elements
        :rtype: str
        """"""","        s = self.text(pointer=pointer, trait=trait, frame=frame, indent=indent)
        for e in self.children:
            s += e.hierarchy_text(
                pointer=pointer, trait=trait, frame=frame, indent=indent + 1
            )
        return s"
495,"def _findexternaltoolwithreporoot(ui, repo, tool):
    """"""Like findexternaltool, but try checking inside the repo for the tool
    before looking in the path.""""""","    exe = _findexternaltoolinternal(ui, tool)
    if not os.path.isabs(exe):
        path = repo.wvfs.join(exe)
        if os.path.isfile(path) and util.isexec(path):
            return path
    return util.findexe(util.expandpath(exe))"
496,"    def delete_persona(self, persona_id):
        """"""
        Deletes the persona.
        """"""","        api_address = 'https://graph.facebook.com/' + persona_id
        response = requests.delete(api_address, params=self.auth_args)
        result = response.json()
        log_utils.print_and_log(
            logging.INFO, '""Facebook response from delete persona: {}""'.format(result)
        )
        return result"
497,"    def progress(self, offset, whence):
        """"""Provide indication about the progress/length of the test run.

        :param offset: Information about the number of tests remaining. If
            whence is PROGRESS_CUR, then offset increases/decreases the
            remaining test count. If whence is PROGRESS_SET, then offset
            specifies exactly the remaining test count.
        :param whence: One of PROGRESS_CUR, PROGRESS_SET, PROGRESS_PUSH,
            PROGRESS_POP.
        """"""","        if whence == PROGRESS_CUR and offset > -1:
            prefix = self._progress_plus
            offset = _b(str(offset))
        elif whence == PROGRESS_PUSH:
            prefix = self._empty_bytes
            offset = self._progress_push
        elif whence == PROGRESS_POP:
            prefix = self._empty_bytes
            offset = self._progress_pop
        else:
            prefix = self._empty_bytes
            offset = _b(str(offset))
        self._stream.write(self._progress_fmt + prefix + offset +
            self._bytes_eol)"
498,"    def label_value(self):
        """"""
        :return: XCUIElement label value
        :rtype: str
        """"""",        return normalize_summary(self.label.GetSummary())
499,"def showmyparenttasks(repo, ctx, templ, **args):
    """"""Show the tasks from the commit's parent, if it has the
    same author as this commit.
    """"""","    return extract_from_parent(ctx, r""\s*(?:Tasks|Task ID): (.*)"")"
500,"def split(p):
    """"""Same as posixpath.split, but faster

    >>> import posixpath
    >>> for f in ['/absolute/path/to/file',
    ...           'relative/path/to/file',
    ...           'file_alone',
    ...           'path/to/directory/',
    ...           '/multiple/path//separators',
    ...           '/file_at_root',
    ...           '///multiple_leading_separators_at_root',
    ...           '']:
    ...     assert split(f) == posixpath.split(f), f
    """"""","    ht = p.rsplit(""/"", 1)
    if len(ht) == 1:
        return """", p
    nh = ht[0].rstrip(""/"")
    if nh:
        return nh, ht[1]
    return ht[0] + ""/"", ht[1]"
501,"    def _exists(self, path: str, **kwargs: Any) -> bool:
        """"""
        Checks if there is a resource at the given URI.
        Args:
            path (str): A URI supported by this PathHandler
        Returns:
            bool: true if the path exists
        """"""","        self._check_kwargs(kwargs)

        return self._head_object(path) is not None"
502,"    def test_save_world_logs(self):
        """"""
        Test that we can save world logs from train model.
        """"""","        with testing_utils.tempdir() as tmpdir:
            log_report = os.path.join(tmpdir, 'world_logs.jsonl')
            valid, test = testing_utils.train_model(
                {
                    'task': 'integration_tests',
                    'validation_max_exs': 10,
                    'model': 'repeat_label',
                    'short_final_eval': True,
                    'num_epochs': 1.0,
                    'world_logs': log_report,
                }
            )
            with PathManager.open(log_report) as f:
                json_lines = f.readlines()
            assert len(json_lines) == 10"
503,"def _overload_dummy(*args, **kwds):
    """"""Helper for @overload to raise when called.""""""","    raise NotImplementedError(
        ""You should not call an overloaded function. ""
        ""A series of @overload-decorated functions ""
        ""outside a stub module should always be followed ""
        ""by an implementation that is not @overload-ed."")"
504,"    def num_episodes(self) -> int:
        """"""
        Get the number of episodes in this dataset.
        """"""","        raise RuntimeError('""num_episodes"" must be overridden by children.')"
505,"def _ShouldPrintError(category, confidence, linenum):
  """"""If confidence >= verbose, category passes filter and is not suppressed.""""""","
  # There are three ways we might decide not to print an error message:
  # a ""NOLINT(category)"" comment appears in the source,
  # the verbosity level isn't high enough, or the filters filter it out.
  if IsErrorSuppressedByNolint(category, linenum):
    return False
  if confidence < _cpplint_state.verbose_level:
    return False

  is_filtered = False
  for one_filter in _Filters():
    if one_filter.startswith('-'):
      if category.startswith(one_filter[1:]):
        is_filtered = True
    elif one_filter.startswith('+'):
      if category.startswith(one_filter[1:]):
        is_filtered = False
    else:
      assert False  # should have been checked for in SetFilter.
  if is_filtered:
    return False

  return True"
506,"    def reset(self):
        """"""Reset the event, if there is any state necessary.""""""",        self.achieved = False
507,"    def strip(self, minlink, transaction):
        """"""truncate the revlog on the first revision with a linkrev >= minlink

        This function is called when we're stripping revision minlink and
        its descendants from the repository.

        We have to remove all revisions with linkrev >= minlink, because
        the equivalent changelog revisions will be renumbered after the
        strip.

        So we truncate the revlog on the first of these revisions, and
        trust that the caller has saved the revisions that shouldn't be
        removed and that it'll re-add them after this truncation.
        """"""","        if len(self) == 0:
            return

        if self._bypasstransaction:
            return

        rev, _ = self.getstrippoint(minlink)
        if rev == len(self):
            return

        # first truncate the files on disk
        end = self.start(rev)
        if not self._inline:
            transaction.add(self.datafile, end)
            end = rev * self._io.size
        else:
            end += rev * self._io.size

        transaction.add(self.indexfile, end)

        # then reset internal state in memory to forget those revisions
        self._cache = None
        self._chaininfocache = {}
        self._chunkclear()
        for x in range(rev, len(self)):
            del self.nodemap[self.node(x)]

        del self.index[rev:-1]

        # index2 does not support `del`. Reload it directly.
        index2 = getattr(self, ""index2"", None)
        if index2 is not None:
            nodemapfile = self.indexfile[:-2] + "".nodemap""
            self.index2 = bindings.revlogindex.revlogindex(
                self.opener.join(self.indexfile), self.opener.join(nodemapfile)
            )"
508,"def recordfilter(ui, originalhunks, operation=None):
    """"""Prompts the user to filter the originalhunks and return a list of
    selected hunks.
    *operation* is used for to build ui messages to indicate the user what
    kind of filtering they are doing: reverting, committing, shelving, etc.
    (see patch.filterpatch).
    """"""","    usecurses = crecordmod.checkcurses(ui)
    testfile = ui.config(""experimental"", ""crecordtest"")
    oldwrite = setupwrapcolorwrite(ui)
    try:
        newchunks, newopts = filterchunks(
            ui, originalhunks, usecurses, testfile, operation
        )
    finally:
        ui.writebytes = oldwrite
    return newchunks, newopts"
509,"    def get_memory(self) -> List[List[str]]:
        """"""
        Get retriever's memories.

        :return memories:
            return a batchsize-length list of memories for each batch item.
        """"""","        assert self.retriever is not None
        return self.retriever.get_memory()"
510,"def CheckLanguage(filename, clean_lines, linenum, file_extension, include_state,
                  error):
  """"""Checks rules from the 'C++ language rules' section of cppguide.html.

  Some of these rules are hard to test (function overloading, using
  uint32 inappropriately), but we do the best we can.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    file_extension: The extension (without the dot) of the filename.
    include_state: An _IncludeState instance in which the headers are inserted.
    error: The function to call with any errors found.
  """"""","  # If the line is empty or consists of entirely a comment, no need to
  # check it.
  line = clean_lines.elided[linenum]
  if not line:
    return

  match = _RE_PATTERN_INCLUDE.search(line)
  if match:
    CheckIncludeLine(filename, clean_lines, linenum, include_state, error)
    return

  # Create an extended_line, which is the concatenation of the current and
  # next lines, for more effective checking of code that may span more than one
  # line.
  if linenum + 1 < clean_lines.NumLines():
    extended_line = line + clean_lines.elided[linenum + 1]
  else:
    extended_line = line

  # Make Windows paths like Unix.
  fullname = os.path.abspath(filename).replace('\\', '/')

  # TODO(unknown): figure out if they're using default arguments in fn proto.

  # Check for non-const references in functions.  This is tricky because &
  # is also used to take the address of something.  We allow <> for templates,
  # (ignoring whatever is between the braces) and : for classes.
  # These are complicated re's.  They try to capture the following:
  # paren (for fn-prototype start), typename, &, varname.  For the const
  # version, we're willing for const to be before typename or after
  # Don't check the implementation on same line.
  fnline = line.split('{', 1)[0]
  if (len(re.findall(r'\([^()]*\b(?:[\w:]|<[^()]*>)+(\s?&|&\s?)\w+', fnline)) >
      len(re.findall(r'\([^()]*\bconst\s+(?:typename\s+)?(?:struct\s+)?'
                     r'(?:[\w:]|<[^()]*>)+(\s?&|&\s?)\w+', fnline)) +
      len(re.findall(r'\([^()]*\b(?:[\w:]|<[^()]*>)+\s+const(\s?&|&\s?)[\w]+',
                     fnline))):

    # We allow non-const references in a few standard places, like functions
    # called ""swap()"" or iostream operators like ""<<"" or "">>"". We also filter
    # out for loops, which lint otherwise mistakenly thinks are functions.
    if not Search(
        r'(for|swap|Swap|operator[<>][<>])\s*\(\s*'
        r'(?:(?:typename\s*)?[\w:]|<.*>)+\s*&',
        fnline) and not filename.endswith('.c'):
      error(filename, linenum, 'runtime/references', 2,
            'Is this a non-const reference? '
            'If so, make const or use a pointer.')

  # Check to see if they're using an conversion function cast.
  # I just try to capture the most common basic types, though there are more.
  # Parameterless conversion functions, such as bool(), are allowed as they are
  # probably a member operator declaration or default constructor.
  match = Search(
      r'(\bnew\s+)?\b'  # Grab 'new' operator, if it's there
      r'(int|float|double|bool|char|int32|uint32|int64|uint64)\([^)]', line)
  if match:
    # gMock methods are defined using some variant of MOCK_METHODx(name, type)
    # where type may be float(), int(string), etc.  Without context they are
    # virtually indistinguishable from int(x) casts. Likewise, gMock's
    # MockCallback takes a template parameter of the form return_type(arg_type),
    # which looks much like the cast we're trying to detect.
    if (match.group(1) is None and  # If new operator, then this isn't a cast
        not (Match(r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(', line) or
             Match(r'^\s*MockCallback<.*>', line))):
      # Try a bit harder to catch gmock lines: the only place where
      # something looks like an old-style cast is where we declare the
      # return type of the mocked method, and the only time when we
      # are missing context is if MOCK_METHOD was split across
      # multiple lines (for example http://go/hrfhr ), so we only need
      # to check the previous line for MOCK_METHOD.
      if (linenum == 0 or
          not Match(r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(\S+,\s*$',
                    clean_lines.elided[linenum - 1])):
        error(filename, linenum, 'readability/casting', 4,
              'Using deprecated casting style.  '
              'Use static_cast<%s>(...) instead' %
              match.group(2))

  CheckCStyleCast(filename, linenum, line, clean_lines.raw_lines[linenum],
                  'static_cast',
                  r'\((int|float|double|bool|char|u?int(16|32|64))\)', error)

  # This doesn't catch all cases. Consider (const char * const)""hello"".
  #
  # (char *) ""foo"" should always be a const_cast (reinterpret_cast won't
  # compile).
  if CheckCStyleCast(filename, linenum, line, clean_lines.raw_lines[linenum],
                     'const_cast', r'\((char\s?\*+\s?)\)\s*""', error):
    pass
  else:
    # Check pointer casts for other than string constants
    CheckCStyleCast(filename, linenum, line, clean_lines.raw_lines[linenum],
                    'reinterpret_cast', r'\((\w+\s?\*+\s?)\)', error)

  # In addition, we look for people taking the address of a cast.  This
  # is dangerous -- casts can assign to temporaries, so the pointer doesn't
  # point where you think.
  if Search(
      r'(&\([^)]+\)[\w(])|(&(static|dynamic|reinterpret)_cast\b)', line):
    error(filename, linenum, 'runtime/casting', 4,
          ('Are you taking an address of a cast?  '
           'This is dangerous: could be a temp var.  '
           'Take the address before doing the cast, rather than after'))

  # Check for people declaring static/global STL strings at the top level.
  # This is dangerous because the C++ language does not guarantee that
  # globals with constructors are initialized before the first access.
  match = Match(
      r'((?:|static +)(?:|const +))string +([a-zA-Z0-9_:]+)\b(.*)',
      line)
  # Make sure it's not a function.
  # Function template specialization looks like: ""string foo<Type>(..."".
  # Class template definitions look like: ""string Foo<Type>::Method(..."".
  if match and not Match(r'\s*(<.*>)?(::[a-zA-Z0-9_]+)?\s*\(([^""]|$)',
                         match.group(3)):
    error(filename, linenum, 'runtime/string', 4,
          'For a static/global string constant, use a C style string instead: '
          '""%schar %s[]"".' %
          (match.group(1), match.group(2)))

  # Check that we're not using RTTI outside of testing code.
  if Search(r'\bdynamic_cast<', line) and not _IsTestFilename(filename):
    error(filename, linenum, 'runtime/rtti', 5,
          'Do not use dynamic_cast<>.  If you need to cast within a class '
          ""hierarchy, use static_cast<> to upcast.  Google doesn't support ""
          'RTTI.')

  if Search(r'\b([A-Za-z0-9_]*_)\(\1\)', line):
    error(filename, linenum, 'runtime/init', 4,
          'You seem to be initializing a member variable with itself.')

  if file_extension == 'h':
    # TODO(unknown): check that 1-arg constructors are explicit.
    #                How to tell it's a constructor?
    #                (handled in CheckForNonStandardConstructs for now)
    # TODO(unknown): check that classes have DISALLOW_EVIL_CONSTRUCTORS
    #                (level 1 error)
    pass

  # Check if people are using the verboten C basic types.  The only exception
  # we regularly allow is ""unsigned short port"" for port.
  if Search(r'\bshort port\b', line):
    if not Search(r'\bunsigned short port\b', line):
      error(filename, linenum, 'runtime/int', 4,
            'Use ""unsigned short"" for ports, not ""short""')
  else:
    match = Search(r'\b(short|long(?! +double)|long long)\b', line)
    if match:
      error(filename, linenum, 'runtime/int', 4,
            'Use int16/int64/etc, rather than the C type %s' % match.group(1))

  if Search(r'\bsnprintf\b', line):
    error(filename, linenum, 'runtime/printf', 5,
          'Do not use snprintf.  Use ph_snprintf instead.')

  # When snprintf is used, the second argument shouldn't be a literal.
  match = Search(r'ph_snprintf\s*\(([^,]*),\s*([0-9]*)\s*,', line)
  if match and match.group(2) != '0':
    # If 2nd arg is zero, snprintf is used to calculate size.
    error(filename, linenum, 'runtime/printf', 3,
          'If you can, use sizeof(%s) instead of %s as the 2nd arg '
          'to ph_snprintf.' % (match.group(1), match.group(2)))

  # Check if some verboten C functions are being used.
  if Search(r'\bsprintf\b', line):
    error(filename, linenum, 'runtime/printf', 5,
          'Never use sprintf.  Use ph_snprintf instead.')
  match = Search(r'\b(strcpy|strcat)\b', line)
  if match:
    error(filename, linenum, 'runtime/printf', 4,
          'Almost always, ph_snprintf is better than %s' % match.group(1))

  if Search(r'\bsscanf\b', line):
    error(filename, linenum, 'runtime/printf', 1,
          'sscanf can be ok, but is slow and can overflow buffers.')

  # Check if some verboten operator overloading is going on
  # TODO(unknown): catch out-of-line unary operator&:
  #   class X {};
  #   int operator&(const X& x) { return 42; }  // unary operator&
  # The trick is it's hard to tell apart from binary operator&:
  #   class Y { int operator&(const Y& x) { return 23; } }; // binary operator&
  if Search(r'\boperator\s*&\s*\(\s*\)', line):
    error(filename, linenum, 'runtime/operator', 4,
          'Unary operator& is dangerous.  Do not use it.')

  # Check for suspicious usage of ""if"" like
  # } if (a == b) {
  if Search(r'\}\s*if\s*\(', line):
    error(filename, linenum, 'readability/braces', 4,
          'Did you mean ""else if""? If not, start a new line for ""if"".')

  # Check for potential format string bugs like printf(foo).
  # We constrain the pattern not to pick things like DocidForPrintf(foo).
  # Not perfect but it can catch printf(foo.c_str()) and printf(foo->c_str())
  # TODO(sugawarayu): Catch the following case. Need to change the calling
  # convention of the whole function to process multiple line to handle it.
  #   printf(
  #       boy_this_is_a_really_long_variable_that_cannot_fit_on_the_prev_line);
  printf_args = _GetTextInside(line, r'(?i)\b(string)?printf\s*\(')
  if printf_args:
    match = Match(r'([\w.\->()]+)$', printf_args)
    if match and match.group(1) != '__VA_ARGS__':
      function_name = re.search(r'\b((?:string)?printf)\s*\(',
                                line, re.I).group(1)
      error(filename, linenum, 'runtime/printf', 4,
            'Potential format string bug. Do %s(""%%s"", %s) instead.'
            % (function_name, match.group(1)))

  # Check for potential memset bugs like memset(buf, sizeof(buf), 0).
  match = Search(r'memset\s*\(([^,]*),\s*([^,]*),\s*0\s*\)', line)
  if match and not Match(r""^''|-?[0-9]+|0x[0-9A-Fa-f]$"", match.group(2)):
    error(filename, linenum, 'runtime/memset', 4,
          'Did you mean ""memset(%s, 0, %s)""?'
          % (match.group(1), match.group(2)))

  if Search(r'\busing namespace\b', line):
    error(filename, linenum, 'build/namespaces', 5,
          'Do not use namespace using-directives.  '
          'Use using-declarations instead.')

  # Detect variable-length arrays.
  match = Match(r'\s*(.+::)?(\w+) [a-z]\w*\[(.+)];', line)
  if (match and match.group(2) != 'return' and match.group(2) != 'delete' and
      match.group(3).find(']') == -1):
    # Split the size using space and arithmetic operators as delimiters.
    # If any of the resulting tokens are not compile time constants then
    # report the error.
    tokens = re.split(r'\s|\+|\-|\*|\/|<<|>>]', match.group(3))
    is_const = True
    skip_next = False
    for tok in tokens:
      if skip_next:
        skip_next = False
        continue

      if Search(r'sizeof\(.+\)', tok): continue
      if Search(r'arraysize\(\w+\)', tok): continue

      tok = tok.lstrip('(')
      tok = tok.rstrip(')')
      if not tok: continue
      if Match(r'\d+', tok): continue
      if Match(r'0[xX][0-9a-fA-F]+', tok): continue
      if Match(r'k[A-Z0-9]\w*', tok): continue
      if Match(r'(.+::)?k[A-Z0-9]\w*', tok): continue
      if Match(r'(.+::)?[A-Z][A-Z0-9_]*', tok): continue
      # A catch all for tricky sizeof cases, including 'sizeof expression',
      # 'sizeof(*type)', 'sizeof(const type)', 'sizeof(struct StructName)'
      # requires skipping the next token because we split on ' ' and '*'.
      if tok.startswith('sizeof'):
        skip_next = True
        continue
      is_const = False
      break
    if not is_const:
      error(filename, linenum, 'runtime/arrays', 1,
            'Do not use variable-length arrays.  Use an appropriately named '
            ""('k' followed by CamelCase) compile-time constant for the size."")

  # If DISALLOW_EVIL_CONSTRUCTORS, DISALLOW_COPY_AND_ASSIGN, or
  # DISALLOW_IMPLICIT_CONSTRUCTORS is present, then it should be the last thing
  # in the class declaration.
  match = Match(
      (r'\s*'
       r'(DISALLOW_(EVIL_CONSTRUCTORS|COPY_AND_ASSIGN|IMPLICIT_CONSTRUCTORS))'
       r'\(.*\);$'),
      line)
  if match and linenum + 1 < clean_lines.NumLines():
    next_line = clean_lines.elided[linenum + 1]
    # We allow some, but not all, declarations of variables to be present
    # in the statement that defines the class.  The [\w\*,\s]* fragment of
    # the regular expression below allows users to declare instances of
    # the class or pointers to instances, but not less common types such
    # as function pointers or arrays.  It's a tradeoff between allowing
    # reasonable code and avoiding trying to parse more C++ using regexps.
    if not Search(r'^\s*}[\w\*,\s]*;', next_line):
      error(filename, linenum, 'readability/constructors', 3,
            match.group(1) + ' should be the last thing in the class')

  # Check for use of unnamed namespaces in header files.  Registration
  # macros are typically OK, so we allow use of ""namespace {"" on lines
  # that end with backslashes.
  if (file_extension == 'h'
      and Search(r'\bnamespace\s*{', line)
      and line[-1] != '\\'):
    error(filename, linenum, 'build/namespaces', 4,
          'Do not use unnamed namespaces in header files.  See '
          'http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Namespaces'
          ' for more information.')"
511,"    def label_anchors(self, anchors: List[Boxes], gt_instances: List[Instances]):
        """"""
        Same interface as :meth:`RetinaNet.label_anchors`, but implemented with FCOS
        anchor matching rule.

        Unlike RetinaNet, there are no ignored anchors.
        """"""","
        gt_labels, matched_gt_boxes = [], []

        for inst in gt_instances:
            if len(inst) > 0:
                match_quality_matrix = self._match_anchors(inst.gt_boxes, anchors)

                # Find matched ground-truth box per anchor. Un-matched anchors are
                # assigned -1. This is equivalent to using an anchor matcher as used
                # in R-CNN/RetinaNet: `Matcher(thresholds=[1e-5], labels=[0, 1])`
                match_quality, matched_idxs = match_quality_matrix.max(dim=0)
                matched_idxs[match_quality < 1e-5] = -1

                matched_gt_boxes_i = inst.gt_boxes.tensor[matched_idxs.clip(min=0)]
                gt_labels_i = inst.gt_classes[matched_idxs.clip(min=0)]

                # Anchors with matched_idxs = -1 are labeled background.
                gt_labels_i[matched_idxs < 0] = self.num_classes
            else:
                matched_gt_boxes_i = torch.zeros_like(Boxes.cat(anchors).tensor)
                gt_labels_i = torch.full(
                    (len(matched_gt_boxes_i),),
                    fill_value=self.num_classes,
                    dtype=torch.long,
                    device=matched_gt_boxes_i.device,
                )

            gt_labels.append(gt_labels_i)
            matched_gt_boxes.append(matched_gt_boxes_i)

        return gt_labels, matched_gt_boxes"
512,"    def GenerateProposalLabels(self, blobs_in):
        """"""Op for generating training labels for RPN proposals. This is used
        when training RPN jointly with Fast/Mask R-CNN (as in end-to-end
        Faster R-CNN training).

        blobs_in:
          - 'rpn_rois': 2D tensor of RPN proposals output by GenerateProposals
          - 'roidb': roidb entries that will be labeled
          - 'im_info': See GenerateProposals doc.

        blobs_out:
          - (variable set of blobs): returns whatever blobs are required for
            training the model. It does this by querying the data loader for
            the list of blobs that are needed.
        """"""","        name = 'GenerateProposalLabelsOp:' + ','.join(
            [str(b) for b in blobs_in]
        )

        # The list of blobs is not known before run-time because it depends on
        # the specific model being trained. Query the data loader to get the
        # list of output blob names.
        blobs_out = fast_rcnn_roi_data.get_fast_rcnn_blob_names(
            is_training=self.train
        )
        blobs_out = [core.ScopedBlobReference(b) for b in blobs_out]

        self.net.Python(GenerateProposalLabelsOp().forward)(
            blobs_in, blobs_out, name=name
        )
        return blobs_out"
513,"def disabledext(name):
    """"""find a specific disabled extension from ext. returns desc""""""","    try:
        from edenscm.ext import __index__

        if name in _order:  # enabled
            return
        elif name in _exclude_list:
            return
        else:
            return gettext(__index__.docs.get(name))
    except (ImportError, AttributeError):
        pass

    paths = _disabledpaths()
    if name in paths and name not in _exclude_list:
        return _disabledhelp(paths[name])"
514,"def define_task(f):
    """"""Use @creator.define_task to decorate a task definition.""""""","
    return TempateTaskScript(f,
                             dict_of_template_values={},
                             version='1',
                             max_tasks=1,
                             search_params=SearchParams())"
515,"    def _init_shared_model(self, opt_key: str):
        """"""
        Initialize a shared version of the ""knowledge"" model.

        This just makes sure that each ""agent"" has the same params, but different
        history objects.

        :param opt_key:
            which sub agent to create with the shared model.
        """"""","        return super().init_shared_model(self.opts[opt_key], self.knowledge_agent)"
516,"    def __init__(self, crop_type: str, crop_size):
        """"""
        Args:
            crop_type (str): one of ""relative_range"", ""relative"", ""absolute"", ""absolute_range"".
            crop_size (tuple[float, float]): two floats, explained below.

        - ""relative"": crop a (H * crop_size[0], W * crop_size[1]) region from an input image of
          size (H, W). crop size should be in (0, 1]
        - ""relative_range"": uniformly sample two values from [crop_size[0], 1]
          and [crop_size[1]], 1], and use them as in ""relative"" crop type.
        - ""absolute"" crop a (crop_size[0], crop_size[1]) region from input image.
          crop_size must be smaller than the input image size.
        - ""absolute_range"", for an input of size (H, W), uniformly sample H_crop in
          [crop_size[0], min(H, crop_size[1])] and W_crop in [crop_size[0], min(W, crop_size[1])].
          Then crop a region (H_crop, W_crop).
        """"""","        # TODO style of relative_range and absolute_range are not consistent:
        # one takes (h, w) but another takes (min, max)
        super().__init__()
        assert crop_type in [""relative_range"", ""relative"", ""absolute"", ""absolute_range""]
        self._init(locals())"
517,"def get_dstats_fname(folder_to_save, task, dt='train'):
    """"""
    gets the data_stats file name.
    """"""","    taskname = task
    if '/' in task:
        taskname = task.split('/')[-1].split('.')[0]
    fname = '_'.join([taskname, dt]) + '.json'
    return os.path.join(folder_to_save, data_stats_folder, fname)"
518,"    def formatdict(data, key, value, fmt, sep):
        """"""stringify key-value pairs separated by sep""""""","        return sep.join(fmt % (k, v) for k, v in _iteritems(data))"
519,"    def get_task_prompt(self) -> str:
        """"""
        To keep flexibility, do not override this function.
        """"""","        prompt = self.opt.get(""task_prompt"", """")
        if prompt == """":
            raise RuntimeError(
                ""Got an empty string for the prompt - is this intentional? If not, set with ""
                ""`parser.set_defaults(task_prompt='<INSERT PROMPT>')` in `add_cmdline_args`.""
            )
        return prompt"
520,"    def _writeundo(self):
        """"""write transaction data for possible future undo call""""""","        if self.undoname is None:
            return
        undobackupfile = self.opener.open(""%s.backupfiles"" % self.undoname, ""wb"")
        undobackupfile.write(encodeutf8(""%d\n"" % version))
        for l, f, b, c in self._backupentries:
            if not f:  # temporary file
                continue
            if not b:
                u = """"
            else:
                if l not in self._vfsmap and c:
                    self.report(
                        ""couldn't remove %s: unknown cache location"" ""%s\n"" % (b, l)
                    )
                    continue
                vfs = self._vfsmap[l]
                base, name = vfs.split(b)
                assert name.startswith(self.journal), name
                uname = name.replace(self.journal, self.undoname, 1)
                u = vfs.reljoin(base, uname)
                util.copyfile(vfs.join(b), vfs.join(u), hardlink=True)
            undobackupfile.write(encodeutf8(""%s\0%s\0%s\0%d\n"" % (l, f, u, c)))
        undobackupfile.close()"
521,"def get_username():
    """"""
  get_username()


  Return Username
  """"""","
    os_platform = platform.system()

    if os_platform == ""Darwin"":
        cmd = [""/usr/bin/stat"", ""-f%Su"", ""/dev/console""]
        default_username = run(cmd)[""stdout""]

    else:
        default_username = getpass.getuser()

    username = input(""Username [%s]: "" % default_username)
    if not username:
        username = default_username

    return username"
522,"def sqltreestrip(ui, repo, rev: int, *args, **opts) -> Optional[int]:
    """"""Strips trees from local and sql history""""""","    try:
        treemfmod = extensions.find(""treemanifest"")
    except KeyError:
        ui.warn(_(""treemanifest is not enabled for this repository\n""))
        return 1

    if not repo.ui.configbool(""treemanifest"", ""server""):
        ui.warn(_(""this repository is not configured to be a treemanifest server\n""))
        return 1

    if not opts.get(""i_know_what_i_am_doing""):
        raise util.Abort(
            ""You must pass --i-know-what-i-am-doing to run this ""
            + ""command. If you have multiple servers using the database, this ""
            + ""command will break your servers until you run it on each one. ""
            + ""Only the Mercurial server admins should ever run this.""
        )

    rootonly = opts.get(""root_only"")
    rev = int(rev)

    ui.warn(
        _(
            ""*** YOU ARE ABOUT TO DELETE TREE HISTORY INCLUDING AND AFTER %s ""
            ""(MANDATORY 5 SECOND WAIT) ***\n""
        )
        % rev
    )
    import time

    time.sleep(5)

    if not opts.get(""local_only""):
        # strip from sql
        reponame = repo.sqlreponame
        with repo.lock():
            repo.sqlconnect()
            repo.sqlwritelock()
            try:
                cursor = repo.sqlcursor

                if rootonly:
                    ui.status(
                        _(""mysql: deleting root trees with linkrevs >= %s\n"") % rev
                    )
                    pathfilter = ""(path = '00manifesttree.i')""
                else:
                    ui.status(_(""mysql: deleting trees with linkrevs >= %s\n"") % rev)
                    pathfilter = ""(path LIKE 'meta/%%' OR path = '00manifesttree.i')""

                cursor.execute(
                    """"""DELETE FROM revisions WHERE repo = %s AND linkrev >= %s
                       AND """"""
                    + pathfilter,
                    (reponame, rev),
                )
                repo.sqlconn.commit()
            finally:
                repo.sqlwriteunlock()
                repo.sqlclose()

    # strip from local
    with repo.wlock(), repo.lock(), repo.transaction(""treestrip"") as tr:
        repo.disablesync = True

        # Duplicating some logic from repair.py
        offset = len(tr.entries)
        tr.startgroup()
        if opts.get(""root_only""):
            ui.status(_(""local: deleting root trees with linkrevs >= %s\n"") % rev)
            treerevlog = repo.manifestlog.treemanifestlog._revlog
            treerevlog.strip(rev, tr)
        else:
            ui.status(_(""local: deleting trees with linkrevs >= %s\n"") % rev)
            files = treemfmod.collectfiles(None, repo, rev)
            treemfmod.striptrees(None, repo, tr, rev, files)
        tr.endgroup()

        for i in range(offset, len(tr.entries)):
            file, troffset, ignore = tr.entries[i]
            with repo.svfs(file, ""a"", checkambig=True) as fp:
                util.truncate(fp, troffset)
            if troffset == 0:
                repo.store.markremoved(file)"
523,"    def _remove_non_bpe(self):
        """"""
        Set the dictionary vocab to the bpe vocab, merging counts.
        """"""","        to_remove = []
        to_add = []
        for token, freq in self.freq.items():
            tokens = self.bpe_tokenize(token)
            if len(tokens) != 1:
                for t in tokens:
                    to_add.append((t, freq))
                to_remove.append(token)
        for token in to_remove:
            del self.freq[token]
            idx = self.tok2ind.pop(token)
            del self.ind2tok[idx]
        for token, freq in to_add:
            self.add_token(token)
            self.freq[token] += freq"
524,"    def update_intrinsic_moments(self, reward_batch):
        """"""Maintains a running mean of reward.""""""","        new_count = len(reward_batch)
        new_sum = torch.sum(reward_batch)
        new_mean = new_sum / new_count

        curr_mean = self.intrinsic_sum / self.intrinsic_count
        new_m2 = torch.sum((reward_batch - new_mean) ** 2) + (
            (self.intrinsic_count * new_count)
            / (self.intrinsic_count + new_count)
            * (new_mean - curr_mean) ** 2
        )

        self.intrinsic_count += new_count
        self.intrinsic_sum += new_sum
        self.intrinsic_m2 += new_m2"
525,"    def _print_fields(print_list):
        """"""Internal function which prints the fields of a struct/class/union.
        """"""","        max_field_name_length = 0
        for pair in print_list:
            if max_field_name_length < len(pair[0]):
                max_field_name_length = len(pair[0])

        for pair in print_list:
            print (""  %*s = %s"" % (max_field_name_length, pair[0], pair[1]))"
526,"    def __init__(
        self,
        cache_dir: Optional[str] = None,
        profile: Optional[str] = ""saml"",
        # pyre-fixme[24]: Generic type `dict` expects 2 type parameters, use
        #  `typing.Dict` to avoid runtime subscripting errors.
        transfer_config_kwargs: Optional[Dict] = None,
    ):
        """"""
        Args:
            cache_dir (str): Local filesystem directory to use for caching. If None,
                uses default from `file_io.get_cache_dir()`.
            transfer_config_kwargs (dict): Settings for boto3.s3.transfer.TransferConfig.
                Used to specify settings for multipart transfers.
                See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html for details.
        """"""","        self.cache_dir = cache_dir
        self.profile = profile
        # pyre-fixme[21]: Could not find module `boto3.s3.transfer`.
        from boto3.s3.transfer import TransferConfig

        # pyre-fixme[4]: Attribute must be annotated.
        self.transfer_config = TransferConfig(
            **(transfer_config_kwargs if transfer_config_kwargs else {})
        )"
527,"def deprecated(pe):
    """"""Check for DEPRECATED
    >>> ped = polib.POEntry(
    ...     msgid = '(DEPRECATED)',
    ...     msgstr= '(DETACERPED)')
    >>> deprecatedsetup([ped])
    >>> pe = polib.POEntry(
    ...     msgid = 'Something (DEPRECATED)',
    ...     msgstr= 'something (DEPRECATED)')
    >>> match(deprecated, pe)
    True
    >>> for e in deprecated(pe): print(e)
    >>> pe = polib.POEntry(
    ...     msgid = 'Something (DEPRECATED)',
    ...     msgstr= 'something (DETACERPED)')
    >>> match(deprecated, pe)
    True
    >>> for e in deprecated(pe): print(e)
    >>> pe = polib.POEntry(
    ...     msgid = 'Something (DEPRECATED)',
    ...     msgstr= 'something')
    >>> match(deprecated, pe)
    True
    >>> for e in deprecated(pe): print(e)
    msgstr inconsistently translated (DEPRECATED)
    >>> pe = polib.POEntry(
    ...     msgid = 'Something (DEPRECATED, foo bar)',
    ...     msgstr= 'something (DETACERPED, foo bar)')
    >>> match(deprecated, pe)
    """"""","    if not (
        ""(DEPRECATED)"" in pe.msgstr
        or (deprecatedpe and deprecatedpe.msgstr in pe.msgstr)
    ):
        yield ""msgstr inconsistently translated (DEPRECATED)"""
528,"    def replacement_reconstruct_batch(self, key, x=None):
        """"""Approximate reconstruction of several vectors from the index.

        Parameters
        ----------
        key : array of ints
            Ids of the vectors to reconstruct
        x : array_like, optional
            pre-allocated array to store the results

        Returns
        -------
        x : array_like
            reconstrcuted vectors, size `len(key), self.d`
        """"""","        key = np.ascontiguousarray(key, dtype='int64')
        n, = key.shape
        if x is None:
            x = np.empty((n, self.d), dtype=np.float32)
        else:
            assert x.shape == (n, self.d)
        self.reconstruct_batch_c(n, swig_ptr(key), swig_ptr(x))
        return x"
529,"    def process_task(self, task):
        """"""
        tries to remap tasks to their external version, and then may ignore the tasks
        w/o ext.

        version depending on `ignore_task`
        """"""","        # processing tasks so that no arguments are included
        # unless it's a fromfile or jsonfile one
        if 'fromfile:' in task or 'jsonfile:' in task or 'internal:' in task:
            return None if self.ignore_task else task
        return task"
530,"    def patch(self):
        """"""Apply all of the patches that have been specified with `add_patch`.

        Reverse this operation using L{restore}.
        """"""","        for obj, name, value in self._patches_to_apply:
            original_value = getattr(obj, name, self._NO_SUCH_ATTRIBUTE)
            self._originals.append((obj, name, original_value))
            setattr(obj, name, value)"
531,"    def dirname(self, path: str) -> str:
        """"""return dirname element of a path (as os.path.dirname would do)

        This exists to allow handling of strange encoding if needed.""""""",        return os.path.dirname(path)
532,"    def _sync_metrics(self, metrics):
        """"""
        Sync training metrics across workers.

        A handful of special cases are handled as exceptions, and the remaining metrics
        are simply averaged across workers.
        """"""","        if not is_distributed():
            # nothing special needed
            return metrics
        all_versions = all_gather_list(metrics)
        return aggregate_unnamed_reports(all_versions)"
533,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Add CLI args.
        """"""","        pass
        return parser"
534,"    def __init__(self, size: int, shuffle: bool = True, seed: Optional[int] = None):
        """"""
        Args:
            size (int): the total number of data of the underlying dataset to sample from
            shuffle (bool): whether to shuffle the indices or not
            seed (int): the initial seed of the shuffle. Must be the same
                across all workers. If None, will use a random seed shared
                among workers (require synchronization among all workers).
        """"""","        if not isinstance(size, int):
            raise TypeError(f""TrainingSampler(size=) expects an int. Got type {type(size)}."")
        if size <= 0:
            raise ValueError(f""TrainingSampler(size=) expects a positive int. Got {size}."")
        self._size = size
        self._shuffle = shuffle
        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()"
535,"def _findstackbottom(ui, repo):
    """"""Find the lowest non-public ancestor of the current changeset.""""""","    if repo["".""].phase() == phases.public:
        raise error.Abort(_(""current changeset is public""))
    bottoms = list(repo.nodes(""roots(draft() & ::.)""))
    if len(bottoms) > 1:
        ui.warn(_(""current stack has multiple bottom changesets, namely:\n""))
        _showchangesets(ui, repo, nodes=bottoms, indices=ui.interactive())
        if ui.interactive():
            return _choosenode(ui, bottoms)
        raise error.Abort(
            _(""ambiguous bottom changeset""),
        )
    else:
        return next(iter(bottoms), None)"
536,"    def _process(self, build_env, path, is_implicit_include, package_implicit_load):
        # type: (_GCT, str, bool, Optional[LoadStatement]) -> Tuple[_GCT, types.ModuleType]
        """"""Process a build file or include at the given path.

        :param build_env: context of the file to process.
        :param path: target-like path to the file to process.
        :param is_implicit_include: whether the file being processed is an implicit include, or was
            included from an implicit include.
        :package_implicit_load: if provided, a dictionary containing the path to
                                load for this given package, and the symbols to load
                                from that .bzl file.
        :returns: build context (potentially different if retrieved from cache) and loaded module.
        """"""","        if isinstance(build_env, IncludeContext):
            default_globals = (
                self._default_globals_for_implicit_include
                if is_implicit_include
                else self._default_globals_for_extension
            )
        else:
            default_globals = self._default_globals_for_build_file

        emit_trace(path)

        # Install the build context for this input as the current context.
        with self._set_build_env(build_env):
            # Don't include implicit includes if the current file being
            # processed is an implicit include
            if not is_implicit_include:
                for include in self._implicit_includes:
                    build_include = self._resolve_include(include)
                    inner_env, mod = self._process_include(build_include, True)
                    self._merge_globals(mod, default_globals)
                    build_env.includes.add(build_include.path)
                    build_env.merge(inner_env)

                if package_implicit_load:
                    self._load_package_implicit(build_env, package_implicit_load)

            # Build a new module for the given file, using the default globals
            # created above.
            module = imp.new_module(path)
            module.__file__ = path
            module.__dict__.update(default_globals)

            # We don't open this file as binary, as we assume it's a textual source
            # file.
            with scoped_trace(""IO"", stats_key=""IO""):
                with self._wrap_file_access(wrap=False):
                    with open(path, ""r"") as f:
                        contents = f.read()

            with scoped_trace(""Compile"", stats_key=""Compile""):
                # Enable absolute imports.  This prevents the compiler from
                # trying to do a relative import first, and warning that
                # this module doesn't exist in sys.modules.
                future_features = absolute_import.compiler_flag
                code = compile(contents, path, ""exec"", future_features, 1)

                # Execute code with build file sandboxing
                with self._build_file_sandboxing():
                    exec(code, module.__dict__)

        return build_env, module"
537,"def register_datasets(
    datasets_data: Iterable[CocoDatasetInfo], datasets_root: Optional[str] = None
) -> None:
    """"""
    Registers provided LVIS DensePose datasets

    Args:
        datasets_data: Iterable[CocoDatasetInfo]
            An iterable of dataset datas
        datasets_root: Optional[str]
            Datasets root folder (default: None)
    """"""","    for dataset_data in datasets_data:
        register_dataset(dataset_data, datasets_root)"
538,"def cloudbackup(ui, repo, *revs, **opts):
    """"""back up commits to commit cloud

    Commits that have already been backed up will be skipped.

    If no revision is specified, backs up all visible commits.
    """"""","    revs = revs + tuple(opts.get(""rev"", ()))
    if ui.configbool(""commitcloud"", ""usehttpupload""):
        opts[""rev""] = revs
        return cloudupload(ui, repo, **opts)

    repo.ignoreautobackup = True

    force = opts.get(""force"")
    inbackground = opts.get(""background"")
    if revs:
        if inbackground:
            raise error.Abort(""'--background' cannot be used with specific revisions"")
        revs = scmutil.revrange(repo, revs)
    else:
        revs = None

    if force and inbackground:
        raise error.Abort(""'--background' cannot be used with '--force'"")

    if inbackground:
        background.backgroundbackup(repo)
        return 0

    backedup, failed = backup.backup(
        repo,
        revs,
        connect_opts=opts,
        force=force,
    )

    if backedup:
        repo.ui.status(
            _n(""backed up %d commit\n"", ""backed up %d commits\n"", len(backedup))
            % len(backedup),
            component=""commitcloud"",
        )
    if failed:
        repo.ui.warn(
            _n(
                ""failed to back up %d commit\n"",
                ""failed to back up %d commits\n"",
                len(failed),
            )
            % len(failed),
            component=""commitcloud"",
        )
    if not backedup and not failed:
        repo.ui.status(_(""nothing to back up\n""))
    return 0 if not failed else 2"
539,"    def get_bounding_boxes(self) -> Boxes:
        """"""
        Returns:
            Boxes: tight bounding boxes around bitmasks.
            If a mask is empty, it's bounding box will be all zero.
        """"""","        boxes = torch.zeros(self.tensor.shape[0], 4, dtype=torch.float32)
        x_any = torch.any(self.tensor, dim=1)
        y_any = torch.any(self.tensor, dim=2)
        for idx in range(self.tensor.shape[0]):
            x = torch.where(x_any[idx, :])[0]
            y = torch.where(y_any[idx, :])[0]
            if len(x) > 0 and len(y) > 0:
                boxes[idx, :] = torch.as_tensor(
                    [x[0], y[0], x[-1] + 1, y[-1] + 1], dtype=torch.float32
                )
        return Boxes(boxes)"
540,"    def __init__(self, unknown_idx, probability):
        """"""
        Initialize layer.

        :param unknown_idx: index of unknown token, replace tokens with this
        :param probability: during training, replaces tokens with unknown token
                            at this rate.
        """"""","        super().__init__()
        self.unknown_idx = unknown_idx
        self.prob = probability"
541,"    def revs(self, expr, *args, **kwargs):
        """"""Find revisions matching a revset.

        The revset is specified as a string ``expr`` that may contain
        %-formatting to escape certain types. See ``revsetlang.formatspec``.

        Revset aliases from the configuration are not expanded. To expand
        user aliases, consider calling ``scmutil.revrange()`` or
        ``repo.anyrevs([expr], user=True)``.

        Returns a revset.abstractsmartset, which is a list-like interface
        that contains integer revisions.
        """"""","        expr = revsetlang.formatspec(expr, *args)
        m = revset.match(None, expr)
        subset = kwargs.get(""subset"", None)
        return m(self, subset=subset)"
542,"    def retrieve_and_score(
        self, query: torch.LongTensor
    ) -> Tuple[List[List[Document]], torch.Tensor]:
        """"""
        Return the search engine docs if in search mode; otherwise, return memories.
        """"""","        if self.retriever_type == 'search':
            return super().retrieve_and_score(query)

        top_docs = []
        top_doc_scores = []
        max_n_docs: int = self.n_docs
        for memories in self.memory:
            docs_i = []
            scores_i = []
            for memory in memories:
                docs_i.append(Document(docid='', text=memory, title=''))
                scores_i.append(1)
            # Change this debug later
            max_n_docs = max(max_n_docs, len(docs_i))
            top_docs.append(docs_i)
            top_doc_scores.append(scores_i)
        # Pad with empty docs
        for i in range(len(top_docs)):
            n_empty = max_n_docs - len(top_docs[i])
            if n_empty:
                top_docs[i] = top_docs[i] + [BLANK_DOC] * n_empty
                top_doc_scores[i] = top_doc_scores[i] + [0] * n_empty
        self.top_docs = top_docs
        return top_docs, torch.Tensor(top_doc_scores).to(query.device)"
543,"def debugbindag(ui, repo, rev=None, output=None) -> None:
    """"""serialize dag to a compat binary format

    See 'dagparser.bindag' for the actual format.

    If --output is not specified, the serialized result will be written to
    'dag.out'.
    """"""","    revs = scmutil.revrange(repo, rev)
    revs.sort()

    parentrevs = repo.changelog.parentrevs
    data = dagparser.bindag(revs, parentrevs)
    util.writefile(output or ""dag.out"", data)"
544,"    def encoder(
        self,
        input: torch.LongTensor,
        input_lengths: torch.LongTensor,
        query_vec: torch.LongTensor,
        input_turns_cnt: torch.LongTensor,
        skip_retrieval_vec: torch.BoolTensor,
        target_lengths: torch.LongTensor,
        positions: Optional[torch.LongTensor] = None,
        segments: Optional[torch.LongTensor] = None,
    ) -> Tuple[
        torch.Tensor,
        torch.BoolTensor,
        Optional[torch.LongTensor],
        Optional[List[List[Document]]],
        Optional[torch.Tensor],
    ]:
        """"""
        Copy most of the ComboFidModel.forward here, but include the target_lengths for
        the encoder.

        Note, however, that we don't use it.
        """"""","        # If we have a full batch of retrieve (or not), can use simple logic here.
        if torch.all(skip_retrieval_vec):
            new_out, new_mask = self.seq2seq_encoder(input, positions, segments)
            return (
                new_out.unsqueeze(-1),
                new_mask,
                None,
                None,
                None,
            )  # need to unsqueeze to handle in decoder
        elif torch.all(~skip_retrieval_vec):
            output = super().encoder(
                input,
                input_lengths,
                query_vec,
                input_turns_cnt,
                None,
                positions,
                segments,
            )
            self.top_docs = output[-2]  # type: ignore
            return output
        assert all(t is None for t in [input_turns_cnt, positions, segments])
        # Encode with `super()` call for non-skip-retrieval inputs
        (
            enc_out_retrieval,
            mask_retrieval,
            _,
            top_docs,
            top_doc_scores,
        ) = super().encoder(
            input[~skip_retrieval_vec],  # type: ignore
            input_lengths[~skip_retrieval_vec],  # type: ignore
            query_vec[~skip_retrieval_vec],  # type: ignore
            input_turns_cnt,
            None,  # don't pass target lengths when combining
            positions,
            segments,
        )
        # Encode with seq2seq_encoder for skip-retrieval inputs
        enc_out_skip_retrieval, mask_skip_retrieval = self.seq2seq_encoder(
            input[skip_retrieval_vec]
        )
        (
            new_out,
            new_mask,
            new_top_docs,
            new_top_doc_scores,
        ) = interleave_fid_combo_outputs(
            enc_out_retrieval,
            enc_out_skip_retrieval.unsqueeze(-1),
            mask_retrieval,
            mask_skip_retrieval,
            skip_retrieval_vec,
            top_docs,  # type: ignore
            top_doc_scores,  # type: ignore
            right_padded=False,
        )
        self.top_docs = new_top_docs
        return new_out, new_mask, input_turns_cnt, new_top_docs, new_top_doc_scores"
545,"    def torule(self):
        """"""build a histedit rule line for an action

        by default lines are in the form:
        <hash> <rev> <summary>
        """"""","        ctx = self.repo[self.node]
        summary = _getsummary(ctx)
        line = ""%s %s %s"" % (self.verb, ctx, summary)
        # trim to 75 columns by default so it's not stupidly wide in my editor
        # (the 5 more are left for verb)
        maxlen = self.repo.ui.configint(""histedit"", ""linelen"")
        maxlen = max(maxlen, 22)  # avoid truncating hash
        return util.ellipsis(line, maxlen)"
546,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Add RAG Args.
        """"""","        RagAgent.add_cmdline_args(parser, partial_opt)
        SearchQuerySearchEngineFiDAgent.add_cmdline_args(parser, partial_opt)
        bb2_group = parser.add_argument_group('BlenderBot2 Args')
        bb2_group.add_argument(
            '--knowledge-access-method',
            type=str,
            default=KnowledgeAccessMethod.CLASSIFY.value,
            choices=[r.value for r in KnowledgeAccessMethod],
            help='How to access knowledge for BlenderBot2 '
            'classify => classify the input text, determine which knowledge to access\n'
            'memory_only => only access memories\n'
            'search_only => only access search\n'
            'all => for each input, access from memories and search\n'
            'none => do not access any knowledge.\n',
        )
        bb2_group.add_argument(
            '--memory-key',
            type=str,
            default='full_text',
            help='Field in the observation from which to read memories.',
        )
        bb2_group.add_argument(
            '--query-generator-key',
            type=str,
            default='full_text',
            help='Field for input to the knowledge access classifier.',
        )
        bb2_group.add_argument(
            '--gold-document-key',
            type=str,
            default=SELECTED_DOCS,
            help='Field for selected docs.',
        )
        bb2_group.add_argument(
            '--gold-sentence-key',
            type=str,
            default=SELECTED_SENTENCES,
            help='Field for selected sentences',
        )
        bb2_group.add_argument(
            '--gold-document-titles-key',
            type=str,
            default=SELECTED_DOCS_TITLES,
            help='Field for selected docs titles.',
        )
        bb2_group.add_argument(
            '--skip-search-key',
            type=str,
            default=SKIP_SEARCH,
            help='Field for whether to skip search or not.',
        )
        bb2_group.add_argument(
            '--insert-gold-docs',
            type='bool',
            default=False,
            help='Set true to insert gold docs into retrieved docs.',
        )
        bb2_group.add_argument(
            '--memory-extractor-phrase',
            type=str,
            default='persona:',
            help=""phrase used to extract memories from `--memory-key` in the observation. ""
            ""For example, set to 'your persona:' to limit memories to only lines that ""
            ""contain 'your persona:'"",
        )
        bb2_group.add_argument(
            '--retriever-ignore-phrase',
            type=str,
            default='persona:',
            help='filter input to the global knowledge retriever such that any utterance containing '
            'the phrase will not be given as input.',
        )
        q_gen_group = parser.add_argument_group('BlenderBot2 Query Generator Args')
        q_gen_group.add_argument(
            '--query-generator-ignore-phrase',
            type=str,
            default='persona:',
            help='filter input to the query generator such that any utterance containing '
            'the phrase will not be given as input.',
        )
        q_gen_group.add_argument(
            '--query-generator-model-file',
            type=str,
            default=ZOO_QUERY_GENERATOR,
            help='path to a query generator; specify if searching OR classifying inputs.',
        )
        q_gen_group.add_argument(
            '--query-generator-delimiter',
            type=str,
            default='\n',
            help='delimiter for the query generator',
        )
        q_gen_group.add_argument(
            '--query-generator-inference',
            type=str,
            default='beam',
            help='query generator inference type',
        )
        q_gen_group.add_argument(
            '--query-generator-beam-size', type=int, default=1, help='SQ Gen Beam Size'
        )
        q_gen_group.add_argument(
            '--query-generator-beam-min-length',
            type=int,
            default=2,
            help='SQ Gen Beam Min Length',
        )
        q_gen_group.add_argument(
            '--query-generator-truncate',
            type=int,
            default=-1,
            help='Specify >0 for truncation to SQ generator',
        )
        bb2_group.add_argument(
            '--memory-retriever-truncate',
            type=int,
            default=-1,
            help='Specify >0 for truncation to the memory retriever.',
        )
        bb2_group.add_argument(
            '--retriever-delimiter',
            type=str,
            default='\n',
            help='delimiter for the retriever',
        )
        bb2_group.add_argument(
            '--share-search-and-memory-query-encoder',
            type='bool',
            default=False,
            help='if true, query encoder is shared between search and memory retrievers.',
        )
        bb2_group.add_argument(
            '--memory-reader-model',
            type=str,
            default=None,
            choices=QUERY_MODEL_TYPES,
            help='Model for accessing the memory',
        )
        bb2_group.add_argument(
            '--memory-doc-title-delimiter',
            type=str,
            default=' / ',
            help='title delimiter for memory docs',
        )
        bb2_group.add_argument(
            '--memory-writer-model',
            type=str,
            default='bert',
            hidden=True,
            help='model for writing the memories',
        )
        bb2_group.add_argument(
            '--memory-writer-model-file',
            type=str,
            default=DPR_ZOO_MODEL,
            hidden=True,
            help='model file for memory writer',
        )
        bb2_group.add_argument(
            '--add-cleaned-reply-to-history',
            type=bool,
            default=False,
            help='whether to add the cleaned bb2 generated text without any special tokens to its history',
        )
        memory_decoder = parser.add_argument_group('BlenderBot2 Memory Decoder Args')
        memory_decoder.add_argument(
            '--memory-decoder-key',
            type=str,
            default='full_text',
            help='key of the observation for the memory decoder',
        )
        memory_decoder.add_argument(
            '--memory-decoder-ignore-phrase',
            type=str,
            default='persona:',
            help='filter input to the memory decoder such that any utterance containing '
            'the phrase will not be given as input.',
        )
        memory_decoder.add_argument(
            '--memory-decoder-model-file',
            type=str,
            default=ZOO_MEMORY_DECODER,
            help='path to a memory decoder.',
        )
        memory_decoder.add_argument(
            '--memory-decoder-delimiter',
            type=str,
            default='\n',
            help='delimiter for the memory decoder',
        )
        memory_decoder.add_argument(
            '--memory-decoder-beam-size',
            type=int,
            default=3,
            help='memory decoder Beam Size',
        )
        memory_decoder.add_argument(
            '--memory-decoder-beam-min-length',
            type=int,
            default=10,
            help='memory decoder Beam Min Length',
        )
        memory_decoder.add_argument(
            '--memory-decoder-truncate',
            type=int,
            default=-1,
            help='Specify >0 for truncation to memory decoder',
        )
        memory_decoder.add_argument(
            '--memory-decoder-one-line-memories',
            type='bool',
            default=False,
            help='specify to combine memories on one line, rather than several.',
        )
        return parser"
547,"    def encode(self, txt: str, txt_pair: Optional[str] = None) -> List[int]:
        """"""
        Encode text.

        :param txt:
            text to encode
        :param txt_pair:
            Optional additional text to encode.
            Useful if encoding two parts of a text, e.g. title & text.

        :return encoding:
            return encoded text.
        """"""","        if self.query_model in ['bert', 'bert_from_parlai_rag']:
            txt = txt.lower().strip()
            if txt_pair:
                txt_pair = txt_pair.lower().strip()
            return self.tokenizer.encode(
                txt,
                text_pair=txt_pair,
                add_special_tokens=True,
                max_length=self.max_length,
                pad_to_max_length=False,
                truncation='longest_first',
            )
        else:
            return self.tokenizer.txt2vec(txt)"
548,"    def step(self):
        """"""
        User should either: (1) Call this function to increment storage.iter when needed. Or
        (2) Set `storage.iter` to the correct iteration number before each iteration.

        The storage will then be able to associate the new data with an iteration number.
        """"""",        self._iter += 1
549,"def trivial_batch_collator(batch):
    """"""
    A batch collator that does nothing.
    """"""",    return batch
550,"    def test_alt_reduction(self):
        """"""
        Test a transformer ranker reduction method other than `mean`.
        """"""","        valid, test = self._overfit_train(
            variant='xlm',
            activation='gelu',
            reduction_type='first',  # this is really what we're trying to test for
        )

        self.assertGreaterEqual(valid['hits@1'], 0.99)
        self.assertGreaterEqual(test['hits@1'], 0.99)"
551,"    def _model_input(
        self, batch: Batch
    ) -> Tuple[
        torch.LongTensor,
        torch.LongTensor,
        torch.LongTensor,
        torch.LongTensor,
        torch.BoolTensor,
    ]:
        """"""
        Override FidModel._model_input to add skip_retrieval_vec.
        """"""","        return (
            batch.text_vec,
            batch.text_vec.ne(self.NULL_IDX).sum(1),
            batch.query_vec,
            batch.input_turn_cnt_vec,
            batch.skip_retrieval_vec,
        )"
552,"    def episode_done(self):
        """"""
        A ParlAI-Mephisto task ends and allows workers to be marked complete when the
        world is finished.
        """"""",        return self.episodeDone
553,"    def _initialize_confidence_estimation_layers(self, cfg: CfgNode, dim_in: int):
        """"""
        Initialize confidence estimation layers based on configuration options

        Args:
            cfg (CfgNode): configuration options
            dim_in (int): number of input channels
        """"""","        dim_out_patches = cfg.MODEL.ROI_DENSEPOSE_HEAD.NUM_PATCHES + 1
        kernel_size = cfg.MODEL.ROI_DENSEPOSE_HEAD.DECONV_KERNEL
        if self.confidence_model_cfg.uv_confidence.enabled:
            if self.confidence_model_cfg.uv_confidence.type == DensePoseUVConfidenceType.IID_ISO:
                self.sigma_2_lowres = ConvTranspose2d(  # pyre-ignore[16]
                    dim_in, dim_out_patches, kernel_size, stride=2, padding=int(kernel_size / 2 - 1)
                )
            elif (
                self.confidence_model_cfg.uv_confidence.type
                == DensePoseUVConfidenceType.INDEP_ANISO
            ):
                self.sigma_2_lowres = ConvTranspose2d(
                    dim_in, dim_out_patches, kernel_size, stride=2, padding=int(kernel_size / 2 - 1)
                )
                self.kappa_u_lowres = ConvTranspose2d(  # pyre-ignore[16]
                    dim_in, dim_out_patches, kernel_size, stride=2, padding=int(kernel_size / 2 - 1)
                )
                self.kappa_v_lowres = ConvTranspose2d(  # pyre-ignore[16]
                    dim_in, dim_out_patches, kernel_size, stride=2, padding=int(kernel_size / 2 - 1)
                )
            else:
                raise ValueError(
                    f""Unknown confidence model type: ""
                    f""{self.confidence_model_cfg.confidence_model_type}""
                )
        if self.confidence_model_cfg.segm_confidence.enabled:
            self.fine_segm_confidence_lowres = ConvTranspose2d(  # pyre-ignore[16]
                dim_in, 1, kernel_size, stride=2, padding=int(kernel_size / 2 - 1)
            )
            self.coarse_segm_confidence_lowres = ConvTranspose2d(  # pyre-ignore[16]
                dim_in, 1, kernel_size, stride=2, padding=int(kernel_size / 2 - 1)
            )"
554,"def pathcopies(x, y, match=None):
    """"""find {dst@y: src@x} copy mapping for directed compare""""""","    # git does not track copy information. It will have to be implemented
    # differently. For now, git pathcopies remains unimplemented.
    # eagerepo is similar - pathcopies need a new implementation.
    from . import eagerepo

    if git.isgitformat(x.repo()) or eagerepo.iseagerepo(x.repo()):
        return {}

    if x == y or not x or not y:
        return {}
    a = y.ancestor(x)
    if a == x:
        return _forwardcopies(x, y, match=match)
    if a == y:
        return _backwardrenames(x, y)
    return _chain(x, y, _backwardrenames(x, a), _forwardcopies(a, y, match=match))"
555,"    def newpart(self, typeid, *args, **kwargs):
        """"""create a new part and add it to the containers

        As the part is directly added to the containers. For now, this means
        that any failure to properly initialize the part after calling
        ``newpart`` should result in a failure of the whole bundling process.

        You can still fall back to manually create and add if you need better
        control.""""""","        part = bundlepart(typeid, *args, **kwargs)
        self.addpart(part)
        return part"
556,"def _checkobsrebase(repo, ui, rebaseobsrevs, rebaseobsskipped) -> None:
    """"""
    Abort if rebase will create divergence or rebase is noop because of markers

    `rebaseobsrevs`: set of obsolete revision in source
    `rebaseobsskipped`: set of revisions from source skipped because they have
    successors in destination
    """"""","    # Obsolete node with successors not in dest leads to divergence
    divergenceok = ui.configbool(""experimental"", ""evolution.allowdivergence"")
    divergencebasecandidates = rebaseobsrevs - rebaseobsskipped

    if divergencebasecandidates and not divergenceok:
        divhashes = (str(repo[r]) for r in divergencebasecandidates)
        msg = _(""this rebase will cause "" ""divergences from: %s"")
        h = _(
            ""to force the rebase please set ""
            ""experimental.evolution.allowdivergence=True""
        )
        raise error.Abort(msg % ("","".join(divhashes),), hint=h)"
557,"  def CheckBegin(self, filename, clean_lines, linenum, error):
    """"""Run checks that applies to text up to the opening brace.

    This is mostly for checking the text after the class identifier
    and the ""{"", usually where the base class is specified.  For other
    blocks, there isn't much to check, so we always pass.

    Args:
      filename: The name of the current file.
      clean_lines: A CleansedLines instance containing the file.
      linenum: The number of the line to check.
      error: The function to call with any errors found.
    """"""",    pass
558,"    def _run_initial_turn(self) -> None:
        """"""
        Run the initial turn for both the human and the bot.

        Optionally show the bot its persona. If we are in Meena-like conversation mode
        show ""Hi!"" to the human and the bot and let the bot respond accordingly.

        Check parley() function for more information on the main logic.
        """"""","        control_msg = {""episode_done"": False}

        if self.opt['include_persona']:
            # The Bot agent
            # We add the personas and 1/3 of the time WoW topic as the
            # first utterance in the history.
            # Previously for BST task, we also had a big first utterance
            # that gave instructions. Removing that for this task.
            persona_strings = [s.strip() for s in self.personas[1]]
            persona_utterance = self._get_persona_utterance(
                persona_strings=persona_strings,
                context_dataset=self.context_info['context_dataset'],
                additional_context=self.context_info['additional_context'],
                is_bot=True,
            )
            message = control_msg.copy()
            message['text'] = persona_utterance
            # The bot seeing its persona does not count as a ""turn""
            self.bots[0].observe(validate(message), increment_turn=False)
            self.bots[1].observe(validate(message), increment_turn=False)

        if self.opt['conversation_start_mode'] == 'hi':
            logging.info('[Displaying ""Hi!"" only as per Meena task.]')
            if self.personas is not None:
                human_persona_strings = [s.strip() for s in self.personas[0]]
            else:
                human_persona_strings = ['', '']
            human_first_msg = {
                'episode_done': False,
                'id': self.agent.id,
                'text': 'Hi!',
                'fake_start': True,
                'agent_idx': 0,
                'task_data': {
                    'human_persona_string_1': human_persona_strings[0],
                    'human_persona_string_2': human_persona_strings[1],
                    'prompt_instruction': self.opt['task_question'],
                },
            }
            for k, v in control_msg.items():
                human_first_msg[k] = v

            # The first message is always ""Hi"", so we have both bots observe the message

            self.dialog.append(human_first_msg)
            self.agent.observe(validate(human_first_msg))
            self.bots[0].observe(validate(human_first_msg))
            self.bots[1].observe(validate(human_first_msg))

            bot_1_response = self.bots[0].act()
            bot_1_response = Compatibility.maybe_fix_act(bot_1_response)

            bot_2_response = self.bots[1].act()
            bot_2_response = Compatibility.maybe_fix_act(bot_2_response)

            if random.random() > 0.5:
                task_data = {
                    'top_bot_data': {
                        'top_bot_id': self.bots[0].worker_id,
                        'top_bot_response': bot_1_response,
                    },
                    'bottom_bot_data': {
                        'bottom_bot_id': self.bots[1].worker_id,
                        'bottom_bot_response': bot_2_response,
                    },
                    'task_turn_idx': self.task_turn_idx,
                }
            else:
                task_data = {
                    'top_bot_data': {
                        'top_bot_id': self.bots[1].worker_id,
                        'top_bot_response': bot_2_response,
                    },
                    'bottom_bot_data': {
                        'bottom_bot_id': self.bots[0].worker_id,
                        'bottom_bot_response': bot_1_response,
                    },
                    'task_turn_idx': self.task_turn_idx,
                }

            # Need an initial human's observe to observe the two choices from the bot
            self.agent.observe({'text': '', 'task_data': task_data})

        else:
            raise ValueError(
                f""Conversation start mode {self.opt['conversation_start_mode']} ""
                f""not recognized!""
            )"
559,"    def assertDetailsProvided(self, case, expected_outcome, expected_keys):
        """"""Assert that when case is run, details are provided to the result.

        :param case: A TestCase to run.
        :param expected_outcome: The call that should be made.
        :param expected_keys: The keys to look for.
        """"""","        result = ExtendedTestResult()
        case.run(result)
        case = result._events[0][1]
        expected = [
            ('startTest', case),
            (expected_outcome, case),
            ('stopTest', case),
            ]
        self.assertEqual(3, len(result._events))
        self.assertEqual(expected[0], result._events[0])
        self.assertEqual(expected[1], result._events[1][0:2])
        # Checking the TB is right is rather tricky. doctest line matching
        # would help, but 'meh'.
        self.assertEqual(sorted(expected_keys),
            sorted(result._events[1][2].keys()))
        self.assertEqual(expected[-1], result._events[-1])"
560,"    def appCleanExit(self):
        """"""Override to perform cleanup when application exits without error.""""""",        self.service.shutdown()
561,"def _make_attr_tuple_class(cls_name, attr_names):
    """"""
    Create a tuple subclass to hold `Attribute`s for an `attrs` class.

    The subclass is a bare tuple with properties for names.

    class MyClassAttributes(tuple):
        __slots__ = ()
        x = property(itemgetter(0))
    """"""","    attr_class_name = ""{}Attributes"".format(cls_name)
    attr_class_template = [
        ""class {}(tuple):"".format(attr_class_name),
        ""    __slots__ = ()"",
    ]
    if attr_names:
        for i, attr_name in enumerate(attr_names):
            attr_class_template.append(
                _tuple_property_pat.format(index=i, attr_name=attr_name)
            )
    else:
        attr_class_template.append(""    pass"")
    globs = {""itemgetter"": itemgetter}
    eval(compile(""\n"".join(attr_class_template), """", ""exec""), globs)
    return globs[attr_class_name]"
562,"    def get_available_image_mode_names(self):
        """"""
        Available image model names.

        resnet and resnext variants available from the ImageLoader. resnext101_XXXXX_wsl
        is the open-sourced FB AI model (960m images, 1.5k hashtags, finetuned on
        ImageNet).
        """"""","        available_model_names = ImageLoader.get_available_model_names()
        return ['no_image_model', 'raw', 'ascii'] + available_model_names"
563,"    def build(cls, decl, defn):
        """"""Parse an alias declaration and definition into an alias object""""""","        repl = efmt = None
        name, args, err = cls._builddecl(decl)
        if err:
            efmt = _('bad declaration of %(section)s ""%(name)s"": %(error)s')
        else:
            try:
                repl = cls._builddefn(defn, args)
            except error.ParseError as inst:
                err = parseerrordetail(inst)
                efmt = _('bad definition of %(section)s ""%(name)s"": %(error)s')
        if err:
            err = efmt % {""section"": cls._section, ""name"": name, ""error"": err}
        return alias(name, args, err, repl)"
564,"def unshelve(ui, repo, *shelved, **opts):
    """"""restore a shelved change to the working copy

    This command accepts an optional name of a shelved change to
    restore. If none is given, the most recent shelved change is used.

    If a shelved change is applied successfully, the bundle that
    contains the shelved changes is moved to a backup location
    (.@prog@/shelve-backup).

    Since you can restore a shelved change on top of an arbitrary
    commit, it is possible that unshelving will result in a conflict. If
    this occurs, you must resolve the conflict, then use ``--continue``
    to complete the unshelve operation. The bundle will not be moved
    until you successfully complete the unshelve.

    Alternatively, you can use ``--abort`` to cancel the conflict
    resolution and undo the unshelve, leaving the shelve bundle intact.

    After a successful unshelve, the shelved changes are stored in a
    backup directory. Only the N most recent backups are kept. N
    defaults to 10 but can be overridden using the ``shelve.maxbackups``
    configuration option.

    .. container:: verbose

       Timestamp in seconds is used to decide the order of backups. More
       than ``maxbackups`` backups are kept if same timestamp prevents
       from deciding exact order of them, for safety.

    Returns 0 on success.
    """"""","    with repo.wlock():
        return _dounshelve(ui, repo, *shelved, **opts)"
565,"def CheckMakePairUsesDeduction(filename, clean_lines, linenum, error):
  """"""Check that make_pair's template arguments are deduced.

  G++ 4.6 in C++0x mode fails badly if make_pair's template arguments are
  specified explicitly, and such use isn't intended in any case.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    error: The function to call with any errors found.
  """"""","  line = clean_lines.elided[linenum]
  match = _RE_PATTERN_EXPLICIT_MAKEPAIR.search(line)
  if match:
    error(filename, linenum, 'build/explicit_make_pair',
          4,  # 4 = high confidence
          'For C++11-compatibility, omit template arguments from make_pair'
          ' OR use pair directly OR if appropriate, construct a pair directly')"
566,"def notloaded():
    """"""return short names of extensions that failed to load""""""","    return [name for name, mod in pycompat.iteritems(_extensions) if mod is None]"
567,"    def __init__(self, stream):
        """"""Create a TestResultStats which outputs to stream.""""""","        testresult.TestResult.__init__(self)
        self._stream = stream
        self.failed_tests = 0
        self.skipped_tests = 0
        self.seen_tags = set()"
568,"def get_task_ids_in_tier(tier_name):
    """"""Returns a list of all task_ids in iter.""""""","    # dict of dicts: template_id -> task_id -> tier.
    template_task_tiers = collections.defaultdict(dict)
    for task_id, task in phyre.loader.load_compiled_task_dict().items():
        template_id = task_id.split(':')[0]
        template_task_tiers[template_id][task_id] = task.tier

    selected_task_ids = set()
    tier_name = tier_name.upper()
    for template_id, task_to_tier in template_task_tiers.items():
        tiers = frozenset(task_to_tier.values())
        if len(tiers) == 1 and next(iter(tiers)) == tier_name:
            selected_task_ids.update(task_to_tier)
    return sorted(selected_task_ids)"
569,"def grouper(iterable, n, fillvalue=None):
    """"""
    Collect data into fixed-length chunks or blocks.

    From https://docs.python.org/3/library/itertools.html#itertools-recipes
    """"""","    from itertools import zip_longest

    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx""
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)"
570,"    def seek(self, offset: int, whence: int = 0) -> int:
        """"""
        Change stream position.

        Change the stream position to byte offset offset. Argument offset is
        interpreted relative to the position indicated by whence.  Values
        for whence are ints:

        * 0 -- start of stream (the default); offset should be zero or positive
        * 1 -- current stream position; offset may be negative
        * 2 -- end of stream; offset is usually negative
        Some operating systems / file systems could provide additional values.

        Return an int indicating the new absolute position.
        """"""","        if whence == 0:
            assert offset >= 0
            self.offset = offset
        elif whence == 1:
            assert offset + self.offset >= 0
            self.offset += offset
        elif whence == 2:
            self.offset = self.length + offset
        return self.offset"
571,"    def count_matrices(self):
        """"""Count of matrices required""""""",        return sum(self) - int(self.do_linear_layer)
572,"def showcurrentbookmark(**args):
    """"""String. The active bookmark, if it is associated with the changeset.
    (DEPRECATED)""""""",    return showactivebookmark(**args)
573,"    def parse(cls, src, dist=None):
        """"""Parse a single entry point from string `src`

        Entry point syntax follows the form::

            name = some.module:some.attr [extra1, extra2]

        The entry name and module name are required, but the ``:attrs`` and
        ``[extras]`` parts are optional
        """"""","        m = cls.pattern.match(src)
        if not m:
            msg = ""EntryPoint must be in 'name=module:attrs [extras]' format""
            raise ValueError(msg, src)
        res = m.groupdict()
        extras = cls._parse_extras(res['extras'])
        attrs = res['attr'].split('.') if res['attr'] else ()
        return cls(res['name'], res['module'], attrs, extras, dist)"
574,"def load_task_scripts_from_folder(
        task_folder=str(phyre.settings.TASK_SCRIPTS_DIR),
        template_id_list=None) -> Sequence[Tuple[str, str, TaskScript]]:
    """"""Loads task builders from the folder.

    Args:
        task_folder: The task folder is expected to contain files with names
          line taskXXX.py, where XXX is an arbitrary string id. The files are
          expected to provide build_task function that returns a Task object.
        template_id_list: None or a list of template ids to load. Task scripts
          outside of the list will not be loaded.

    Returns:
        List of tuples (template_id, task_path, task_script_module).
    """"""","    if not os.path.exists(task_folder):
        raise RuntimeError(f'Cannot find task folder: {task_folder}')
    # We need some fake uniq module name to mount the modules in.
    path_slug = re.sub('[^a-zA-Z0-9]', '_',
                       os.path.realpath(task_folder)).strip('_')
    tasks = []
    for fname in sorted(os.listdir(task_folder)):
        if not fname.startswith('task') or not fname.endswith('.py'):
            continue
        template_id = fname[4:-3]
        if (template_id_list is not None and
                template_id not in template_id_list):
            continue
        fpath = os.path.join(task_folder, fname)
        spec = importlib.util.spec_from_file_location(
            f'{path_slug}.task{template_id}', fpath)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        if not hasattr(module, 'build_task'):
            raise RuntimeError(f'Loaded {fname} from {task_folder}, but'
                               ' haven\'t found ""build_task"" method')
        tasks.append((template_id, fpath, module))
    return tasks"
575,"    def get_tokenizer(self, opt):
        """"""
        Instantiate tokenizer.
        """"""","        model_sz = opt[""gpt2_size""]
        fle_key = f""microsoft/DialoGPT-{model_sz}""
        return GPT2Tokenizer.from_pretrained(fle_key)"
576,"def get_partial_cache_folder(action_size: int) -> pathlib.Path:
    """"""Path to the partial cache files.""""""",    return _get_root_cache_folder() / 'partial' / str(action_size)
577,"    def between(self, pairs):
        """"""Obtain nodes between pairs of nodes.

        ``pairs`` is an iterable of node pairs.

        Returns an iterable of iterables of nodes corresponding to each
        requested pair.
        """"""",
578,"    def complete(self):
        """"""Returns true if this response is completely loaded.

        Note that if this is a connection where complete means the
        socket is closed, this will nearly always return False, even
        in cases where all the data has actually been loaded.
        """"""","        if self._reader:
            return self._reader.done()"
579,"  def IncrementErrorCount(self, category):
    """"""Bumps the module's error statistic.""""""","    self.error_count += 1
    if self.counting in ('toplevel', 'detailed'):
      if self.counting != 'detailed':
        category = category.split('/')[0]
      if category not in self.errors_by_category:
        self.errors_by_category[category] = 0
      self.errors_by_category[category] += 1"
580,"    def _getalias(cls, aliases, tree):
        """"""If tree looks like an unexpanded alias, return (alias, pattern-args)
        pair. Return None otherwise.
        """"""","        if not isinstance(tree, tuple):
            return None
        if tree[0] == cls._symbolnode:
            name = tree[1]
            a = aliases.get(name)
            if a and a.args is None:
                return a, None
        func = cls._trygetfunc(tree)
        if func:
            name, args = func
            a = aliases.get(name)
            if a and a.args is not None:
                return a, args
        return None"
581,"    def tonodes(self, revs):
        """"""Convert an IdSet to Set. The reverse of torevs.""""""","        # translate fullreposet to dag.all() that preserves the 'full' hint.
        if isinstance(revs, smartset.fullreposet):
            return self.dag.all()
        # 'idset' has a fast path - pass the Rust-binding 'spans' directly.
        if isinstance(revs, smartset.idset):
            return self.inner.tonodes(revs._spans)
        # 'nameset' has a fast path - it contains the Rust nameset that uses
        # nodes directly.
        if isinstance(revs, smartset.nameset):
            return revs._set
        return self.inner.tonodes(revs)"
582,"    def clientBegin(self, iprot, oprot):
        """"""Deprecated: Called when a new connection is made to the server.

        For all servers other than TNonblockingServer, this function is called
        whenever newConnection is called and vice versa.  This is the old-style
        for event handling and is not supported for TNonblockingServer. New
        code should always use the newConnection method.
        """"""",        pass
583,"    def filefoldmap(self):
        """"""Returns a dictionary mapping normalized case paths to their
        non-normalized versions.
        """"""","
        def lookup(key):
            f = self.getcasefoldedtracked(key, util.normcase)
            if f is not None and self._rmap.hastrackedfile(f):
                return f
            else:
                return None

        return treestate._overlaydict(lookup)"
584,"    def replacement_train(self, x):
        """"""Trains the index on a representative set of vectors.
        The index must be trained before vectors can be added to it.

        Parameters
        ----------
        x : array_like
            Query vectors, shape (n, d) where d is appropriate for the index.
            `dtype` must be float32.
        """"""","        n, d = x.shape
        assert d == self.d
        x = np.ascontiguousarray(x, dtype='float32')
        self.train_c(n, swig_ptr(x))"
585,"    def rollbackworkspace(self, reponame, workspace, version):
        """"""Rollback the given workspace to a specific version""""""",
586,"    def symlink(self, path: str, contents: str, add: bool = True) -> None:
        """"""
        Create a symlink at the specified path, pointed at the given
        destination path contents.
        """"""","        self.make_parent_dir(path)
        full_path = self.get_path(path)
        try:
            os.unlink(full_path)
        except OSError as ex:
            if ex.errno != errno.ENOENT:
                raise

        os.symlink(contents, full_path)
        if add:
            self.add_file(path)"
587,"    def shortstr(self):
        """"""represent opts in a short string, suitable for a directory name""""""","        result = """"
        if not self.followrename:
            result += ""r0""
        if not self.followmerge:
            result += ""m0""
        if self.diffopts is not None:
            assert isinstance(self.diffopts, mdiff.diffopts)
            diffopthash = hashdiffopts(self.diffopts)
            if diffopthash != _defaultdiffopthash:
                result += ""i"" + diffopthash
        return result or ""default"""
588,"    def set_extraction_path(self, path):
        """"""Set the base path where resources will be extracted to, if needed.

        If you do not call this routine before any extractions take place, the
        path defaults to the return value of ``get_default_cache()``.  (Which
        is based on the ``PYTHON_EGG_CACHE`` environment variable, with various
        platform-specific fallbacks.  See that routine's documentation for more
        details.)

        Resources are extracted to subdirectories of this path based upon
        information given by the ``IResourceProvider``.  You may set this to a
        temporary directory, but then you must call ``cleanup_resources()`` to
        delete the extracted files when done.  There is no guarantee that
        ``cleanup_resources()`` will be able to remove all extracted files.

        (Note: you may not change the extraction path for a given resource
        manager once resources have been extracted, unless you first call
        ``cleanup_resources()``.)
        """"""","        if self.cached_files:
            raise ValueError(
                ""Can't change extraction path, files already extracted""
            )

        self.extraction_path = path"
589,"    def _get_attention_losses(
        self, fwd_pass: ForwardPassOutputs
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """"""
        Return attention losses.

        Compute and return losses on encoder and decoder self-attention and decoder
        enc/dec attention.
        """"""","        enc_self_attn_loss = self._get_and_record_component_attention_loss(
            student_attention_matrices=fwd_pass.student_attention_matrices['encoder'],
            teacher_attention_matrices=fwd_pass.teacher_attention_matrices['encoder'],
            mask=fwd_pass.context_mask,
            tokens_per_example=fwd_pass.context_tokens_per_example,
            num_tokens=fwd_pass.num_context_tokens,
            mapped_layers=self.mapped_enc_layers,
            attn_type='self_attn',
            metric_name='enc_self_attn_loss',
        )
        dec_self_attn_loss = self._get_and_record_component_attention_loss(
            student_attention_matrices=fwd_pass.student_attention_matrices['decoder'],
            teacher_attention_matrices=fwd_pass.teacher_attention_matrices['decoder'],
            mask=fwd_pass.decoder_mask,
            tokens_per_example=fwd_pass.tokens_per_example,
            num_tokens=fwd_pass.num_tokens,
            mapped_layers=self.mapped_dec_layers,
            attn_type='self_attn',
            metric_name='dec_self_attn_loss',
        )
        enc_dec_attn_loss = self._get_and_record_component_attention_loss(
            student_attention_matrices=fwd_pass.student_attention_matrices['decoder'],
            teacher_attention_matrices=fwd_pass.teacher_attention_matrices['decoder'],
            mask=fwd_pass.decoder_mask,
            tokens_per_example=fwd_pass.tokens_per_example,
            num_tokens=fwd_pass.num_tokens,
            mapped_layers=self.mapped_dec_layers,
            attn_type='encoder_attn',
            metric_name='enc_dec_attn_loss',
        )
        return enc_self_attn_loss, dec_self_attn_loss, enc_dec_attn_loss"
590,"def index_cpu_to_gpus_list(index, co=None, gpus=None, ngpu=-1):
    """""" Here we can pass list of GPU ids as a parameter or ngpu to
    use first n GPU's. gpus mut be a list or None""""""","    if (gpus is None) and (ngpu == -1):  # All blank
        gpus = range(get_num_gpus())
    elif (gpus is None) and (ngpu != -1):  # Get number of GPU's only
        gpus = range(ngpu)
    res = [StandardGpuResources() for _ in gpus]
    index_gpu = index_cpu_to_gpu_multiple_py(res, index, co, gpus)
    return index_gpu"
591,"def find_nodes_in_packages(graph, pkg_prefixes):
    """"""
    Find all nodes that fall under the list of :pkg_prefixes.
    """"""","    nodes = set()
    for node in list(graph.nodes.values()):
        for pkg_prefix in pkg_prefixes:
            # If we have an array, use its base type
            base_type = node.name.lstrip(""["")
            if base_type.startswith(pkg_prefix):
                nodes.add(node)
    return nodes"
592,"def setup_args():
    """"""
    Set up conversion args.
    """"""","    parser = ParlaiParser()
    parser.add_argument(
        '-n',
        '--num-episodes',
        default=-1,
        type=int,
        help='Total number of episodes to convert, -1 to convert all examples',
    )
    parser.add_argument(
        '-of',
        '--outfile',
        default=None,
        type=str,
        help='Output file where to save, by default will be created in /tmp',
    )
    parser.add_argument(
        '-s1id', '--speaker-0-id', type=str, help='Speaker id of agent who speaks first'
    )
    parser.add_argument(
        '-s1id',
        '--speaker-1-id',
        type=str,
        help='Speaker id of agent who speaks second',
    )
    parser.add_argument(
        '--prepended-context',
        type='bool',
        default=False,
        help='specify if the context is prepended to the first act',
    )
    parser.add_argument('-ltim', '--log-every-n-secs', type=float, default=10)
    parser.set_defaults(datatype='train:ordered')

    return parser"
593,"def expaths(orig, ui, repo, *args, **opts):
    """"""allow adding and removing remote paths

    This is very hacky and only exists as an experimentation.

    """"""","    delete = opts.get(""delete"")
    add = opts.get(""add"")
    configrepofile = ui.identity.configrepofile()
    if delete:
        # find the first section and remote path that matches, and delete that
        foundpaths = False
        if not repo.localvfs.isfile(configrepofile):
            raise error.Abort(_(""could not find repo config file""))
        oldrepoconfigfile = repo.localvfs.readutf8(configrepofile).splitlines(True)
        f = repo.localvfs(configrepofile, ""w"", atomictemp=True)
        for line in oldrepoconfigfile:
            if ""[paths]"" in line:
                foundpaths = True
            if not (foundpaths and line.strip().startswith(delete)):
                f.writeutf8(line)
        f.close()
        saveremotenames(repo, {delete: {}})
        precachedistance(repo)
        return

    if add:
        # find the first section that matches, then look for previous value; if
        # not found add a new entry
        foundpaths = False
        oldrepoconfigfile = []
        if repo.localvfs.isfile(configrepofile):
            oldrepoconfigfile = repo.localvfs.readutf8(configrepofile).splitlines(True)
        f = repo.localvfs(configrepofile, ""w"", atomictemp=True)
        done = False
        for line in oldrepoconfigfile:
            if ""[paths]"" in line:
                foundpaths = True
            if foundpaths and line.strip().startswith(add):
                done = True
                line = ""%s = %s\n"" % (add, args[0])
            f.writeutf8(line)

        # did we not find an existing path?
        if not done:
            done = True
            f.writeutf8(""[paths]\n"")
            f.writeutf8(""%s = %s\n"" % (add, args[0]))

        f.close()
        return

    return orig(ui, repo, *args)"
594,"    def _single_gpu_build_func(model):
        """"""Build the model on a single GPU. Can be called in a loop over GPUs
        with name and device scoping to create a data parallel model.
        """"""","        # Add the conv body (called ""backbone architecture"" in papers)
        # E.g., ResNet-50, ResNet-50-FPN, ResNeXt-101-FPN, etc.
        blob_conv, dim_conv, spatial_scale_conv = add_conv_body_func(model)
        if freeze_conv_body:
            for b in c2_utils.BlobReferenceList(blob_conv):
                model.StopGradient(b, b)

        if not model.train:  # == inference
            # Create a net that can be used to execute the conv body on an image
            # (without also executing RPN or any other network heads)
            model.conv_body_net = model.net.Clone('conv_body_net')

        head_loss_gradients = {
            'rpn': None,
            'box': None,
            'mask': None,
            'keypoints': None,
        }

        if cfg.RPN.RPN_ON:
            # Add the RPN head
            head_loss_gradients['rpn'] = rpn_heads.add_generic_rpn_outputs(
                model, blob_conv, dim_conv, spatial_scale_conv
            )

        if cfg.FPN.FPN_ON:
            # After adding the RPN head, restrict FPN blobs and scales to
            # those used in the RoI heads
            blob_conv, spatial_scale_conv = _narrow_to_fpn_roi_levels(
                blob_conv, spatial_scale_conv
            )

        if not cfg.MODEL.RPN_ONLY:
            # Add the Fast R-CNN head
            head_loss_gradients['box'] = _add_fast_rcnn_head(
                model, add_roi_box_head_func, blob_conv, dim_conv,
                spatial_scale_conv
            )

        if cfg.MODEL.MASK_ON:
            # Add the mask head
            head_loss_gradients['mask'] = _add_roi_mask_head(
                model, add_roi_mask_head_func, blob_conv, dim_conv,
                spatial_scale_conv
            )

        if cfg.MODEL.KEYPOINTS_ON:
            # Add the keypoint head
            head_loss_gradients['keypoint'] = _add_roi_keypoint_head(
                model, add_roi_keypoint_head_func, blob_conv, dim_conv,
                spatial_scale_conv
            )

        if model.train:
            loss_gradients = {}
            for lg in head_loss_gradients.values():
                if lg is not None:
                    loss_gradients.update(lg)
            return loss_gradients
        else:
            return None"
595,"    def register(self, dataset_type: DatasetType, factory: Callable[[Metadata, CfgNode], Dataset]):
        """"""
        Args:
            dataset_type (DatasetType): a DatasetType e.g. DatasetType.VIDEO_LIST
            factory (Callable[Metadata, CfgNode]): a callable which takes Metadata and cfg
            arguments and returns a dataset object.
        """"""","        assert dataset_type not in self, ""Dataset '{}' is already registered!"".format(dataset_type)
        self[dataset_type] = factory"
596,"def walkchangerevs(repo, match, opts, prepare):
    """"""Iterate over files and the revs in which they changed.

    Callers most commonly need to iterate backwards over the history
    in which they are interested. Doing so has awful (quadratic-looking)
    performance, so we use iterators in a ""windowed"" way.

    We walk a window of revisions in the desired order.  Within the
    window, we first walk forwards to gather data, then in the desired
    order (usually backwards) to display it.

    This function returns an iterator yielding contexts. Before
    yielding each context, the iterator will first call the prepare
    function on each context in the window in forward order.""""""","
    follow = opts.get(""follow"") or opts.get(""follow_first"")
    revs = _logrevs(repo, opts)
    if not revs:
        return []
    wanted = set()
    slowpath = match.anypats() or (
        (match.isexact() or match.prefix()) and opts.get(""removed"")
    )
    fncache = {}
    change = repo.changectx

    # First step is to fill wanted, the set of revisions that we want to yield.
    # When it does not induce extra cost, we also fill fncache for revisions in
    # wanted: a cache of filenames that were changed (ctx.files()) and that
    # match the file filtering conditions.

    if match.always():
        # No files, no patterns.  Display all revs.
        wanted = revs
    elif not slowpath:
        # We only have to read through the filelog to find wanted revisions

        try:
            wanted = walkfilerevs(repo, match, follow, revs, fncache)
        except FileWalkError:
            slowpath = True

            # We decided to fall back to the slowpath because at least one
            # of the paths was not a file. Check to see if at least one of them
            # existed in history, otherwise simply return
            for path in match.files():
                if path == ""."" or path in repo.store:
                    break
            else:
                return []

    if slowpath:
        # We have to read the changelog to match filenames against
        # changed files

        if follow:
            raise error.Abort(
                _(""can only follow copies/renames for explicit "" ""filenames"")
            )

        # The slow path checks files modified in every changeset.
        # This is really slow on large repos, so compute the set lazily.
        class lazywantedset(object):
            def __init__(self):
                self.set = set()
                self.revs = set(revs)

            # No need to worry about locality here because it will be accessed
            # in the same order as the increasing window below.
            def __contains__(self, value):
                if value in self.set:
                    return True
                elif not value in self.revs:
                    return False
                else:
                    self.revs.discard(value)
                    ctx = change(value)
                    matches = filter(match, ctx.files())
                    if matches:
                        fncache[value] = matches
                        self.set.add(value)
                        return True
                    return False

            def discard(self, value):
                self.revs.discard(value)
                self.set.discard(value)

        wanted = lazywantedset()

    # it might be worthwhile to do this in the iterator if the rev range
    # is descending and the prune args are all within that range
    for rev in opts.get(""prune"", ()):
        rev = repo[rev].rev()
        ff = _followfilter(repo)
        stop = min(revs[0], revs[-1])
        for x in range(rev, stop - 1, -1):
            if ff.match(x):
                wanted = wanted - [x]

    # Now that wanted is correctly initialized, we can iterate over the
    # revision range, yielding only revisions in wanted.
    def iterate():
        if follow and match.always():
            ff = _followfilter(repo, onlyfirst=opts.get(""follow_first""))

            def want(rev):
                return ff.match(rev) and rev in wanted

        else:

            def want(rev):
                return rev in wanted

        it = iter(revs)
        stopiteration = False
        for windowsize in increasingwindows():
            nrevs = []
            for i in range(windowsize):
                rev = next(it, None)
                if rev is None:
                    stopiteration = True
                    break
                elif want(rev):
                    nrevs.append(rev)
            for rev in sorted(nrevs):
                fns = fncache.get(rev)
                ctx = change(rev)
                if not fns:

                    def fns_generator():
                        for f in ctx.files():
                            if match(f):
                                yield f

                    fns = fns_generator()
                prepare(ctx, fns)
            for rev in nrevs:
                yield change(rev)

            if stopiteration:
                break

    return iterate()"
597,"def parseerrordetail(inst):
    """"""Compose error message from specified ParseError object""""""","    if len(inst.args) > 1:
        return _(""at %d: %s"") % (inst.args[1], inst.args[0])
    else:
        return inst.args[0]"
598,"def pushed(repo, subset, x):
    """"""Select changesets in any remote repository according to remotenames.""""""","    revset.getargs(x, 0, 0, ""pushed takes no arguments"")
    return upstream_revs(lambda x: True, repo, subset, x)"
599,"def advanceboundary(repo, tr, targetphase, nodes) -> None:
    """"""Add nodes to a phase changing other nodes phases if necessary.

    This function move boundary *forward* this means that all nodes
    are set in the target phase or kept in a *lower* phase.

    Simplify boundary to contains phase roots only.""""""","    phcache = repo._phasecache.copy()
    phcache.advanceboundary(repo, tr, targetphase, nodes)
    repo._phasecache.replace(phcache)"
600,"def render_node(node):
    """"""
    Create a HTML string representing a link. When clicked, it will execute
    code to display the details of :node.
    """"""","    name = html.escape(node.name)
    link = phabricator_link(node)
    if link is None:
        return ""<pre>{}</pre>"".format(name)
    return '<pre><a href=""javascript:print_node(\'{0}\')"">{0}</a> <a href=""{1}"">[phab]</a></pre>'.format(
        name, link
    )"
601,"    def init_contexts(self, shared=None):
        """"""
        Override to load or instantiate contexts to be used to seed the chat.
        """"""",        pass
602,"def push(repo, namespace, key, old, new):
    """"""should succeed iff value was old""""""","    pk = _get(namespace)[0]
    return pk(repo, key, old, new)"
603,"    def parse(self, text):
        """"""
        Convert string to token indices.
        """"""","        vec = self.dict.txt2vec(text)
        if vec == []:
            vec = [self.dict[self.dict.null_token]]
        return vec"
604,"def CheckIncludeLine(filename, clean_lines, linenum, include_state, error):
  """"""Check rules that are applicable to #include lines.

  Strings on #include lines are NOT removed from elided line, to make
  certain tasks easier. However, to prevent false positives, checks
  applicable to #include lines in CheckLanguage must be put here.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    include_state: An _IncludeState instance in which the headers are inserted.
    error: The function to call with any errors found.
  """"""","  fileinfo = FileInfo(filename)

  line = clean_lines.lines[linenum]

  # ""include"" should use the new style ""foo/bar.h"" instead of just ""bar.h""
  if _RE_PATTERN_INCLUDE_NEW_STYLE.search(line):
    error(filename, linenum, 'build/include', 4,
          'Include the directory when naming .h files')

  # we shouldn't include a file more than once. actually, there are a
  # handful of instances where doing so is okay, but in general it's
  # not.
  match = _RE_PATTERN_INCLUDE.search(line)
  if match:
    include = match.group(2)
    is_system = (match.group(1) == '<')
    if include in include_state:
      error(filename, linenum, 'build/include', 4,
            '""%s"" already included at %s:%s' %
            (include, filename, include_state[include]))
    else:
      include_state[include] = linenum

      # We want to ensure that headers appear in the right order:
      # 1) for foo.cc, foo.h  (preferred location)
      # 2) c system files
      # 3) cpp system files
      # 4) for foo.cc, foo.h  (deprecated location)
      # 5) other google headers
      #
      # We classify each include statement as one of those 5 types
      # using a number of techniques. The include_state object keeps
      # track of the highest type seen, and complains if we see a
      # lower type after that.
      error_message = include_state.CheckNextIncludeOrder(
          _ClassifyInclude(fileinfo, include, is_system))
      if error_message:
        error(filename, linenum, 'build/include_order', 4,
              '%s. Should be: %s.h, c system, c++ system, other.' %
              (error_message, fileinfo.BaseName()))
      if not include_state.IsInAlphabeticalOrder(include):
        error(filename, linenum, 'build/include_alpha', 4,
              'Include ""%s"" not in alphabetical order' % include)

  # Look for any of the stream classes that are part of standard C++.
  match = _RE_PATTERN_INCLUDE.match(line)
  if match:
    include = match.group(2)
    if Match(r'(f|ind|io|i|o|parse|pf|stdio|str|)?stream$', include):
      # Many unit tests use cout, so we exempt them.
      if not _IsTestFilename(filename):
        error(filename, linenum, 'readability/streams', 3,
              'Streams are highly discouraged.')"
605,"    def _set_text_vec(self, obs, history, truncate):
        """"""
        Set the 'text_vec' field in the observation.

        Overridden to include both local utterance (text_vec) and full history
        (context_vec)
        """"""","        if ""text"" not in obs:
            return obs

        if ""text_vec"" not in obs:
            # text vec is not precomputed, so we set it using the history
            history_string = history.get_history_str()
            # when text not exist, we get text_vec from history string
            # history could be none if it is an image task and 'text'
            # filed is be empty. We don't want this
            if history_string is None:
                return obs
            obs[""full_text""] = history_string
            if history_string:
                history_vec = history.get_history_vec_list()
                obs[""text_vec""] = history_vec[-1]
                obs[""full_text_vec""] = history.get_history_vec()
                obs[""context_vec""] = history_vec

        # check truncation
        if obs.get(""text_vec"") is not None:
            truncated_vec = self._check_truncate(obs[""text_vec""], truncate, True)
            obs.force_set(""text_vec"", torch.LongTensor(truncated_vec))
        return obs"
606,"    def extract_knowledge_graph_str(
        self, state: Dict[str, Any], include_inv_objs: bool = False
    ) -> str:
        """"""
        Generates the string representing knowledge graph.

        It may modifiy the knowledge graph, for example when --prune-knowledge-graph is
        true.
        """"""","
        def has_word_overlap(main_text, content_text_tokens):
            if not main_text or not content_text_tokens:
                return False

            for word in main_text.lower().split():
                if not word or len(word) < 3 or word in consts.GRAPH_VERT_SKIP_TOKEN:
                    continue
                if word in content_text_tokens:
                    return True
            return False

        def keep_edge(edge, text_tokens):
            """"""
            Returns false for edges that can NOT be readily inferred from the context.

            Edges that are referring to objects not referenced in the location
            description, except player (you). Thus, we drop these edges. Also, for some
            teachers (eg, ActionKGTeacher) the model can see user inventory before, and
            after the action, and they are important in result of action,. Thus, we keep
            < you , have, X > edges as well.
            """"""
            # Each graph edge is a tuple: (subject, relation, object)
            sub, rel, obj = [s.strip().lower() for s in edge]

            if sub == 'you':
                # player location
                if rel == 'in':
                    return True
                # user inventory object
                elif rel == 'have' and self._keep_inv_during_kg_prune:
                    return True

            return has_word_overlap(sub, text_tokens) and has_word_overlap(
                obj, text_tokens
            )

        if self._prune_kg:
            # Prunning the knowledge graph for the entities that are mentioned in the description
            old_graph = state['graph']
            new_graph = []

            # `text_desc` is the context that we show the model the generate the knowledge graph from.
            text_desc = f'{state[""location_desc""]} {state[""location_name""]}'
            if include_inv_objs:
                text_desc = f'{text_desc} {state[""inventory_objs""]}'
            text_desc_tokens = text_desc.lower().split()

            # We prune the knowledge graph for items that are partially mentioned in `text_desc`
            for edge in old_graph:
                if keep_edge(edge, text_desc_tokens):
                    new_graph.append(edge)

            state['graph'] = new_graph

        return knowledge_graph_as_str(state['graph'])"
607,"    def add(self, dist):
        """"""Add `dist` if we ``can_add()`` it and it has not already been added
        """"""","        if self.can_add(dist) and dist.has_version():
            dists = self._distmap.setdefault(dist.key, [])
            if dist not in dists:
                dists.append(dist)
                dists.sort(key=operator.attrgetter('hashcmp'), reverse=True)"
608,"def reprflags(flags):
    """"""Turn flags into human-readable string""""""","    return "" "".join(
        name
        for name in (""EXIST_P1"", ""EXIST_P2"", ""EXIST_NEXT"", ""COPIED"", ""NEED_CHECK"")
        if flags & getattr(treestate, name)
    )"
609,"def burn_in_executor(us):
    """"""
    The ""busy wait"" functions are CPU bound; to prevent blocking
    they need to be run in an executor. This function
    provides the implementation.
    """"""","    start = time.time()
    end = start + us_to_sec(us)
    while time.time() < end:
        pass"
610,"    def evaluate(self):
        """"""
        Evaluate/summarize the performance, after processing all input/output pairs.

        Returns:
            dict:
                A new evaluator class can return a dict of arbitrary format
                as long as the user can process the results.
                In our train_net.py, we expect the following format:

                * key: the name of the task (e.g., bbox)
                * value: a dict of {metric name: score}, e.g.: {""AP50"": 80}
        """"""",        pass
611,"def on_same_filesystem(path1, path2):
    """"""Returns True if `path` and `path2` reside on the same mount""""""","    return resolve_partition(path1).mountpoint == \
            resolve_partition(path2).mountpoint"
612,"    def _get_batch_context(self, batch):
        """"""
        Override to always provide full context.
        """"""","        if 'full_text_vec' not in batch:
            logging.warn('Batch does not have full text vec, resorting to text vec')
            return batch.text_vec
        return batch.full_text_vec"
613,"def localhgenv():
    """"""Get an environment dictionary to use for invoking or importing
    mercurial from the local repository.""""""","    # Execute hg out of this directory with a custom environment which takes
    # care to not use any hgrc files and do no localization.
    env = {
        ""HGMODULEPOLICY"": ""py"",
        ""HGRCPATH"": """",
        ""LANGUAGE"": ""C"",
        ""PATH"": """",
    }  # make pypi modules that use os.environ['PATH'] happy
    if ""LD_LIBRARY_PATH"" in os.environ:
        env[""LD_LIBRARY_PATH""] = os.environ[""LD_LIBRARY_PATH""]
    if ""SystemRoot"" in os.environ:
        # SystemRoot is required by Windows to load various DLLs.  See:
        # https://bugs.python.org/issue13524#msg148850
        env[""SystemRoot""] = os.environ[""SystemRoot""]
    return env"
614,"    def upload_fb_attachment(self, payload):
        """"""
        Uploads an attachment using the Attachment Upload API and returns an attachment
        ID.
        """"""","        api_address = f'https://graph.facebook.com/{API_VERSION}/me/message_attachments'
        assert payload['type'] in [
            'image',
            'video',
            'file',
            'audio',
        ], 'unsupported attachment type'
        if 'url' in payload:
            message = {
                ""message"": {
                    ""attachment"": {
                        ""type"": payload['type'],
                        ""payload"": {""is_reusable"": ""true"", ""url"": payload['url']},
                    }
                }
            }
            response = requests.post(api_address, params=self.auth_args, json=message)
        elif 'filename' in payload:
            message = {
                ""attachment"": {
                    ""type"": payload['type'],
                    ""payload"": {""is_reusable"": ""true""},
                }
            }
            with open(payload['filename'], 'rb') as f:
                filedata = {
                    ""filedata"": (
                        payload['filename'],
                        f,
                        payload['type'] + '/' + payload['format'],
                    )
                }
                response = requests.post(
                    api_address,
                    params=self.auth_args,
                    data={""message"": json.dumps(message)},
                    files=filedata,
                )
        result = response.json()
        log_utils.print_and_log(
            logging.INFO,
            '""Facebook response from attachment upload: {}""'.format(result),
        )
        return result"
615,"def debugmutationfromobsmarkers(ui, repo, **opts) -> int:
    """"""convert obsolescence markers to mutation records""""""","    # pyre-fixme[16]: Module `mutation` has no attribute `convertfromobsmarkers`.
    entries, commits, written = mutation.convertfromobsmarkers(repo)
    repo.ui.write(
        _(""wrote %s of %s entries for %s commits\n"") % (written, entries, commits)
    )
    return 0"
616,"    def check_episode_end_call(
        self, env, previous_observation, action, observation
    ) -> bool:
        """"""Check if the task has ended, and accumulate any reward from the
        transition in ``self._reward``.

        Args:
            env (MiniHack):
                The MiniHack environment in question.
            previous_observation (tuple):
                The previous state observation.
            action (int):
                The action taken.
            observation (tuple):
                The current observation.
        Returns:
            bool: Boolean whether the episode has ended.

        """"""",        raise NotImplementedError
617,"    def commitpending(self):
        """"""Used in alternative filelog implementations to commit pending
        additions.""""""","        if eagerepo.iseagerepo(self.repo):
            self.contentstore.flush()"
618,"def extract_result(deferred):
    """"""Extract the result from a fired deferred.

    It can happen that you have an API that returns Deferreds for
    compatibility with Twisted code, but is in fact synchronous, i.e. the
    Deferreds it returns have always fired by the time it returns.  In this
    case, you can use this function to convert the result back into the usual
    form for a synchronous API, i.e. the result itself or a raised exception.

    It would be very bad form to use this as some way of checking if a
    Deferred has fired.
    """"""","    failures = []
    successes = []
    deferred.addCallbacks(successes.append, failures.append)
    if len(failures) == 1:
        failures[0].raiseException()
    elif len(successes) == 1:
        return successes[0]
    else:
        raise DeferredNotFired(""%r has not fired yet."" % (deferred,))"
619,"def get_word_stats(text, agent_dict, bins=(0, 100, 1000, 100000)):
    """"""
    Function which takes text sequence and dict, returns word freq and length
    statistics.

    :param sequence: text sequence
    :param agent_dict: can be external dict or dict from the model
    :param bins: list with range boundaries
    :return: freqs dictionary, num words, avg word length, avg char length
    """"""","    pred_list = agent_dict.tokenize(text)
    pred_freq = [agent_dict.freq[word] for word in pred_list]
    freqs = {i: 0 for i in bins}
    for f in pred_freq:
        for b in bins:
            if f <= b:
                freqs[b] += 1
                break

    wlength = len(pred_list)
    clength = len(text)  # including spaces
    return freqs, len(pred_freq), wlength, clength"
620,"    def re_tokenize(text):
        r""""""
        Tokenize using a liberal regular expression.

        Find boundaries between word characters, newlines, and non-word
        non-whitespace tokens ``(r'[\\w\\n]+ | [^\\w\\s] | \\n')``.

        This splits along whitespace and punctuation and keeps the newline as
        a token in the returned list.
        """"""",        return RETOK.findall(text)
621,"    def run_on_image(self, image):
        """"""
        Args:
            image (np.ndarray): an image of shape (H, W, C) (in BGR order).
                This is the format used by OpenCV.

        Returns:
            predictions (dict): the output of the model.
            vis_output (VisImage): the visualized image output.
        """"""","        vis_output = None
        predictions = self.predictor(image)
        # Convert image from OpenCV BGR format to Matplotlib RGB format.
        image = image[:, :, ::-1]
        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)
        if ""panoptic_seg"" in predictions:
            panoptic_seg, segments_info = predictions[""panoptic_seg""]
            vis_output = visualizer.draw_panoptic_seg_predictions(
                panoptic_seg.to(self.cpu_device), segments_info
            )
        else:
            if ""sem_seg"" in predictions:
                vis_output = visualizer.draw_sem_seg(
                    predictions[""sem_seg""].argmax(dim=0).to(self.cpu_device)
                )
            if ""instances"" in predictions:
                instances = predictions[""instances""].to(self.cpu_device)
                vis_output = visualizer.draw_instance_predictions(predictions=instances)

        return predictions, vis_output"
622,"    def changegroup(self, nodes, kind):
        """"""Obtain a changegroup with data for descendants of specified nodes.""""""",
623,"    def after_step(self):
        """"""
        Called after each iteration.
        """"""",        pass
624,"    def unresolved(self):
        """"""Obtain the paths of unresolved files.""""""","
        for f, entry in pycompat.iteritems(self._state):
            if entry[0] in (""u"", ""pu""):
                yield f"
625,"def _interestingfiles(repo, matcher):
    """"""Walk dirstate with matcher, looking for files that addremove would care
    about.

    This is different from dirstate.status because it doesn't care about
    whether files are modified or clean.""""""","    removed, forgotten = [], []
    audit_path = pathutil.pathauditor(repo.root, cached=True)

    dirstate = repo.dirstate
    exists = repo.wvfs.isfileorlink
    status = dirstate.status(matcher, False, False, True)

    unknown = [file for file in status.unknown if audit_path.check(file)]

    for file in status.removed:
        # audit here to make sure ""file"" hasn't reappeared behind a symlink
        if exists(file) and audit_path.check(file):
            if dirstate.normalize(file) == file:
                forgotten.append(file)
            else:
                removed.append(file)
        else:
            removed.append(file)

    # The user may have specified ignored files. It's expensive to compute them
    # via status, so let's manually add them here.
    ignored = repo.dirstate._ignore
    unknown.extend(
        file
        for file in matcher.files()
        if ignored(file) and repo.wvfs.isfileorlink(file) and audit_path.check(file)
    )

    return status.added, unknown, status.deleted, removed, forgotten"
626,"def get_build_platform():
    """"""Return this platform's string for platform-specific distributions

    XXX Currently this is the same as ``distutils.util.get_platform()``, but it
    needs some hacks for Linux and Mac OS X.
    """"""","    try:
        # Python 2.7 or >=3.2
        from sysconfig import get_platform
    except ImportError:
        from distutils.util import get_platform

    plat = get_platform()
    if sys.platform == ""darwin"" and not plat.startswith('macosx-'):
        try:
            version = _macosx_vers()
            machine = os.uname()[4].replace("" "", ""_"")
            return ""macosx-%d.%d-%s"" % (int(version[0]), int(version[1]),
                _macosx_arch(machine))
        except ValueError:
            # if someone is running a non-Mac darwin system, this will fall
            # through to the default implementation
            pass
    return plat"
627,"    def to(self, device: torch.device):
        """"""
        Transfers all tensors to the given device
        """"""","        coarse_segm = self.coarse_segm.to(device)
        embedding = self.embedding.to(device)
        return DensePoseEmbeddingPredictorOutput(coarse_segm=coarse_segm, embedding=embedding)"
628,"def range_header_to_tuple(range_header):
    """"""Get a (firstbyte,lastbyte) tuple from a Range header value.

    Range headers have the form ""bytes=<firstbyte>-<lastbyte>"". This
    function pulls the firstbyte and lastbyte values and returns
    a (firstbyte,lastbyte) tuple. If lastbyte is not specified in
    the header value, it is returned as an empty string in the
    tuple.

    Return None if range_header is None
    Return () if range_header does not conform to the range spec
    pattern.

    """"""","    global _rangere
    if range_header is None:
        return None
    if _rangere is None:
        _rangere = re.compile(r""^bytes=(\d{1,})-(\d*)"")
    match = _rangere.match(range_header)
    if match:
        tup = range_tuple_normalize(match.group(1, 2))
        if tup and tup[1]:
            tup = (tup[0], tup[1] + 1)
        return tup
    return ()"
629,"    def __init__(
        self,
        dataset_name,
        tasks=None,
        distributed=True,
        output_dir=None,
        *,
        max_dets_per_image=None,
    ):
        """"""
        Args:
            dataset_name (str): name of the dataset to be evaluated.
                It must have the following corresponding metadata:
                ""json_file"": the path to the LVIS format annotation
            tasks (tuple[str]): tasks that can be evaluated under the given
                configuration. A task is one of ""bbox"", ""segm"".
                By default, will infer this automatically from predictions.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump results.
            max_dets_per_image (None or int): limit on maximum detections per image in evaluating AP
                This limit, by default of the LVIS dataset, is 300.
        """"""","        from lvis import LVIS

        self._logger = logging.getLogger(__name__)

        if tasks is not None and isinstance(tasks, CfgNode):
            self._logger.warn(
                ""COCO Evaluator instantiated using config, this is deprecated behavior.""
                "" Please pass in explicit arguments instead.""
            )
            self._tasks = None  # Infering it from predictions should be better
        else:
            self._tasks = tasks

        self._distributed = distributed
        self._output_dir = output_dir
        self._max_dets_per_image = max_dets_per_image

        self._cpu_device = torch.device(""cpu"")

        self._metadata = MetadataCatalog.get(dataset_name)
        json_file = PathManager.get_local_path(self._metadata.json_file)
        self._lvis_api = LVIS(json_file)
        # Test set json files do not contain annotations (evaluation must be
        # performed using the LVIS evaluation server).
        self._do_evaluation = len(self._lvis_api.get_ann_ids()) > 0"
630,"    def plainbytes(self, text, **opts):
        """"""show raw bytes for non-templated mode""""""",
631,"def truncatelonglines(context, mapping, args):
    """"""Truncate lines in text to no more than ""maxwidth"" in width. If ""suffix""
    is supplied, then it replaces the last characters on the line if, and only
    if, the lines was truncated.""""""","    if not (2 <= len(args) <= 3):
        # i18n: ""truncatelonglines"" is a keyword
        raise error.ParseError(
            _(""truncatelonglines expects two or three arguments, got %d"") % len(args)
        )
    text = evalstring(context, mapping, args[0])
    maxwidth = evalinteger(
        context,
        mapping,
        args[1],
        # i18n: ""truncatelonglines"" is a keyword
        _(""truncatelonglines expects an integer line width""),
    )
    truncatedwidth = maxwidth
    if len(args) == 3:
        suffix = evalstring(context, mapping, args[2])
        if isinstance(suffix, bytes):
            # Python 2
            truncatedwidth -= len(suffix.decode(""utf-8""))
        else:
            # Python 3
            truncatedwidth -= len(suffix)
    else:
        suffix = None

    if truncatedwidth < 0:
        truncatedwidth = 0

    lines = text.splitlines(True)
    output = []
    for line in lines:
        stripped = line.rstrip(""\r\n"")
        if len(stripped) > maxwidth:
            eol = line[len(stripped) :]
            line = stripped[:truncatedwidth]
            if suffix is not None:
                line += suffix
            line += eol

        output.append(line)

    return """".join(output)"
632,"    def _compile_checkbox_stats(self) -> Dict[str, pd.DataFrame]:
        """"""
        Return the fraction of time that Turkers selected each checkbox.

        Results are cut both (1) by matchup and winner and (2) by just the winner. Each
        checkbox represents one reason that the Turkers could have chosen the speaker
        that they did.
        """"""","        checkbox_columns = [
            col
            for col in self.dataframe.columns
            if col.startswith(self.checkbox_prefix)
        ]
        group_column_types = {
            'matchup_and_winner': ['matchup', 'winner'],
            'winner': ['winner'],
        }
        grouped_dataframes = {}
        for group_type, group_columns in group_column_types.items():
            selected_columns = (
                self.dataframe[group_columns + checkbox_columns]
                .rename(
                    columns={
                        col: col[len(self.checkbox_prefix) :]
                        for col in checkbox_columns
                    }
                )
                .set_index(group_columns)
                .fillna(False)
            )
            grouped_dataframes[group_type] = selected_columns.groupby(
                group_columns
            ).mean()
        return grouped_dataframes"
633,"def tracking(repo):
    """"""returns true if this repo is explicitly tracking visible mutable heads""""""","    return ""visibleheads"" in repo.storerequirements"
634,"def _load_lvis_annotations(json_file: str):
    """"""
    Load COCO annotations from a JSON file

    Args:
        json_file: str
            Path to the file to load annotations from
    Returns:
        Instance of `pycocotools.coco.COCO` that provides access to annotations
        data
    """"""","    from lvis import LVIS

    json_file = PathManager.get_local_path(json_file)
    logger = logging.getLogger(__name__)
    timer = Timer()
    lvis_api = LVIS(json_file)
    if timer.seconds() > 1:
        logger.info(""Loading {} takes {:.2f} seconds."".format(json_file, timer.seconds()))
    return lvis_api"
635,"def universal_sentence_embedding(sentences, mask, sqrt=True):
    """"""
    Perform Universal Sentence Encoder averaging (https://arxiv.org/abs/1803.11175).

    This is really just sum / sqrt(len).

    :param Tensor sentences: an N x T x D of Transformer outputs. Note this is
        the exact output of TransformerEncoder, but has the time axis first
    :param ByteTensor: an N x T binary matrix of paddings

    :return: an N x D matrix of sentence embeddings
    :rtype Tensor:
    """"""","    # need to mask out the padded chars
    sentence_sums = th.bmm(
        sentences.permute(0, 2, 1), mask.float().unsqueeze(-1)
    ).squeeze(-1)
    divisor = mask.sum(dim=1).view(-1, 1).float()
    if sqrt:
        divisor = divisor.sqrt()

    divisor[divisor < 1] = 1
    sentence_sums /= divisor
    return sentence_sums"
636,"def continuecmd(ui, repo):
    """"""resume operation after resolving conflicts""""""","    for name, cmd in cmdutil.afterresolvedstates:
        if repo.localvfs.exists(name):
            args = shlex.split(cmd)
            if not ui.interactive():
                args.append(""--noninteractive"")
            return bindings.commands.run(args, ui.fin, ui.fout, ui.ferr)
    else:
        ms = mergemod.mergestate.read(repo)
        if ms.files():
            if ms.unresolvedcount() == 0:
                # no command support --continue, just delete the merge state.
                ui.status(_(""(exiting merge state)\n""))
                ms.reset()
            else:
                raise error.Abort(
                    _(""outstanding merge conflicts""),
                    hint=_(
                        ""use '@prog@ resolve -l' to see a list of conflicted files, '@prog@ resolve -m' to mark files as resolved""
                    ),
                )
        else:
            raise error.Abort(_(""nothing to continue""))"
637,"def commitfuncfor(repo, src):
    """"""Build a commit function for the replacement of <src>

    This function ensure we apply the same treatment to all changesets.

    - Add a 'histedit_source' entry in extra.

    Note that fold has its own separated logic because its handling is a bit
    different and not easily factored out of the fold method.
    """"""","    phasemin = src.phase()

    def commitfunc(**kwargs):
        overrides = {(""phases"", ""new-commit""): phasemin}
        with repo.ui.configoverride(overrides, ""histedit""):
            extra = kwargs.get(r""extra"", {}).copy()
            extra[""histedit_source""] = src.hex()
            kwargs[r""mutinfo""] = mutation.record(repo, extra, [src.node()], ""histedit"")
            kwargs[r""extra""] = extra
            kwargs[r""loginfo""] = {""predecessors"": src.hex(), ""mutation"": ""histedit""}
            return repo.commit(**kwargs)

    return commitfunc"
638,"    def _init_attributes(self, opt: Opt):
        """"""
        Given opt dictionary, initialize relevant attributes for the predictor.

        :param opt:
            options dict
        """"""","        optfile = f""{self.predictor_model_file}.opt""
        opt_from_file = Opt.load(optfile)
        overrides = opt.get('override')
        opt_from_file.update(overrides)

        assert 'num_utterances' in opt_from_file, opt_from_file
        self.num_utterances = opt_from_file['num_utterances']
        self.annotate_speaker = opt_from_file['annotate_speaker']
        self.speaker_annotation_position = opt_from_file['speaker_annotation_position']
        self.speaker_label_type = opt_from_file['speaker_label_type']
        self.speaker_separator = opt_from_file['speaker_separator']
        self.include_context = opt_from_file['include_light_context']

        if opt_from_file.get('exclude_from_context'):
            self.exclude_from_context = opt_from_file['exclude_from_context'].split(',')
        else:
            self.exclude_from_context = []"
639,"    def save(self, path=None):
        """"""
        Save model parameters if model_file is set.
        """"""","        path = self.opt.get('model_file', None) if path is None else path
        if path and hasattr(self, 'model'):
            data = {}
            data['model'] = self.model.state_dict()
            data['optimizer'] = self.optimizer.state_dict()
            data['opt'] = self.opt
            torch_utils.atomic_save(data, path)
            with PathManager.open(path + '.opt', 'w') as handle:
                json.dump(self.opt, handle)"
640,"def CheckComment(comment, filename, linenum, error):
  """"""Checks for common mistakes in TODO comments.

  Args:
    comment: The text of the comment from the line in question.
    filename: The name of the current file.
    linenum: The number of the line to check.
    error: The function to call with any errors found.
  """"""","  match = _RE_PATTERN_TODO.match(comment)
  if match:
    # One whitespace is correct; zero whitespace is handled elsewhere.
    leading_whitespace = match.group(1)
    if len(leading_whitespace) > 1:
      error(filename, linenum, 'whitespace/todo', 2,
            'Too many spaces before TODO')

    username = match.group(2)
    if not username:
      error(filename, linenum, 'readability/todo', 2,
            'Missing username in TODO; it should look like '
            '""// TODO(my_username): Stuff.""')

    middle_whitespace = match.group(3)
    # Comparisons made explicit for correctness -- pylint: disable-msg=C6403
    if middle_whitespace != ' ' and middle_whitespace != '':
      error(filename, linenum, 'whitespace/todo', 2,
            'TODO(my_username) should be followed by a space')"
641,"    def test_generator_backcomp(self):
        """"""
        Tests that the generator model files work over time.
        """"""","        _, test = testing_utils.eval_model(
            dict(
                task='integration_tests:multiturn_candidate',
                model='transformer/generator',
                model_file='zoo:unittest/transformer_generator2/model',
                dict_file='zoo:unittest/transformer_generator2/model.dict',
                rank_candidates=False,
                batchsize=64,
            ),
            skip_valid=True,
        )

        self.assertLessEqual(test['ppl'], 1.01)
        self.assertGreaterEqual(test['accuracy'], 0.99)
        self.assertGreaterEqual(test['f1'], 0.99)"
642,"    def scale(self, scale_x: float, scale_y: float) -> None:
        """"""
        Scale the rotated box with horizontal and vertical scaling factors
        Note: when scale_factor_x != scale_factor_y,
        the rotated box does not preserve the rectangular shape when the angle
        is not a multiple of 90 degrees under resize transformation.
        Instead, the shape is a parallelogram (that has skew)
        Here we make an approximation by fitting a rotated rectangle to the parallelogram.
        """"""","        self.tensor[:, 0] *= scale_x
        self.tensor[:, 1] *= scale_y
        theta = self.tensor[:, 4] * math.pi / 180.0
        c = torch.cos(theta)
        s = torch.sin(theta)

        # In image space, y is top->down and x is left->right
        # Consider the local coordintate system for the rotated box,
        # where the box center is located at (0, 0), and the four vertices ABCD are
        # A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)
        # the midpoint of the left edge AD of the rotated box E is:
        # E = (A+D)/2 = (-w / 2, 0)
        # the midpoint of the top edge AB of the rotated box F is:
        # F(0, -h / 2)
        # To get the old coordinates in the global system, apply the rotation transformation
        # (Note: the right-handed coordinate system for image space is yOx):
        # (old_x, old_y) = (s * y + c * x, c * y - s * x)
        # E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)
        # F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)
        # After applying the scaling factor (sfx, sfy):
        # E(new) = (-sfx * c * w / 2, sfy * s * w / 2)
        # F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)
        # The new width after scaling tranformation becomes:

        # w(new) = |E(new) - O| * 2
        #        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2
        #        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w
        # i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]
        #
        # For example,
        # when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;
        # when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y
        self.tensor[:, 2] *= torch.sqrt((scale_x * c) ** 2 + (scale_y * s) ** 2)

        # h(new) = |F(new) - O| * 2
        #        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2
        #        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h
        # i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]
        #
        # For example,
        # when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;
        # when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x
        self.tensor[:, 3] *= torch.sqrt((scale_x * s) ** 2 + (scale_y * c) ** 2)

        # The angle is the rotation angle from y-axis in image space to the height
        # vector (top->down in the box's local coordinate system) of the box in CCW.
        #
        # angle(new) = angle_yOx(O - F(new))
        #            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )
        #            = atan2(sfx * s * h / 2, sfy * c * h / 2)
        #            = atan2(sfx * s, sfy * c)
        #
        # For example,
        # when sfx == sfy, angle(new) == atan2(s, c) == angle(old)
        self.tensor[:, 4] = torch.atan2(scale_x * s, scale_y * c) * 180 / math.pi"
643,"def find_on_path(importer, path_item, only=False):
    """"""Yield distributions accessible on a sys.path directory""""""","    path_item = _normalize_cached(path_item)

    if os.path.isdir(path_item) and os.access(path_item, os.R_OK):
        if path_item.lower().endswith('.egg'):
            # unpacked egg
            yield Distribution.from_filename(
                path_item, metadata=PathMetadata(
                    path_item, os.path.join(path_item,'EGG-INFO')
                )
            )
        else:
            # scan for .egg and .egg-info in directory
            for entry in os.listdir(path_item):
                lower = entry.lower()
                if lower.endswith('.egg-info') or lower.endswith('.dist-info'):
                    fullpath = os.path.join(path_item, entry)
                    if os.path.isdir(fullpath):
                        # egg-info directory, allow getting metadata
                        metadata = PathMetadata(path_item, fullpath)
                    else:
                        metadata = FileMetadata(fullpath)
                    yield Distribution.from_location(
                        path_item, entry, metadata, precedence=DEVELOP_DIST
                    )
                elif not only and lower.endswith('.egg'):
                    dists = find_distributions(os.path.join(path_item, entry))
                    for dist in dists:
                        yield dist
                elif not only and lower.endswith('.egg-link'):
                    with open(os.path.join(path_item, entry)) as entry_file:
                        entry_lines = entry_file.readlines()
                    for line in entry_lines:
                        if not line.strip():
                            continue
                        path = os.path.join(path_item, line.rstrip())
                        dists = find_distributions(path)
                        for item in dists:
                            yield item
                        break"
644,"def flatten_dicts(*args, **_):
    """"""Flatten the given list of dictionaries by merging args[1:] onto
    args[0], one at a time.

    :param *args: the list of dict objects to flatten.
    :param **_: ignore the build_env kwarg
    :return: a single dict containing the flattened list
    """"""",    return flatten_list_of_dicts(args)
645,"def multiprocess_eval(
    rank, opt, port=61337, rank_offset=0, gpu=None, hostname='localhost'
):
    """"""
    Run a multiprocessing evaluation.

    Invoked by launch_and_eval, not instantiated directly.
    """"""","    init_method = f'tcp://{hostname}:{port}'
    with distributed_utils.distributed_context(
        rank, opt, rank_offset, gpu, init_method=init_method
    ) as opt:
        opt['multiprocessing'] = True
        return eval_model.eval_model(opt)"
646,"    def removefile(self, f: str, oldstate: str, size: int) -> None:
        """"""
        Mark a file as removed in the dirstate.

        The `size` parameter is used to store sentinel values that indicate
        the file's previous state.  In the future, we should refactor this
        to be more explicit about what that state is.
        """"""","        if oldstate not in ""?r"" and ""_dirs"" in self.__dict__:
            self._dirs.delpath(f)
        if oldstate == ""?"" and ""_alldirs"" in self.__dict__:
            self._alldirs.addpath(f)
        if ""filefoldmap"" in self.__dict__:
            normed = util.normcase(f)
            self.filefoldmap.pop(normed, None)
        self._insert_tuple(f, ""r"", 0, size, 0)
        self.nonnormalset.add(f)"
647,"def _filemerge(premerge, repo, wctx, mynode, orig, fcd, fco, fca, labels=None):
    """"""perform a 3-way merge in the working directory

    premerge = whether this is a premerge
    mynode = parent node before merge
    orig = original local filename before merge
    fco = other file context
    fca = ancestor file context
    fcd = local file context for current/destination file

    Returns whether the merge is complete, the return value of the merge, and
    a boolean indicating whether the file was deleted from disk.""""""","
    if not fco.cmp(fcd):  # files ide gtical?
        return True, None, False

    ui = repo.ui
    fd = fcd.path()
    relorig = repo.pathto(orig)
    relfo = repo.pathto(fco.path())
    relfd = repo.pathto(fd)

    binary = fcd.isbinary() or fco.isbinary() or fca.isbinary()
    symlink = ""l"" in fcd.flags() + fco.flags()
    changedelete = fcd.isabsent() or fco.isabsent()
    tool, toolpath = _picktool(repo, ui, fd, binary, symlink, changedelete)
    if tool in internals and tool.startswith(""internal:""):
        # normalize to new-style names (':merge' etc)
        tool = tool[len(""internal"") :]
    ui.debug(
        ""picked tool '%s' for %s (binary %s symlink %s changedelete %s)\n""
        % (
            tool,
            fd,
            pycompat.bytestr(binary),
            pycompat.bytestr(symlink),
            pycompat.bytestr(changedelete),
        )
    )

    if tool in internals:
        func = internals[tool]
        mergetype = func.mergetype
        onfailure = func.onfailure
        precheck = func.precheck
    else:
        if wctx.isinmemory():
            func = _xmergeimm
        else:
            func = _xmerge
        mergetype = fullmerge
        onfailure = _(""merging %s failed!\n"")
        precheck = None

    toolconf = tool, toolpath, binary, symlink

    if mergetype == nomerge:
        r, deleted = func(repo, mynode, orig, fcd, fco, fca, toolconf, labels)
        return True, r, deleted

    if premerge:
        if orig != fco.path():
            ui.status(_(""merging %s and %s to %s\n"") % (relorig, relfo, relfd))
        else:
            ui.status(_(""merging %s\n"") % relfd)

    ui.debug(""my %s other %s ancestor %s\n"" % (fcd, fco, fca))

    def getfailuremsg(on_failure, num_conflicts):
        # on_failure can be a string or a function which we'll call.
        if callable(on_failure):
            return on_failure(num_conflicts, repo, mynode, orig, fcd, fco, fca)
        else:
            return on_failure % relfd

    if precheck and not precheck(repo, mynode, orig, fcd, fco, fca, toolconf):
        if onfailure:
            if wctx.isinmemory():
                raise error.InMemoryMergeConflictsError(
                    ""in-memory merge does not support merge conflicts"",
                    type=error.InMemoryMergeConflictsError.TYPE_FILE_CONFLICTS,
                    paths=[fcd.path()],
                )
            ui.warn(getfailuremsg(onfailure, 1))
        return True, 1, False

    back = _makebackup(repo, ui, wctx, fcd, premerge)
    files = (None, None, None, back)
    r = 1
    try:
        markerstyle = ui.config(""ui"", ""mergemarkers"")
        if not labels:
            labels = _defaultconflictlabels
        if markerstyle != ""basic"":
            labels = _formatlabels(repo, fcd, fco, fca, labels)

        if premerge and mergetype == fullmerge:
            r = _premerge(repo, fcd, fco, fca, toolconf, files, labels=labels)
            # complete if premerge successful (r is 0)
            return not r, r, False

        needcheck, r, deleted = func(
            repo, mynode, orig, fcd, fco, fca, toolconf, files, labels=labels
        )

        if needcheck:
            r = _check(repo, r, ui, tool, fcd, files)

        if r:
            if onfailure:
                if wctx.isinmemory():
                    raise error.InMemoryMergeConflictsError(
                        ""in-memory merge does not support merge conflicts"",
                        type=error.InMemoryMergeConflictsError.TYPE_FILE_CONFLICTS,
                        paths=[fcd.path()],
                    )
                ui.warn(getfailuremsg(onfailure, r))
            _onfilemergefailure(ui)

        return True, r, deleted
    finally:
        if not r and back is not None:
            back.remove()"
648,"    def _construct_subagent_opts(self, opt: Opt):
        """"""
        Construct opts for each sub agent.

        :param opt:
            original Opt.
        """"""","        self.opts = {}
        self.opts['init'] = opt
        override_opts = defaultdict(dict)
        agent_mapping = {
            'krm': 'knowledge_agent',
            'drm': 'dialogue_agent',
            'sqm': 'search_query_agent',
            'sdm': 'search_decision_agent',
        }
        for k, v in opt['override'].items():
            k_set = False
            for prefix, ag_opt in agent_mapping.items():
                if k.startswith(f'{prefix}_'):
                    override_opts[ag_opt][k] = v
                    k_set = True
                    break
            if not k_set:
                override_opts['general'][k] = v
        self.opts['override'] = override_opts
        for prefix, key in agent_mapping.items():
            if f'{prefix}_interactive_mode' not in override_opts[key]:
                override_opts[key][f'{prefix}_interactive_mode'] = opt.get(
                    'interactive_mode', False
                )

        for agent in agent_mapping.values():
            filename = opt[f""{agent.replace('agent', 'response')}_model_path""]
            if not filename:
                continue
            self.opts[agent] = self._get_subagent_opt(
                filename=filename,
                specific_override_args=override_opts[agent],
                general_override_args=override_opts['general'],
            )
            self.opts[agent]['model_file'] = filename
            self.opts[agent]['override']['model_file'] = filename"
649,"def _get_blobs(im, rois, target_scale, target_max_size):
    """"""Convert an image and RoIs within that image into network inputs.""""""","    blobs = {}
    blobs['data'], im_scale, blobs['im_info'] = \
        blob_utils.get_image_blob(im, target_scale, target_max_size)
    if rois is not None:
        blobs['rois'] = _get_rois_blob(rois, im_scale)
    return blobs, im_scale"
650,"def migratetodoublewrite(repo, requirename=""doublewritechangelog""):
    """"""Migrate to ""double write"" backend.

    Commit graph and IdMap use segments, commit text falls back to revlog.
    This can take about 1 minute for a large repo.
    """"""","    if requirename in repo.storerequirements:
        return
    svfs = repo.svfs
    revlogdir = svfs.join("""")
    segmentsdir = svfs.join(SEGMENTS_DIR)
    hgcommitsdir = svfs.join(HGCOMMITS_DIR)
    with repo.lock():
        master = list(repo.nodes(""present(%s)"", bookmod.mainbookmark(repo)))
        with progress.spinner(repo.ui, _(""migrating commit graph"")):
            bindings.dag.commits.migraterevlogtosegments(
                revlogdir, segmentsdir, hgcommitsdir, master
            )
        _removechangelogrequirements(repo)
        repo.storerequirements.add(requirename)
        repo._writestorerequirements()
        repo.invalidatechangelog()"
651,"    def split(self, path: str) -> ""Tuple[str, str]"":
        """"""split top-most element of a path (as os.path.split would do)

        This exists to allow handling of strange encoding if needed.""""""",        return os.path.split(path)
652,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Add command-line arguments specifically for this agent.
        """"""","        TransformerRankerAgent.add_cmdline_args(parser, partial_opt=partial_opt)
        agent = parser.add_argument_group('Polyencoder Arguments')
        agent.add_argument(
            '--polyencoder-type',
            type=str,
            default='codes',
            choices=['codes', 'n_first'],
            help='Type of polyencoder, either we compute'
            'vectors using codes + attention, or we '
            'simply take the first N vectors.',
            recommended='codes',
        )
        agent.add_argument(
            '--poly-n-codes',
            type=int,
            default=64,
            help='number of vectors used to represent the context'
            'in the case of n_first, those are the number'
            'of vectors that are considered.',
            recommended=64,
        )
        agent.add_argument(
            '--poly-attention-type',
            type=str,
            default='basic',
            choices=['basic', 'sqrt', 'multihead'],
            help='Type of the top aggregation layer of the poly-'
            'encoder (where the candidate representation is'
            'the key)',
            recommended='basic',
        )
        agent.add_argument(
            '--poly-attention-num-heads',
            type=int,
            default=4,
            help='In case poly-attention-type is multihead, '
            'specify the number of heads',
        )

        # Those arguments are here in case where polyencoder type is 'code'
        agent.add_argument(
            '--codes-attention-type',
            type=str,
            default='basic',
            choices=['basic', 'sqrt', 'multihead'],
            help='Type ',
            recommended='basic',
        )
        agent.add_argument(
            '--codes-attention-num-heads',
            type=int,
            default=4,
            help='In case codes-attention-type is multihead, '
            'specify the number of heads',
        )
        return agent"
653,"  def SetOutputFormat(self, output_format):
    """"""Sets the output format for errors.""""""",    self.output_format = output_format
654,"def CheckStyle(filename, clean_lines, linenum, file_extension, nesting_state,
               error):
  """"""Checks rules from the 'C++ style rules' section of cppguide.html.

  Most of these rules are hard to test (naming, comment style), but we
  do what we can.  In particular we check for 2-space indents, line lengths,
  tab usage, spaces inside code, etc.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    file_extension: The extension (without the dot) of the filename.
    nesting_state: A _NestingState instance which maintains information about
                   the current stack of nested blocks being parsed.
    error: The function to call with any errors found.
  """"""","
  raw_lines = clean_lines.raw_lines
  line = raw_lines[linenum]

  if line.find('\t') != -1:
    error(filename, linenum, 'whitespace/tab', 1,
          'Tab found; better to use spaces')

  # One or three blank spaces at the beginning of the line is weird; it's
  # hard to reconcile that with 2-space indents.
  # NOTE: here are the conditions rob pike used for his tests.  Mine aren't
  # as sophisticated, but it may be worth becoming so:  RLENGTH==initial_spaces
  # if(RLENGTH > 20) complain = 0;
  # if(match($0, "" +(error|private|public|protected):"")) complain = 0;
  # if(match(prev, ""&& *$"")) complain = 0;
  # if(match(prev, ""\\|\\| *$"")) complain = 0;
  # if(match(prev, ""[\"",=><] *$"")) complain = 0;
  # if(match($0, "" <<"")) complain = 0;
  # if(match(prev, "" +for \\("")) complain = 0;
  # if(prevodd && match(prevprev, "" +for \\("")) complain = 0;
  initial_spaces = 0
  cleansed_line = clean_lines.elided[linenum]
  while initial_spaces < len(line) and line[initial_spaces] == ' ':
    initial_spaces += 1
  if line and line[-1].isspace():
    error(filename, linenum, 'whitespace/end_of_line', 4,
          'Line ends in whitespace.  Delete these extra spaces.')
  # There are certain situations we allow one space, notably for labels
  elif ((initial_spaces == 1 or initial_spaces == 3) and
        not Match(r'\s*\w+\s*:\s*$', cleansed_line)):
    error(filename, linenum, 'whitespace/indent', 3,
          'Weird number of spaces at line-start.  '
          'Are you using a 2-space indent?')
  # Labels should always be indented at least one space.
  elif initial_spaces and line[:2] != '//' and \
      not Search(r'(case|default)', line) and \
      Search(r'[^:]:\s*$', line):
    error(filename, linenum, 'whitespace/labels', 4,
          'Labels should always be at the start of the line')


  # Check if the line is a header guard.
  is_header_guard = False
  if file_extension == 'h':
    cppvar = GetHeaderGuardCPPVariable(filename)
    if (line.startswith('#ifndef %s' % cppvar) or
        line.startswith('#define %s' % cppvar) or
        line.startswith('#endif  // %s' % cppvar)):
      is_header_guard = True
  # #include lines and header guards can be long, since there's no clean way to
  # split them.
  #
  # URLs can be long too.  It's possible to split these, but it makes them
  # harder to cut&paste.
  #
  # The ""$Id:...$"" comment may also get very long without it being the
  # developers fault.
  if (not line.startswith('#include') and not is_header_guard and
      not Match(r'^\s*//.*http(s?)://\S*$', line) and
      not Match(r'^// \$Id:.*#[0-9]+ \$$', line)):
    line_width = GetLineWidth(line)
    if line_width > 100:
      error(filename, linenum, 'whitespace/line_length', 4,
            'Lines should very rarely be longer than 100 characters')
    elif line_width > 80:
      error(filename, linenum, 'whitespace/line_length', 2,
            'Lines should be <= 80 characters long')

  if (cleansed_line.count(';') > 1 and
      # for loops are allowed two ;'s (and may run over two lines).
      cleansed_line.find('for') == -1 and
      (GetPreviousNonBlankLine(clean_lines, linenum)[0].find('for') == -1 or
       GetPreviousNonBlankLine(clean_lines, linenum)[0].find(';') != -1) and
      # It's ok to have many commands in a switch case that fits in 1 line
      not ((cleansed_line.find('case ') != -1 or
            cleansed_line.find('default:') != -1) and
           cleansed_line.find('break;') != -1)):
    error(filename, linenum, 'whitespace/newline', 0,
          'More than one command on the same line')

  # Some more style checks
  CheckBraces(filename, clean_lines, linenum, error)
  CheckEmptyLoopBody(filename, clean_lines, linenum, error)
  CheckAccess(filename, clean_lines, linenum, nesting_state, error)
  CheckSpacing(filename, clean_lines, linenum, nesting_state, error)
  CheckCheck(filename, clean_lines, linenum, error)
  CheckAltTokens(filename, clean_lines, linenum, error)
  classinfo = nesting_state.InnermostClass()
  if classinfo:
    CheckSectionSpacing(filename, clean_lines, classinfo, linenum, error)"
655,"    def get_innodb_version(self):
        """""" SHOW VARIABLES LIKE innodb_version 
            mostly used as a check to ensure if a 
            test should/shouldn't be executed

        """""" ","
        query = ""SHOW VARIABLES LIKE 'innodb_version'""
        retcode, result = execute_query(query, self)
        return retcode, result "
656,"def find_keys(keys, criteria):
    """"""
    Finds all the keys that satisfy *all* of the given criterions.
    A criterion is a tuple consisting of a pattern and optionally a field name.
    If field name is specified, only tokens under the given field name are considered.
    """"""","    if len(criteria) == 0:
        return []
    res = []
    for key, tokens in keys.iteritems():
        for pattern, field_name in criteria:
            field_found = field_name is None
            for token in tokens:
                if field_name is not None and token_type(token) == ""key"":
                    field_found = re.search(field_name, token_value(token)) is not None
                if field_found and re.search(pattern, token) is not None:
                    break
            else:
                # loop finished normally, meaning the current criterion was not met
                break
        else:
            # loop finished normally, meaning all the criterions were met
            res.append(key)
    return res"
657,"    def _remove_person_tokens(self, text: str) -> str:
        """"""
        Remove person tokens from a text input.
        """"""","        return text.replace(f'{self.P1_TOKEN} ', '').replace(f'{self.P2_TOKEN} ', '')"
658,"def maybekindpats(expr) -> Optional[List[Tuple[str, str]]]:
    """"""attempt convert fileset expression to a list of (kind, pat)s.

    Return None if the fileset expression cannot be converted.
    """"""","    tree = parse(expr)
    return _maybekindpats(tree)"
659,"    def cross_entropy_batch(self, batch: List[str]) -> List[Tuple[float, int]]:
        """"""
        Return cross-entropy across all tokens in batch, and the number of target tokens
        in this batch.
        """"""","        h_encodings = self.ppl_tokenizer(
            batch,
            return_tensors=""pt"",
            padding=True,
        )
        input_ids = h_encodings.input_ids
        attn_mask = h_encodings.attention_mask

        # find token count for each string, to be used for averaging later
        token_counts = attn_mask.sum(-1).tolist()

        # add start token
        bos_tokens_tensor = torch.tensor(
            [[self.ppl_tokenizer.bos_token_id]] * input_ids.size(dim=0)
        )
        input_ids = torch.cat([bos_tokens_tensor, input_ids], dim=1)
        attn_mask = torch.cat(
            [
                torch.ones(bos_tokens_tensor.size(), dtype=torch.int64),
                attn_mask,
            ],
            dim=1,
        )

        losses = self.cross_entropy_strided(
            input_ids=input_ids, attention_mask=attn_mask
        )

        assert len(losses) == len(batch), ""Losses should be separated by example.""

        return list(zip(losses, token_counts))"
660,"def load_teacher_module(taskname: str):
    """"""
    Get the module of the teacher agent specified by `--task`.

    Can be formatted in several different ways:

    * full: ``-t parlai.tasks.babi.agents:DefaultTeacher``
    * shorthand: ``-t babi``, which will check
      ``parlai.tasks.babi.agents:DefaultTeacher``
    * shorthand specific: ``-t babi:task10k``, which will check
      ``parlai.tasks.babi.agents:Task10kTeacher``

    The base path to search when using shorthand formats can be changed from
    ""parlai"" to ""parlai_internal"" by prepending ""internal:"" to the path, e.g.
    ""internal:babi"".

    Options can be sent to the teacher by adding an additional colon,
    for example ``-t babi:task10k:1`` directs the babi Task10kTeacher to use
    task number 1.

    :param taskname: path to task class in one of the above formats.

    :return:
        teacher module
    """"""","    global TEACHER_REGISTRY
    if taskname in TEACHER_REGISTRY:
        return TEACHER_REGISTRY[taskname]

    task_module = load_task_module(taskname)
    task_path_list, repo = _get_task_path_and_repo(taskname)

    if len(task_path_list) > 1 and '=' not in task_path_list[1]:
        task_path_list[1] = task_path_list[1][0].upper() + task_path_list[1][1:]
        teacher = task_path_list[1]
        if '.' not in task_path_list[0] and 'Teacher' not in teacher:
            # Reformat from underscore to CamelCase and append ""Teacher"" to
            # class name by default if a complete path is not given.
            words = teacher.split('_')
            teacher_name = ''
            for w in words:
                teacher_name += w[0].upper() + w[1:]
            teacher = teacher_name + ""Teacher""
    else:
        teacher = ""DefaultTeacher""

    teacher_class = getattr(task_module, teacher)
    return teacher_class"
661,"    def enabled_summary(self):
        """"""
        :return: XCUIElement is enabled summary
        :rtype: str | None
        """"""","        if not self.enabled_value:
            return ""enabled: {}"".format(self.enabled_value)
        return None"
662,"    def debugwireargs(self, one, two, three=None, four=None, five=None):
        """"""used to test argument passing over the wire""""""","        return ""%s %s %s %s %s"" % (one, two, three, four, five)"
663,"    def get_task_data(self) -> List[Dict[str, Any]]:
        """"""
        Retrieves task data for a list of Mephisto task units.
        """"""","        task_data = []
        for unit in self.get_task_units():
            unit_data = self.get_data_from_unit(unit)
            if unit_data and self.is_unit_acceptable(unit_data):
                task_data.append(unit_data)

        return task_data"
664,"    def removedirs(self, path: ""Optional[str]"" = None) -> None:
        """"""Remove a leaf directory and all empty intermediate ones""""""",        return util.removedirs(self.join(path))
665,"def remotenameskw(**args) -> _hybrid:
    """""":remotenames: List of strings. List of remote names associated with the
    changeset.
    """"""","    repo, ctx = args[""repo""], args[""ctx""]

    remotenames = []
    if ""remotebookmarks"" in repo.names:
        remotenames = repo.names[""remotebookmarks""].names(repo, ctx.node())

    return showlist(""remotename"", remotenames, args, plural=""remotenames"")"
666,"def _valid_for_check(cls):
    """"""An internal helper to prohibit isinstance([1], List[str]) etc.""""""","    if cls is Generic:
        raise TypeError(""Class %r cannot be used with class ""
                        ""or instance checks"" % cls)
    if (cls.__origin__ is not None and
        sys._getframe(3).f_globals['__name__'] not in ['abc', 'functools']):
        raise TypeError(""Parameterized generics cannot be used with class ""
                        ""or instance checks"")"
667,"    def batch_act_sqm(
        self, observations: List[Dict[str, Message]], search_indices: List[int]
    ) -> List[Message]:
        """"""
        Search Query Generator batch act.

        :param observations:
            list of observations
        :param search_indices:
            list of batch indices for which search is required.

        :return batch_reply:
            return the batch reply from the search query agent
        """"""","        return super().batch_act_search_query_generation(
            observations,
            [o['search_query_agent'] for o in observations],
            search_indices,
            self.search_query_agent,
            self.knowledge_agent,
            self.inject_query_string,
        )"
668,"    def explore_expr(expr, value, is_child):
        """"""Function to explore array values.
        See Explorer.explore_expr for more information.
        """"""","        referenced_value = value.referenced_value()
        Explorer.explore_expr(expr, referenced_value, is_child)
        return False"
669,"    def get_acts(self):
        """"""
        Return the acts in the *current* subworld.
        """"""",        return self.worlds[self.world_idx].get_acts()
670,"def incrementalrepack(repo):
    """"""This repacks the repo by looking at the distribution of pack files in the
    repo and performing the most minimal repack to keep the repo in good shape.
    """"""","    _dorepack(repo, True)"
671,"    def readLine(self):
        """"""read a line
        Maintains its own buffer, callers of the transport should not mix
        calls to readBytes and readLine.
        """"""","        if self.buf is None:
            self.buf = []

        # Buffer may already have a line if we've received unilateral
        # response(s) from the server
        if len(self.buf) == 1 and b""\n"" in self.buf[0]:
            (line, b) = self.buf[0].split(b""\n"", 1)
            self.buf = [b]
            return line

        while True:
            b = self.readBytes(4096)
            if b""\n"" in b:
                result = b"""".join(self.buf)
                (line, b) = b.split(b""\n"", 1)
                self.buf = [b]
                return result + line
            self.buf.append(b)"
672,"    def requeue_task_data(self, worker_id: str, task_data: List[PairingsDict]):
        """"""
        Return task to task_queue.

        If the task is an onboarding task, indicate that the worker has
        another onboarding task to do.

        :param worker_id:
            worker id of worker who is returning task

        :param task_data:
            list of unfinished tasks to return to the queue.
        """"""","        worker_data = self._get_worker_data(worker_id)
        for subtask_data in task_data:
            if subtask_data[""task_specs""].get(""is_onboarding"", False):
                worker_data[""onboarding_todo""].append(subtask_data[""pair_id""])
            else:
                self.task_queue.put(subtask_data)
                try:
                    worker_data[""tasks_completed""].remove(subtask_data[""pair_id""])
                    for d_id in self._get_dialogue_ids(subtask_data):
                        worker_data[""conversations_seen""].remove(d_id)
                except ValueError:
                    # Task may have shown up in worker's task queue twice
                    # due to some unfortunate race condition
                    logger.exception(
                        f""could not remove task from worker {worker_id} history"",
                        exc_info=True,
                    )"
673,"    def getreferences(self, reponame, workspace, baseversion, clientinfo):
        """"""Gets the current references if they differ from the base version""""""",
674,"    def _check_timeout(self, timeout=None):
        """"""
        Return whether enough time has passed than the timeout amount.
        """"""","        if timeout:
            return time.time() - self.message_request_time > timeout
        return False"
675,"    def set_result(self, result):
        """"""Indicate that execution has completed""""""","        self.timer.stop()
        if self.future is not None:
            self.future.set_result(result)
        if self.deferred is not None:
            self.deferred.callback(result)"
676,"def on_open(ws):
    """"""
    Starts a new thread that loops, taking user input and sending it to the websocket.

    :param ws: websocket.WebSocketApp that sends messages to a terminal_manager
    """"""","    id = _get_rand_id()
    threading.Thread(target=_run, args=(ws, id)).start()"
677,"    def unregister_except(self, fd):
        """"""Unregister `fd` from selecting for delete""""""","        callback = self._xcallbacks.pop(fd, None)
        #self.logger.debug('Unregistered %s from except on %d', callback, fd)
        self.control(SelectTask.NEWFD)
        return callback"
678,"    def summarize(self):
        """"""
        Compute and display summary metrics for evaluation results.
        Note this function can *only* be applied on the default parameter setting
        """"""","
        def _summarize(ap=1, iouThr=None, areaRng=""all"", maxDets=100):
            p = self.params
            iStr = "" {:<18} {} @[ {}={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}""
            titleStr = ""Average Precision"" if ap == 1 else ""Average Recall""
            typeStr = ""(AP)"" if ap == 1 else ""(AR)""
            measure = ""IoU""
            if self.params.iouType == ""keypoints"":
                measure = ""OKS""
            elif self.params.iouType == ""densepose"":
                measure = ""OGPS""
            iouStr = (
                ""{:0.2f}:{:0.2f}"".format(p.iouThrs[0], p.iouThrs[-1])
                if iouThr is None
                else ""{:0.2f}"".format(iouThr)
            )

            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]
            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]
            if ap == 1:
                # dimension of precision: [TxRxKxAxM]
                s = self.eval[""precision""]
                # IoU
                if iouThr is not None:
                    t = np.where(np.abs(iouThr - p.iouThrs) < 0.001)[0]
                    s = s[t]
                s = s[:, :, :, aind, mind]
            else:
                # dimension of recall: [TxKxAxM]
                s = self.eval[""recall""]
                if iouThr is not None:
                    t = np.where(np.abs(iouThr - p.iouThrs) < 0.001)[0]
                    s = s[t]
                s = s[:, :, aind, mind]
            if len(s[s > -1]) == 0:
                mean_s = -1
            else:
                mean_s = np.mean(s[s > -1])
            logger.info(iStr.format(titleStr, typeStr, measure, iouStr, areaRng, maxDets, mean_s))
            return mean_s

        def _summarizeDets():
            stats = np.zeros((12,))
            stats[0] = _summarize(1)
            stats[1] = _summarize(1, iouThr=0.5, maxDets=self.params.maxDets[2])
            stats[2] = _summarize(1, iouThr=0.75, maxDets=self.params.maxDets[2])
            stats[3] = _summarize(1, areaRng=""small"", maxDets=self.params.maxDets[2])
            stats[4] = _summarize(1, areaRng=""medium"", maxDets=self.params.maxDets[2])
            stats[5] = _summarize(1, areaRng=""large"", maxDets=self.params.maxDets[2])
            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])
            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])
            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])
            stats[9] = _summarize(0, areaRng=""small"", maxDets=self.params.maxDets[2])
            stats[10] = _summarize(0, areaRng=""medium"", maxDets=self.params.maxDets[2])
            stats[11] = _summarize(0, areaRng=""large"", maxDets=self.params.maxDets[2])
            return stats

        def _summarizeKps():
            stats = np.zeros((10,))
            stats[0] = _summarize(1, maxDets=20)
            stats[1] = _summarize(1, maxDets=20, iouThr=0.5)
            stats[2] = _summarize(1, maxDets=20, iouThr=0.75)
            stats[3] = _summarize(1, maxDets=20, areaRng=""medium"")
            stats[4] = _summarize(1, maxDets=20, areaRng=""large"")
            stats[5] = _summarize(0, maxDets=20)
            stats[6] = _summarize(0, maxDets=20, iouThr=0.5)
            stats[7] = _summarize(0, maxDets=20, iouThr=0.75)
            stats[8] = _summarize(0, maxDets=20, areaRng=""medium"")
            stats[9] = _summarize(0, maxDets=20, areaRng=""large"")
            return stats

        def _summarizeUvs():
            stats = [_summarize(1, maxDets=self.params.maxDets[0])]
            min_threshold = self.params.iouThrs.min()
            if min_threshold <= 0.201:
                stats += [_summarize(1, maxDets=self.params.maxDets[0], iouThr=0.2)]
            if min_threshold <= 0.301:
                stats += [_summarize(1, maxDets=self.params.maxDets[0], iouThr=0.3)]
            if min_threshold <= 0.401:
                stats += [_summarize(1, maxDets=self.params.maxDets[0], iouThr=0.4)]
            stats += [
                _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.5),
                _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.75),
                _summarize(1, maxDets=self.params.maxDets[0], areaRng=""medium""),
                _summarize(1, maxDets=self.params.maxDets[0], areaRng=""large""),
                _summarize(0, maxDets=self.params.maxDets[0]),
                _summarize(0, maxDets=self.params.maxDets[0], iouThr=0.5),
                _summarize(0, maxDets=self.params.maxDets[0], iouThr=0.75),
                _summarize(0, maxDets=self.params.maxDets[0], areaRng=""medium""),
                _summarize(0, maxDets=self.params.maxDets[0], areaRng=""large""),
            ]
            return np.array(stats)

        def _summarizeUvsOld():
            stats = np.zeros((18,))
            stats[0] = _summarize(1, maxDets=self.params.maxDets[0])
            stats[1] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.5)
            stats[2] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.55)
            stats[3] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.60)
            stats[4] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.65)
            stats[5] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.70)
            stats[6] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.75)
            stats[7] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.80)
            stats[8] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.85)
            stats[9] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.90)
            stats[10] = _summarize(1, maxDets=self.params.maxDets[0], iouThr=0.95)
            stats[11] = _summarize(1, maxDets=self.params.maxDets[0], areaRng=""medium"")
            stats[12] = _summarize(1, maxDets=self.params.maxDets[0], areaRng=""large"")
            stats[13] = _summarize(0, maxDets=self.params.maxDets[0])
            stats[14] = _summarize(0, maxDets=self.params.maxDets[0], iouThr=0.5)
            stats[15] = _summarize(0, maxDets=self.params.maxDets[0], iouThr=0.75)
            stats[16] = _summarize(0, maxDets=self.params.maxDets[0], areaRng=""medium"")
            stats[17] = _summarize(0, maxDets=self.params.maxDets[0], areaRng=""large"")
            return stats

        if not self.eval:
            raise Exception(""Please run accumulate() first"")
        iouType = self.params.iouType
        if iouType in [""segm"", ""bbox""]:
            summarize = _summarizeDets
        elif iouType in [""keypoints""]:
            summarize = _summarizeKps
        elif iouType in [""densepose""]:
            summarize = _summarizeUvs
        self.stats = summarize()"
679,"    def test_codec(self):
        """"""check that the error is in the same ballpark as PQ.""""""","        ds = datasets.SyntheticDataset(64, 3000, 3000, 0)

        xt = ds.get_train()
        xb = ds.get_database()

        nsplits = 2
        Msub = 2
        nbits = 4

        plsq = faiss.ProductLocalSearchQuantizer(ds.d, nsplits, Msub, nbits)
        plsq.train(xt)
        err_plsq = eval_codec(plsq, xb)

        pq = faiss.ProductQuantizer(ds.d, nsplits * Msub, nbits)
        pq.train(xt)
        err_pq = eval_codec(pq, xb)

        print(err_plsq, err_pq)
        self.assertLess(err_plsq, err_pq)"
680,"def check(ui, repo, **opts) -> int:
    """"""check the filesystem of a virtual checkout""""""","    args = [repo.root]
    if ui.verbose:
        args.append(""--verbose"")
    if opts[""dry_run""]:
        # edenfsctl uses a different name for --dry-run
        args.append(""--check-only"")
        opts[""dry_run""] = False
    return _calledenfsctl(ui, ""fsck"", args, opts=opts)"
681,"    def _sshgetdesignatednodes(self, keys):
        """"""
        Fetch the specified tree nodes over SSH.

        This method requires the server to support the ""designatednodes""
        capability. This capability overloads the gettreepack wireprotocol
        command to allow the client to specify an exact set of tree nodes
        to fetch; the server will then provide only those nodes.

        Returns False if the server does not support ""designatednodes"",
        and True otherwise.
        """"""","        fallbackpath = getfallbackpath(self._repo)
        with self._repo.connectionpool.get(fallbackpath) as conn:
            if ""designatednodes"" not in conn.peer.capabilities():
                raise error.ProgrammingError(""designatednodes must be supported"")

            mfnodes = [node for path, node in keys]
            directories = [path for path, node in keys]

            if self.ui.configbool(""remotefilelog"", ""debug"") and len(keys) == 1:
                name = keys[0][0]
                node = keys[0][1]
                msg = _(""fetching tree %r %s"") % (name, hex(node))
                self.ui.warn(msg + ""\n"")

            start = util.timer()
            with self.ui.timesection(""getdesignatednodes""):
                _gettrees(
                    self._repo,
                    conn.peer,
                    """",
                    mfnodes,
                    [],
                    directories,
                    start,
                    depth=1,
                    ondemandfetch=True,
                )

        return True"
682,"def _add_finder(importer, finder):
  """"""Register a new pkg_resources path finder that does not replace the existing finder.""""""","
  existing_finder = _get_finder(importer)

  if not existing_finder:
    pkg_resources.register_finder(importer, finder)
  else:
    pkg_resources.register_finder(importer, ChainedFinder.of(existing_finder, finder))"
683,"def reset(frequency=None):
    """"""Clear out the state of the profiler.  Do not call while the
    profiler is running.

    The optional frequency argument specifies the number of samples to
    collect per second.""""""","    assert state.profile_level == 0, ""Can't reset() while statprof is running""
    CodeSite.cache.clear()
    state.reset(frequency)"
684,"def get_dist_group():
    """"""
    Find the default pytorch distributed group.

    Used within FSDP to mark which workers are participating. Important to manually call
    this because FSDP will cache old groups, but our test suite will instantiate new
    groups per test.
    """"""","    from torch.distributed.distributed_c10d import _get_default_group

    return _get_default_group()"
685,"    def reorder_decoder_incremental_state(
        self, incremental_state: Dict[int, dict], inds: th.Tensor
    ) -> Dict[int, dict]:
        """"""
        Reorder the decoder incremental state.

        See ``TorchGeneratorModel.reorder_decoder_incremental_state`` for a description.

        Here, incremental_state is a dict whose keys are layer indices and whose values
        are dicts containing the incremental state for that layer.
        """"""","        return {
            idx: layer.reorder_incremental_state(incremental_state[idx], inds)
            for idx, layer in enumerate(self.decoder.transformer.layers)
        }"
686,"    def __init__(self, factory, takes_self=False):
        """"""
        `Factory` is part of the default machinery so if we want a default
        value here, we have to implement it ourselves.
        """"""","        self.factory = factory
        self.takes_self = takes_self"
687,"def _tryremove(repo, svfs, name):
    """"""Attempt to remote a file from svfs.
    Failures such as EPERM are ignored.
    """"""","    try:
        if svfs.isdir(name):
            svfs.rmtree(name)
        else:
            svfs.unlink(name)
        repo.ui.note_err(_(""removed backup file %s\n"") % name)
    except Exception as e:
        repo.ui.status_err(""cannot remove backup file %s: %s\n"" % (name, e))"
688,"def try_import(name, alternative=None):
    """"""Attempt to import ``name``.  If it fails, return ``alternative``.

    When supporting multiple versions of Python or optional dependencies, it
    is useful to be able to try to import a module.

    :param name: The name of the object to import, e.g. ``os.path`` or
        ``os.path.join``.
    :param alternative: The value to return if no module can be imported.
        Defaults to None.
    """"""","    module_segments = name.split('.')
    while module_segments:
        module_name = '.'.join(module_segments)
        try:
            module = __import__(module_name)
        except ImportError:
            module_segments.pop()
            continue
        else:
            break
    else:
        return alternative
    nonexistent = object()
    for segment in name.split('.')[1:]:
        module = getattr(module, segment, nonexistent)
        if module is nonexistent:
            return alternative
    return module"
689,"    def epoch_done(self):
        """"""
        Return if *all* the subworlds are done.
        """"""","        for t in self.worlds:
            if not t.epoch_done():
                return False
        return True"
690,"def debugvisibilitystart(ui, repo) -> int:
    """"""start tracking commit visibility explicitly""""""","    visibility.starttracking(repo)
    return 0"
691,"def maybe_annotate(
    character: str,
    text: str,
    annotate_speaker: bool,
    speaker_separator: bool,
    speaker_annotation_position: str,
) -> str:
    """"""
    Annotate text.

    Depending on setting of opt arg, either prepend or append.

    :param text:
        text to augment
    :param character:
        character to add
    :param annotate_speaker:
        whether to annotate the speaker
    :param speaker_separator:
        whether to incude speaker separator tokens
    :param speaker_annotation_position:
        where to annotate the speaker in the utterance

    :return text:
        return augmented text
    """"""","    if not annotate_speaker:
        return text
    speaker_text = character
    if speaker_separator:
        speaker_text = f'{BEGIN_SPEAKER} {character} {END_SPEAKER}'
    if speaker_annotation_position == 'prefix':
        return f'{speaker_text} {text}'
    else:
        return f'{text} {speaker_text}'"
692,"    def mv(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:
        """"""
        Moves (renames) a source path supported by NativePathHandler to
        a destination path.

        Args:
            src_path (str): A URI supported by NativePathHandler
            dst_path (str): A URI supported by NativePathHandler

        Returns:
            status (bool): True on success
        Exception:
            Asserts if both the src and dest paths are not supported by
            NativePathHandler.
        """"""","
        # Moving across handlers is not supported.
        assert self.__get_path_handler(  # type: ignore
            src_path
        ) == self.__get_path_handler(
            dst_path
        ), ""Src and dest paths must be supported by the same path handler.""
        handler = self.__get_path_handler(src_path)
        bret = handler._mv(src_path, dst_path, **kwargs)
        kvs = {""op"": ""mv"", ""path"": src_path, ""dst_path"": dst_path}
        self.__log_tmetry_keys(handler, kvs)
        return bret"
693,"    def is_system_idle(
        self, tty_idle_timeout: datetime.timedelta, root_path: Path
    ) -> bool:
        """"""Return true if the system seems idle""""""",        raise NotImplementedError()
694,"    def reorder_decoder_incremental_state(
        self,
        incremental_state: Dict[int, Any],
        inds: Union[List[int], torch.LongTensor],
        decoder: RagDecoder,
    ) -> Dict[int, dict]:
        """"""
        Reorder the decoder incremental state, for incremental decoding.

        See ``TorchGeneratorModel.reorder_decoder_incremental_state`` for a description.

        Each RagModelType will require specialized reordering, depending on the method used.
        """"""",
695,"    def addSuccess(self, test, details=None):
        """"""Report a success in a test.""""""","        self._addOutcome(""successful"", test, details=details, error_permitted=False)"
696,"def showgraphwidth(repo, ctx, templ, **args):
    """"""Integer. The width of the graph drawn by 'log --graph' or zero.""""""","    # The value args['graphwidth'] will be this function, so we use an internal
    # name to pass the value through props into this function.
    return args.get(""_graphwidth"", 0)"
697,"    def knowledge_graph_context(self, example_state: Dict) -> str:
        """"""
        Generate the context for the flattened knowledge graph.
        """"""","        knowledge_graph = self.extract_knowledge_graph_str(example_state)
        return wrap_content(knowledge_graph, consts.KNOWLEDGE_GRAPH)"
698,"    def done(self):
        """"""Returns true if the response body is entirely read.""""""",        return self._finished
699,"def _graft(op, rev, mapping, lastdestnode, getcommitdate):
    '''duplicate changeset ""rev"" with parents from ""mapping""'''","    repo = op.repo
    oldp1 = rev.p1().node()
    oldp2 = rev.p2().node()
    newp1 = mapping.get(oldp1, oldp1)
    newp2 = mapping.get(oldp2, oldp2)

    m = _getmanifest(op, rev)

    def getfilectx(repo, memctx, path):
        if path in m:
            # We can't use the normal rev[path] accessor here since it will try
            # to go through the flat manifest, which may not exist.
            # That is, fctx.flags() might fail. Therefore use m.flags.
            flags = m.flags(path)
            fctx = _getfilectx(rev, m, path)
            return context.overlayfilectx(fctx, ctx=memctx, flags=flags)
        else:
            return None

    # If the incoming commit has no parents, but requested a rebase,
    # allow it only for the first commit. The null/null commit will always
    # be the first commit since we only allow a nullid->nonnullid mapping if the
    # incoming commits are a completely distinct history (see `sharedparents` in
    # getrevs()), so there's no risk of commits with a single null parent
    # accidentally getting translated first.
    if oldp1 == nullid and oldp2 == nullid:
        if newp1 != nullid:
            newp2 = nullid
            del mapping[nullid]

    if oldp1 != nullid and oldp2 != nullid:
        # The way commits work is they copy p1, then apply the necessary changes
        # to get to the new state. In a pushrebase situation, we are applying
        # changes from the pre-rebase commit to a post-rebase commit, which
        # means we need to ensure that changes caused by the rebase are
        # preserved. In a merge commit, if p2 is the post-rebase commit that
        # contains all the files from the rebase destination, those changes will
        # be lost, since the newp1 doesn't have those changes, and
        # oldp1.diff(oldrev) doesn't have them either. The solution is to ensure
        # that the parent that contains all the original rebase destination
        # files is always p1. We do that by just swapping them here.
        if newp2 == lastdestnode:
            newtemp = newp1
            oldtemp = oldp1
            oldp1 = oldp2
            oldp2 = oldtemp
            newp1 = newp2
            newp2 = newtemp

        # If it's a merge commit, Mercurial's rev.files() only returns the files
        # that are different from both p1 and p2, so it would not capture all of
        # the incoming changes from p2 (for instance, new files in p2). The fix
        # is to manually diff the rev manifest and it's p1 to get the list of
        # files that have changed. We only need to diff against p1, and not p2,
        # because Mercurial constructs new commits by applying our specified
        # files on top of a copy of the p1 manifest, so we only need the diff
        # against p1.
        bundlerepo = rev._repo
        files = _getmanifest(op, rev).diff(_getmanifest(op, bundlerepo[oldp1])).keys()
    else:
        files = rev.files()

    date = getcommitdate(repo.ui, rev.hex(), rev.date())

    extra = rev.extra().copy()
    mutinfo = mutation.record(repo, extra, [rev.node()], ""pushrebase"")
    loginfo = {""predecessors"": rev.hex(), ""mutation"": ""pushrebase""}

    return _commit(
        repo,
        [repo[newp1], repo[newp2]],
        rev.description(),
        files,
        getfilectx,
        rev.user(),
        date,
        extra,
        loginfo,
        mutinfo,
    )"
700,"    def basename(self, path: str) -> str:
        """"""return base element of a path (as os.path.basename would do)

        This exists to allow handling of strange encoding if needed.""""""",        return os.path.basename(path)
701,"def trackrevnumfortests(repo, specs):
    """"""Attempt to collect information to replace revision number with revset
    expressions in tests.

    This works with the TESTFILE and TESTLINE environment variable set by
    run-tests.py.

    Information will be written to $TESTDIR/.testrevnum.
    """"""","    if not util.istest():
        return

    trackrevnum = encoding.environ.get(""TRACKREVNUM"")
    testline = encoding.environ.get(""TESTLINE"")
    testfile = encoding.environ.get(""TESTFILE"")
    testdir = encoding.environ.get(""TESTDIR"")
    if not trackrevnum or not testline or not testfile or not testdir:
        return

    for spec in specs:
        # 'spec' should be in sys.argv
        if not any(spec in a for a in pycompat.sysargv):
            continue
        # Consider 'spec' as a revision number.
        rev = int(spec)
        if rev < -1:
            continue
        ctx = repo[rev]
        if not ctx:
            return

        # Check candidate revset expressions.
        candidates = []
        if rev == -1:
            candidates.append(""null"")
        desc = ctx.description()
        if desc:
            candidates.append(""desc(%s)"" % desc.split()[0])
            candidates.append(""max(desc(%s))"" % desc.split()[0])
        candidates.append(""%s"" % ctx.hex())

        for candidate in candidates:
            try:
                nodes = list(repo.nodes(candidate))
            except Exception:
                continue
            if nodes == [ctx.node()]:
                with open(testdir + ""/.testrevnum"", ""ab"") as f:
                    f.write(
                        ""fix(%r, %s, %r, %r)\n"" % (testfile, testline, spec, candidate)
                    )
                break"
702,"    def contains(self, item, prereleases=None):
        """"""
        Determines if the given item is contained within this specifier.
        """"""",
703,"        def __setattr__(self, name, value):
            """"""mimics the read-only attributes of Python file objects
            by raising 'TypeError: readonly attribute' if someone tries:
              f = posixfile('foo.txt')
              f.name = 'bla'""""""","            return self._file.__setattr__(name, value)"
704,"def parse_expr(expr_text, valid_variables):
    """"""parses the simple criteria expression syntax used in
    dependency specifications.
    Returns an ExprNode instance that can be evaluated like this:

    ```
    expr = parse_expr(""os=windows"")
    ok = expr.eval({
        ""os"": ""windows""
    })
    ```

    Whitespace is allowed between tokens.  The following terms
    are recognized:

    KEY = VALUE   # Evaluates to True if ctx[KEY] == VALUE
    not(EXPR)     # Evaluates to True if EXPR evaluates to False
                  # and vice versa
    all(EXPR1, EXPR2, ...) # Evaluates True if all of the supplied
                           # EXPR's also evaluate True
    any(EXPR1, EXPR2, ...) # Evaluates True if any of the supplied
                           # EXPR's also evaluate True, False if
                           # none of them evaluated true.
    """"""","
    p = Parser(expr_text, valid_variables)
    return p.parse()"
705,"    def get_image_features_path(self, task, image_model_name, dt):
        """"""
        Override so that subclasses can see same image features.
        """"""","        # In default implementation, self.data_path already has task name added
        image_features_path = os.path.join(self.data_path, 'image_features')

        if not os.path.isdir(image_features_path):
            PathManager.mkdirs(image_features_path)

        return os.path.join(
            image_features_path, f'{image_model_name}_{dt}_features_dict'
        )"
706,"def distributed_context(
    rank, opt, rank_offset=0, gpu=None, init_method=""tcp://localhost:61337""
):
    """"""
    A context which wraps initialization of a distributed/multiprocessing run.

    Every process in the distributed run should launch with this. In true
    distributed setting you may wish to use slurm_distributed_context instead.

    :param int rank:
        This process's rank, less rank_offset.
    :param int rank_offset:
        Used as an offset of rank. Used between multiprocessing vs true distributed,
        and a hack around torch.multiprocessing.spawn being only used for the
        non-primary workers.
    :param opt:
        command line options
        distributed training setups on the same machine.
    :param int gpu:
        Which GPU to use. Defaults to using rank and local devices, but must be
        manually specified when using many-hosts.
    :param str init method:
        Init method, such as ``tcp://localhost:61337``. See torch.distributed docs.
    """"""","    # Set per-host options
    opt = copy.deepcopy(opt)
    # we need to manually adjust the rank differently in multiprocessing
    # and distributed train
    rank = rank + rank_offset
    opt['rank'] = rank
    if gpu is None:
        # default assumption is local GPUs
        gpu = rank % torch.cuda.device_count()
    opt['gpu'] = gpu
    # make sure we don't just use whatever GPU was saved in the model file
    if 'override' not in opt:
        opt['override'] = {}
    opt['override']['gpu'] = gpu

    # Suppress output of workers except the main host.
    if opt.get('verbose') or rank != 0:
        print_prefix = 'rank:{:3d} |'.format(rank)
    else:
        print_prefix = None
    suppress_output = not opt.get('verbose') and rank != 0

    with override_print(suppress_output, print_prefix):
        # perform distributed setup, ensuring all hosts are ready
        if opt['gpu'] != -1:
            torch.cuda.set_device(opt['gpu'])
        dist.init_process_group(
            backend=""nccl"",
            init_method=init_method,
            world_size=opt['distributed_world_size'],
            rank=rank,
        )
        logging.info(""Distributed group initialized"")

        # manual_seed can be a noop without this
        torch.cuda.init()
        # make sure all parameters will be in sync
        torch.manual_seed(42)
        # force a sync so that no one gets ahead, and all are seeded together
        sync_object(None)

        try:
            yield opt
        finally:
            dist.destroy_process_group()"
707,"def video_list_from_file(video_list_fpath: str, base_path: Optional[str] = None):
    """"""
    Create a list of paths to video files from a text file.

    Args:
        video_list_fpath (str): path to a plain text file with the list of videos
        base_path (str): base path for entries from the video list (default: None)
    """"""","    video_list = []
    with PathManager.open(video_list_fpath, ""r"") as io:
        for line in io:
            video_list.append(maybe_prepend_base_path(base_path, str(line.strip())))
    return video_list"
708,"    def build_model(self, states=None):
        """"""
        Build and return model.
        """"""","        return HFGPT2Model(self.opt, self.dict)"
709,"    def dirfoldmap(self):
        """"""
        Returns a dictionary mapping normalized case paths to their
        non-normalized versions for directories.
        """"""","
        def lookup(key):
            d = self.getcasefoldedtracked(key + ""/"", util.normcase)
            if d is not None and self._rmap.hastrackeddir(d):
                return d.rstrip(""/"")
            else:
                return None

        return treestate._overlaydict(lookup)"
710,"def cloudrejoin(ui, repo, **opts):
    """"""reconnect the local repository to commit cloud

    If the local repository is not connected to commit cloud, attempt to connect
    it.  If the repository cannot be connected, then display a message
    describing how to connect to commit cloud.

    If connection is successful, then commits and bookmarks will be synchronized
    between all repositories that have been connected to the same commit cloud workspace.

    Use `@prog@ cloud sync` to trigger a new synchronization.
    """"""","    if workspace.currentworkspace(repo):
        return

    active = []
    try:
        workspacename = workspace.parseworkspace(ui, opts)
        if workspacename is None:
            # If the workspace name is not given, figure out the sensible default.
            # The specific hostname workspace will be preferred over the default workspace.
            reponame = ccutil.getreponame(repo)
            hostnameworkspace = workspace.hostnameworkspace(ui)
            winfos = service.get(ui).getworkspaces(
                reponame, workspace.userworkspaceprefix(ui)
            )

            active = [winfo for winfo in winfos if not winfo.archived]

            if winfos and any([winfo.name == hostnameworkspace for winfo in active]):
                workspacename = hostnameworkspace
            else:
                workspacename = workspace.defaultworkspace(ui)

        ui.status(
            _(""attempting to connect to the '%s' workspace for the '%s' repo\n"")
            % (workspacename, ccutil.getreponame(repo)),
            component=""commitcloud"",
        )

        # update the raw_workspace option as workspacename has been already parsed
        for opt in workspace.workspaceopts:
            opts.pop(opt[1], None)
        opts.update({""raw_workspace"": workspacename})
        cloudjoin(ui, repo, **opts)

    except ccerror.RegistrationError:
        ui.status(
            _(""unable to connect: not authenticated with Commit Cloud on this host\n""),
            component=""commitcloud"",
        )
        educationpage = ui.config(""commitcloud"", ""education_page"")
        if educationpage:
            ui.status(_(""learn more about Commit Cloud at %s\n"") % educationpage)

    else:
        # provide a hint if several alternatives have been available
        if len(active) > 1:
            hintutil.trigger(""commitcloud-switch"", ui, active)"
711,"def lr_func_steps_with_lrs(cur_iter):
    """"""For cfg.SOLVER.LR_POLICY = 'steps_with_lrs'

    Change the learning rate to specified values at specified iterations.

    Example:
    cfg.SOLVER.MAX_ITER: 90
    cfg.SOLVER.STEPS:    [0,    60,    80]
    cfg.SOLVER.LRS:      [0.02, 0.002, 0.0002]
    for cur_iter in [0, 59]   use 0.02
                 in [60, 79]  use 0.002
                 in [80, inf] use 0.0002
    """"""","    ind = get_step_index(cur_iter)
    return cfg.SOLVER.LRS[ind]"
712,"    def _makesmartloginfo(data):
        """"""Returns a SmartlogInfo that supports DAG operations like heads, parents,
        roots, ancestors, descendants, etc.
        """"""","        nodeinfos = _makenodes(data)
        version = data.get(""version"")
        timestamp = data.get(""timestamp"")

        public = _getpublic(nodeinfos)

        # Sort public by date. Connect them. Assume they form a linear history.
        # XXX: This can be incorrect if public history is not linear or not
        # sorted by date. However, nodeinfos only have limited information and
        # sort by date is the best effort we can do here.
        public.sort(key=lambda node: (nodeinfos[node].date, node), reverse=True)

        # {node: [parentnode]}
        publicparents = {node: public[i + 1 : i + 2] for i, node in enumerate(public)}

        def getparents(node):
            parents = publicparents.get(node)
            if parents is None:
                parents = [p for p in nodeinfos[node].parents if p in nodeinfos]
            return parents

        dag = bindings.dag.commits.openmemory()
        commits = [(node, getparents(node), b"""") for node in sorted(nodeinfos.keys())]
        dag.addcommits(commits)
        dag = dag.dagalgo()
        return SmartlogInfo(
            dag=dag,
            public=public,
            draft=list(dag.all() - public),
            nodeinfos=nodeinfos,
            version=version,
            timestamp=timestamp,
        )"
713,"    def __init__(self, *args, **kwargs):
        """"""
        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:

        Args:
            norm (nn.Module, optional): a normalization layer
            activation (callable(Tensor) -> Tensor): a callable activation function

        It assumes that norm layer is used before activation.
        """"""","        norm = kwargs.pop(""norm"", None)
        activation = kwargs.pop(""activation"", None)
        super().__init__(*args, **kwargs)

        self.norm = norm
        self.activation = activation"
714,"  def __init__(self, options):
    """"""
    __init__(self):

    Instantiate class variables
    """"""","
    # TODO reseach decorators for logging and configs
    if not options:
      options = config.get_options('default', {})
    self.options = options
    self.logger = logger.get_logger()
    self.paths = utils.get_paths()
    self.git_path = self.paths['repo_root']"
715,"def checkvers(name, desc, vers):
    """"""Registers a check function for each of a series of versions.

    vers can be a list or an iterator""""""","
    def decorator(func):
        def funcv(v):
            def f():
                return func(v)

            return f

        for v in vers:
            v = str(v)
            f = funcv(v)
            checks[""%s%s"" % (name, v.replace(""."", """"))] = (f, desc % v)
        return func

    return decorator"
716,"    def test_label(self, path, k=1, threshold=0.0):
        """"""
        Return the precision and recall score for each label.

        The returned value is a dictionary, where the key is the label.
        For example:
        f.test_label(...)
        {'__label__italian-cuisine' : {'precision' : 0.7, 'recall' : 0.74}}
        """"""","        return self.f.testLabel(path, k, threshold)"
717,"    def is_scalar_type(type):
        """"""Checks whether a type is a scalar type.
        A type is a scalar type of its type is
            gdb.TYPE_CODE_CHAR or
            gdb.TYPE_CODE_INT or
            gdb.TYPE_CODE_BOOL or
            gdb.TYPE_CODE_FLT or
            gdb.TYPE_CODE_VOID or
            gdb.TYPE_CODE_ENUM.

        Arguments:
            type: The type to be checked.

        Returns:
            'True' if 'type' is a scalar type. 'False' otherwise.
        """"""",        return type.code in Explorer._SCALAR_TYPE_LIST
718,"    def guess_split_size(item: Chunk, num_gpus: Optional[int] = None, dim=0) -> int:
        """"""
        Estimate the number of chunks we should split the batch into via heuristics.
        """"""","        if num_gpus is None:
            num_gpus = torch.cuda.device_count()  # type: ignore

        if isinstance(item, torch.Tensor):
            if num_gpus == 1:
                # no point in chunking if we're not really doing model parallel
                return item.size(dim)
            # heuristic: use the same number of chunks as 2 * num_gpus.  this
            # isn't perfect (it ideally would be tuned differently for every model
            # and number of GPUs), but it seems to work reasonably wellenough in several
            # architectures tested.
            return max(1, item.size(dim) // int(num_gpus * 2))
        elif isinstance(item, tuple):
            return PipelineHelper.guess_split_size(item[0], num_gpus)
        elif isinstance(item, dict):
            return PipelineHelper.guess_split_size(list(item.values())[0], num_gpus)
        raise TypeError(f'Cannot determine split size for {type(item)}')"
719,"    def test_cloned_testcase_does_not_share_details(self):
        """"""A cloned TestCase does not share the details dict.""""""","        class Test(TestCase):
            def test_foo(self):
                self.addDetail(
                    'foo', content.Content('text/plain', lambda: 'foo'))
        orig_test = Test('test_foo')
        cloned_test = clone_test_with_new_id(orig_test, self.getUniqueString())
        orig_test.run(unittest.TestResult())
        self.assertEqual('foo', orig_test.getDetails()['foo'].iter_bytes())
        self.assertEqual(None, cloned_test.getDetails().get('foo'))"
720,"def add_training_inputs(model, roidb=None):
    """"""Create network input ops and blobs used for training. To be called
    *after* model_builder.create().
    """"""","    # Implementation notes:
    #   Typically, one would create the input ops and then the rest of the net.
    #   However, creating the input ops depends on loading the dataset, which
    #   can take a few minutes for COCO.
    #   We prefer to avoid waiting so debugging can fail fast.
    #   Thus, we create the net *without input ops* prior to loading the
    #   dataset, and then add the input ops after loading the dataset.
    #   Since we defer input op creation, we need to do a little bit of surgery
    #   to place the input ops at the start of the network op list.
    assert model.train, 'Training inputs can only be added to a trainable model'
    if roidb is not None:
        # To make debugging easier you can set cfg.DATA_LOADER.NUM_THREADS = 1
        model.roi_data_loader = RoIDataLoader(
            roidb,
            num_loaders=cfg.DATA_LOADER.NUM_THREADS,
            minibatch_queue_size=cfg.DATA_LOADER.MINIBATCH_QUEUE_SIZE,
            blobs_queue_capacity=cfg.DATA_LOADER.BLOBS_QUEUE_CAPACITY
        )
    orig_num_op = len(model.net._net.op)
    blob_names = roi_data_minibatch.get_minibatch_blob_names(is_training=True)
    for gpu_id in range(cfg.NUM_GPUS):
        with c2_utils.NamedCudaScope(gpu_id):
            for blob_name in blob_names:
                workspace.CreateBlob(core.ScopedName(blob_name))
            model.net.DequeueBlobs(
                model.roi_data_loader._blobs_queue_name, blob_names
            )
    # A little op surgery to move input ops to the start of the net
    diff = len(model.net._net.op) - orig_num_op
    new_op = model.net._net.op[-diff:] + model.net._net.op[:-diff]
    del model.net._net.op[:]
    model.net._net.op.extend(new_op)"
721,"    def test_l1_l2_sizesplit_bothget(self):
        """"""
        Basic functionality. Allow full setst to both pools.
        """"""","        mcr = self.get_mcrouter(self.config_bothset)

        self.assertFalse(mcr.get(""key1""))

        # small key should only exist in L1
        mcr.set(""key1"", ""value1"")

        # small key should be normal value in L1
        self.assertEqual(self.l1.get(""key1""), ""value1"")
        # small key shouldn't be in L2
        self.assertFalse(self.l2.get(""key1""), ""value1"")

        # perform a get and check the response
        self.assertEqual(mcr.get(""key1""), ""value1"")

        # key should end up split. end up in both pools.
        value2 = ""foo"" * 200
        mcr.set(""key2"", value2)
        # The write to L2 is async and we're checking it right away.
        time.sleep(1)

        self.assertEqual(self.l1.get(""key2""), value2)
        self.assertEqual(self.l2.get(""key2""), value2)
        self.assertEqual(mcr.get(""key2""), value2)"
722,"    def _build_rpa_episodes(self, ep: List[Message]) -> List[Message]:
        """"""
        Construct new episodes from old, LIGHT ones.

        enumerate over all possible start and label positions

        :param ep:
            episode to explode and build into new eps

        :return episodes:
            return a list of episodes after enumerating over possible labels.
        """"""","        episodes = []
        context, characters, utterances = self._explode_episode(
            ep, self.exclude_from_context, self.use_speech_prefix
        )
        candidates = (
            self.candidates
            if self.inline_candidate_type == 'all'
            else list(characters.values())
        )

        # determine initial start and end indices
        num_utts = self.num_utterances
        if num_utts < 0:
            num_utts = len(utterances) - 1

        start_idx, end_idx = (0, num_utts - 1)

        # Enumerate over all possible start, end positions
        while end_idx < len(utterances) - 1:
            # Step 0: (maybe) annotate the prior utterances of dialogue
            prev_utts = [
                maybe_annotate(
                    *utt[:-1],
                    self.annotate_speaker,
                    self.speaker_separator,
                    self.speaker_annotation_position,
                )
                for utt in utterances[start_idx:end_idx]
            ]
            if self.include_light_context:
                prev_utts = [context] + prev_utts

            # Step 1: enumerate over each successive utterance
            for speaker, label, listener in utterances[end_idx:]:
                # Step 2: determine the label control / task type
                if self.classifier_label_type == 'character':
                    speaker_label = (
                        speaker if self.speaker_label_type == 'speaker' else listener
                    )
                    label_control = (
                        WHO_AM_I
                        if self.speaker_label_type == 'speaker'
                        else WHO_ARE_YOU
                    )
                else:
                    speaker_label = (
                        SELF if speaker == characters['_self_name'] else PARTNER
                    )
                    label_control = WHO_IS_THIS
                # Step 3: Determine what label candidates to use
                if (
                    self.num_train_inline_candidates > 0
                    and DatatypeHelper.is_training(self.datatype)
                    and self.inline_candidate_type == 'all'
                ):
                    label_cands = [speaker, listener]
                    while speaker in label_cands and listener in label_cands:
                        label_cands = random.sample(
                            candidates, self.num_train_inline_candidates - 2
                        )
                    label_cands += [speaker, listener]
                    random.shuffle(label_cands)
                else:
                    label_cands = candidates

                # Step 4: Build the Message
                if self.left_to_right:
                    label_words = label.split(' ')
                    for i in range(1, len(label_words) + 1):
                        if self.delimit_label_control:
                            text = self.delimiter.join(
                                prev_utts + [label_control, ' '.join(label_words[:i])]
                            )
                        else:
                            text = self.delimiter.join(
                                prev_utts[:-1]
                                + [f""{prev_utts[-1]} {label_control}""]
                                + [' '.join(label_words[:i])]
                            )
                        message = Message(
                            {
                                'text': text,
                                'labels': [speaker_label],
                                'label_candidates': label_cands,
                                'episode_done': True,
                            }
                        )
                        episodes.append([message])
                else:
                    if self.delimit_label_control:
                        text = self.delimiter.join(prev_utts + [label_control, label])
                    else:
                        text = self.delimiter.join(
                            prev_utts[:-1]
                            + [f""{prev_utts[-1]} {label_control}""]
                            + [label]
                        )
                    message = Message(
                        {
                            'text': text,
                            'labels': [speaker_label],
                            'label_candidates': label_cands,
                            'episode_done': True,
                        }
                    )
                    episodes.append([message])

            if start_idx == end_idx:
                # edge case where num_utterances == 1
                break
            else:
                start_idx += 1
                end_idx += 1

        return episodes"
723,"    def findmissingrevs(self, common=None, heads=None):
        """"""Return the revision numbers of the ancestors of heads that
        are not ancestors of common.

        More specifically, return a list of revision numbers corresponding to
        nodes N such that every N satisfies the following constraints:

          1. N is an ancestor of some node in 'heads'
          2. N is not an ancestor of any node in 'common'

        The list is sorted by revision number, meaning it is
        topologically sorted.

        'heads' and 'common' are both lists of revision numbers.  If heads is
        not supplied, uses all of the revlog's heads.  If common is not
        supplied, uses nullid.""""""","        if common is None:
            common = [nullrev]
        if heads is None:
            heads = self.headrevs()

        inc = self.incrementalmissingrevs(common=common)
        return inc.missingancestors(heads)"
724,"def benchmarkrevsets(ui, repo, *args, **opts):
    """"""benchmark revsets

    Runs simple benchmark on revsets.
    Benchmarks run 3 to 30 times. The fastest wall clock time is picked.
    """"""","    cl = repo.changelog

    # Backends to test
    origbackend = cl.algorithmbackend
    if opts.get(""multi_backend""):
        backends = [""segments"", ""revlog"", ""revlog-cpy""]
    else:
        backends = [origbackend]

    # Prepare revsets
    xname = opts.get(""rev_x"") or ""min(bookmark())""
    yname = opts.get(""rev_y"") or ""max(bookmark())""
    xnode = scmutil.revsingle(repo, xname).node()
    ynode = scmutil.revsingle(repo, yname).node()
    alias = {""x"": hex(xnode), ""y"": hex(ynode)}
    ui.write(_(""# x:  %s  (%s)\n"") % (hex(xnode), xname))
    ui.write(_(""# y:  %s  (%s)\n\n"") % (hex(ynode), yname))

    specs = opts.get(""expr"") or []
    if opts.get(""default""):
        specs += [
            ""ancestor(x, x)"",
            ""ancestor(x, y)"",
            ""ancestors(x)"",
            ""ancestors(y)"",
            ""children(x)"",
            ""children(y)"",
            ""descendants(x)"",
            ""descendants(y)"",
            ""y % x"",
            ""x::y"",
            ""heads(_all())"",
            ""roots(_all())"",
        ]

    # Result table: [(name, *backendresult)]
    table = [[""revset \\ backend""] + backends] + [
        [spec] + [""""] * len(backends) for spec in specs
    ]

    try:
        dynamic = dynamiccontent()
        for backendindex, backend in enumerate(backends):
            migrate(repo, backend)
            # invalidate phase cache
            repo.invalidatechangelog()
            for specindex, spec in enumerate(specs):
                # parse the revset expression once
                m = revset.matchany(None, [spec], localalias=alias)
                # execute the revset multiple times
                seconds = bench(lambda: len(m(repo)))
                table[specindex + 1][backendindex + 1] = descseconds(seconds)
                rendered = rendertable(table)
                dynamic.render(rendered, ui.write)
    finally:
        migrate(repo, origbackend)
    return 0"
725,"def _convertoutputs(repo, annotated, contents):
    """"""convert fastannotate outputs to vanilla annotate format""""""","    # fastannotate returns: [(nodeid, linenum, path)], [linecontent]
    # convert to what fctx.annotate returns: [((fctx, linenum), linecontent)]
    results = []
    fctxmap = {}
    annotateline = getattr(hgcontext, ""annotateline"", None)
    for i, (hsh, linenum, path) in enumerate(annotated):
        if (hsh, path) not in fctxmap:
            fctxmap[(hsh, path)] = _lazyfctx(repo, hsh, path)
        # linenum: the user wants 1-based, we have 0-based.
        lineno = linenum + 1
        fctx = fctxmap[(hsh, path)]
        line = contents[i]
        if annotateline is None:
            results.append(((fctx, lineno), line))
        else:
            # 2e32c6a31cc7 introduced annotateline
            results.append((annotateline(fctx=fctx, lineno=lineno), line))
    return results"
726,"def sec_to_min_pretty(time_secs: int) -> str:
    """"""
    Returns formatted string for converting secs to mins.
    """"""","    if time_secs % 60 == 0:
        return f'{time_secs // 60}'
    m = time_secs / 60
    return f'{m:.2g}'"
727,"        def keep_edge(edge, text_tokens):
            """"""
            Returns false for edges that can NOT be readily inferred from the context.

            Edges that are referring to objects not referenced in the location
            description, except player (you). Thus, we drop these edges. Also, for some
            teachers (eg, ActionKGTeacher) the model can see user inventory before, and
            after the action, and they are important in result of action,. Thus, we keep
            < you , have, X > edges as well.
            """"""","            # Each graph edge is a tuple: (subject, relation, object)
            sub, rel, obj = [s.strip().lower() for s in edge]

            if sub == 'you':
                # player location
                if rel == 'in':
                    return True
                # user inventory object
                elif rel == 'have' and self._keep_inv_during_kg_prune:
                    return True

            return has_word_overlap(sub, text_tokens) and has_word_overlap(
                obj, text_tokens
            )"
728,"    def __init__(self, trans, strictRead=False, client_types=None, client_type=None):
        """"""Create a THeaderProtocol instance

        @param transport(TTransport) The underlying transport.
        @param strictRead(bool) Turn on strictRead if using TBinaryProtocol
        @param client_types([CLIENT_TYPE.HEADER, ...])
                   List of client types to support.  Defaults to
                   CLIENT_TYPE.HEADER only.
        """"""","
        if isinstance(trans, THeaderTransport):
            trans._THeaderTransport__supported_client_types = set(
                client_types or (CLIENT_TYPE.HEADER,)
            )
            if client_type is not None:
                trans._THeaderTransport__client_type = client_type
            htrans = trans
        else:
            htrans = THeaderTransport(trans, client_types, client_type)
        TProtocolBase.__init__(self, htrans)
        self.strictRead = strictRead
        self.reset_protocol()"
729,"def simplifyinfixops(tree, targetnodes):
    """"""Flatten chained infix operations to reduce usage of Python stack

    >>> from . import pycompat
    >>> def f(tree):
    ...     s = prettyformat(simplifyinfixops(tree, ('or',)), ('symbol',))
    ...     print(s)
    >>> f(('or',
    ...     ('or',
    ...       ('symbol', '1'),
    ...       ('symbol', '2')),
    ...     ('symbol', '3')))
    (or
      (symbol '1')
      (symbol '2')
      (symbol '3'))
    >>> f(('func',
    ...     ('symbol', 'p1'),
    ...     ('or',
    ...       ('or',
    ...         ('func',
    ...           ('symbol', 'sort'),
    ...           ('list',
    ...             ('or',
    ...               ('or',
    ...                 ('symbol', '1'),
    ...                 ('symbol', '2')),
    ...               ('symbol', '3')),
    ...             ('negate',
    ...               ('symbol', 'rev')))),
    ...         ('and',
    ...           ('symbol', '4'),
    ...           ('group',
    ...             ('or',
    ...               ('or',
    ...                 ('symbol', '5'),
    ...                 ('symbol', '6')),
    ...               ('symbol', '7'))))),
    ...       ('symbol', '8'))))
    (func
      (symbol 'p1')
      (or
        (func
          (symbol 'sort')
          (list
            (or
              (symbol '1')
              (symbol '2')
              (symbol '3'))
            (negate
              (symbol 'rev'))))
        (and
          (symbol '4')
          (group
            (or
              (symbol '5')
              (symbol '6')
              (symbol '7'))))
        (symbol '8')))
    """"""","    if not isinstance(tree, tuple):
        return tree
    op = tree[0]
    if op not in targetnodes:
        return (op,) + tuple(simplifyinfixops(x, targetnodes) for x in tree[1:])

    # walk down left nodes taking each right node. no recursion to left nodes
    # because infix operators are left-associative, i.e. left tree is deep.
    # e.g. '1 + 2 + 3' -> (+ (+ 1 2) 3) -> (+ 1 2 3)
    simplified = []
    x = tree
    while x[0] == op:
        l, r = x[1:]
        simplified.append(simplifyinfixops(r, targetnodes))
        x = l
    simplified.append(simplifyinfixops(x, targetnodes))
    simplified.append(op)
    return tuple(reversed(simplified))"
730,"    def get_reranker_opts(self, opt: Opt) -> Dict[str, Any]:
        """"""
        Provide options used when building the rerankers.

        Base class ensures that various optimizations (cuda, fp16, parallel)
        are accounted for.

        :param opt:
            base opt

        :return options_dict:
            return a dictionary mapping options to values.
        """"""","        return {
            'no_cuda': opt['no_cuda'],
            'fp16': opt['fp16'],
            'model_parallel': opt['model_parallel'],
            'data_parallel': opt['data_parallel'],
        }"
731,"def _moveto(repo, bookmark, ctx, clean=False):
    """"""Moves the given bookmark and the working copy to the given revision.
    By default it does not overwrite the working copy contents unless clean is
    True.

    Assumes the wlock is already taken.
    """"""","    # Move working copy over
    if clean:
        merge.update(
            repo,
            ctx.node(),
            False,  # not a branchmerge
            True,  # force overwriting files
            None,
        )  # not a partial update
    else:
        # Mark any files that are different between the two as normal-lookup
        # so they show up correctly in hg status afterwards.
        wctx = repo[None]
        m1 = wctx.manifest()
        m2 = ctx.manifest()
        diff = m1.diff(m2)

        changedfiles = []
        changedfiles.extend(pycompat.iterkeys(diff))

        dirstate = repo.dirstate
        dirchanges = [f for f in dirstate if dirstate[f] != ""n""]
        changedfiles.extend(dirchanges)

        if changedfiles or ctx.node() != repo["".""].node():
            with dirstate.parentchange():
                dirstate.rebuild(ctx.node(), m2, changedfiles)

    # Move bookmark over
    if bookmark:
        lock = tr = None
        try:
            lock = repo.lock()
            tr = repo.transaction(""reset"")
            changes = [(bookmark, ctx.node())]
            repo._bookmarks.applychanges(repo, tr, changes)
            tr.close()
        finally:
            lockmod.release(lock, tr)"
732,"    def _install_builtins(self, namespace, force_native_rules=False):
        """"""
        Installs the build functions, by their name, into the given namespace.
        """"""","
        for name, function in iteritems(self._global_functions):
            namespace[name] = function.invoke
        if not self._disable_implicit_native_rules or force_native_rules:
            for name, function in iteritems(self._native_functions):
                namespace[name] = function.invoke"
733,"    def _grid_anchors(self, grid_sizes: List[List[int]]):
        """"""
        Returns:
            list[Tensor]: #featuremap tensors, each is (#locations x #cell_anchors) x 4
        """"""","        anchors = []
        # buffers() not supported by torchscript. use named_buffers() instead
        buffers: List[torch.Tensor] = [x[1] for x in self.cell_anchors.named_buffers()]
        for size, stride, base_anchors in zip(grid_sizes, self.strides, buffers):
            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, base_anchors)
            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)

            anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))

        return anchors"
734,"    def from_roi_masks(roi_masks: ""ROIMasks"", height: int, width: int) -> ""BitMasks"":
        """"""
        Args:
            roi_masks:
            height, width (int):
        """"""","        return roi_masks.to_bitmasks(height, width)"
735,"    def add_eat_event(
        self,
        name: str,
        reward=1,
        repeatable=False,
        terminal_required=True,
        terminal_sufficient=False,
    ):
        """"""Add an event which is triggered when `name` is eaten.

        Args:
            name (str):
                The name of the object being eaten.
            reward (float):
                The reward for this event. Defaults to 1.
            repeatable (bool):
                Whether this event can be triggered multiple times. Defaults to
                False.
            terminal_required (bool):
                Whether this event is required for termination. Defaults to
                True.
            terminal_sufficient (bool):
                Whether this event is sufficient for termination. Defaults to
                False.
        """"""","        msgs = [
            f""This {name} is delicious"",
            ""Blecch!  Rotten food!"",
            ""last bite of your meal"",
        ]
        if name == ""apple"":
            msgs.append(""Delicious!  Must be a Macintosh!"")
            msgs.append(""Core dumped."")
        if name == ""pear"":
            msgs.append(""Core dumped."")

        self._add_message_event(
            msgs, reward, repeatable, terminal_required, terminal_sufficient
        )"
736,"  def UpdatePreprocessor(self, line):
    """"""Update preprocessor stack.

    We need to handle preprocessors due to classes like this:
      #ifdef SWIG
      struct ResultDetailsPageElementExtensionPoint {
      #else
      struct ResultDetailsPageElementExtensionPoint : public Extension {
      #endif
    (see http://go/qwddn for original example)

    We make the following assumptions (good enough for most files):
    - Preprocessor condition evaluates to true from #if up to first
      #else/#elif/#endif.

    - Preprocessor condition evaluates to false from #else/#elif up
      to #endif.  We still perform lint checks on these lines, but
      these do not affect nesting stack.

    Args:
      line: current line to check.
    """"""","    if Match(r'^\s*#\s*(if|ifdef|ifndef)\b', line):
      # Beginning of #if block, save the nesting stack here.  The saved
      # stack will allow us to restore the parsing state in the #else case.
      self.pp_stack.append(_PreprocessorInfo(copy.deepcopy(self.stack)))
    elif Match(r'^\s*#\s*(else|elif)\b', line):
      # Beginning of #else block
      if self.pp_stack:
        if not self.pp_stack[-1].seen_else:
          # This is the first #else or #elif block.  Remember the
          # whole nesting stack up to this point.  This is what we
          # keep after the #endif.
          self.pp_stack[-1].seen_else = True
          self.pp_stack[-1].stack_before_else = copy.deepcopy(self.stack)

        # Restore the stack to how it was before the #if
        self.stack = copy.deepcopy(self.pp_stack[-1].stack_before_if)
      else:
        # TODO(unknown): unexpected #else, issue warning?
        pass
    elif Match(r'^\s*#\s*endif\b', line):
      # End of #if or #else blocks.
      if self.pp_stack:
        # If we saw an #else, we will need to restore the nesting
        # stack to its former state before the #else, otherwise we
        # will just continue from where we left off.
        if self.pp_stack[-1].seen_else:
          # Here we can just use a shallow copy since we are the last
          # reference to it.
          self.stack = self.pp_stack[-1].stack_before_else
        # Drop the corresponding #if
        self.pp_stack.pop()
      else:
        # TODO(unknown): unexpected #endif, issue warning?
        pass"
737,"    def state_dict(self):
        """"""
        Hooks are stateless by default, but can be made checkpointable by
        implementing `state_dict` and `load_state_dict`.
        """"""",        return {}
738,"    def test_perplexities(self):
        """"""
        Test perplexities of reduced-bias models in the zoo.
        """"""","        test_cases = [
            ('gender__name_scrambling', 'transformer/generator', 22.91),
            (
                'gender__ctrl_gen_tokens',
                'projects.dialogue_bias.agents:NoBiasStyleGenAgent',
                22.61,
            ),
            ('gender__unlikelihood_sequence_level', 'transformer/generator', 11.44),
            ('gender_ethnicity__name_scrambling', 'transformer/generator', 19.57),
        ]
        # Perplexities are high because models were tuned on conversations starting with
        # ""Hi! My name is ___.""
        for model_name, model, desired_ppl in test_cases:
            _, test = testing_utils.eval_model(
                opt={
                    'batchsize': 4,
                    'beam_block_full_context': True,
                    'fp16': True,
                    'num_examples': 16,
                    'model': model,
                    'model_file': f'zoo:dialogue_bias/{model_name}/model',
                    'skip_generation': True,
                    'task': 'blended_skill_talk',
                },
                skip_valid=True,
            )
            self.assertAlmostEqual(test['ppl'], desired_ppl, delta=0.05)"
739,"def overload(func):
    """"""Decorator for overloaded functions/methods.

    In a stub file, place two or more stub definitions for the same
    function in a row, each decorated with @overload.  For example:

      @overload
      def utf8(value: None) -> None: ...
      @overload
      def utf8(value: bytes) -> bytes: ...
      @overload
      def utf8(value: str) -> bytes: ...

    In a non-stub file (i.e. a regular .py file), do the same but
    follow it with an implementation.  The implementation should *not*
    be decorated with @overload.  For example:

      @overload
      def utf8(value: None) -> None: ...
      @overload
      def utf8(value: bytes) -> bytes: ...
      @overload
      def utf8(value: str) -> bytes: ...
      def utf8(value):
          # implementation goes here
    """"""",    return _overload_dummy
740,"def create_parser() -> argparse.ArgumentParser:
    """"""Returns a parser""""""","    parser = argparse.ArgumentParser(
        prog=""edenfsctl"", description=""Manage EdenFS checkouts.""
    )
    # TODO: We should probably rename this argument to --state-dir.
    # This directory contains materialized file state and the list of managed checkouts,
    # but doesn't really contain configuration.
    parser.add_argument(
        ""--config-dir"",
        help=""The path to the directory where EdenFS stores its internal state."",
    )
    parser.add_argument(
        ""--etc-eden-dir"",
        help=""Path to directory that holds the system configuration files."",
    )
    parser.add_argument(
        ""--home-dir"", help=""Path to directory where .edenrc config file is stored.""
    )
    parser.add_argument(""--checkout-dir"", help=argparse.SUPPRESS)
    parser.add_argument(
        ""--version"", ""-v"", action=""store_true"", help=""Print EdenFS version.""
    )
    parser.add_argument(
        ""--debug"",
        action=""store_true"",
        help=""Enable debug mode (more verbose logging, traceback, etc..)"",
    )
    parser.add_argument(
        ""--press-to-continue"",
        action=""store_true"",
        help=argparse.SUPPRESS,
    )

    subcmd_add_list: List[Type[Subcmd]] = [
        subcmd_mod.HelpCmd,
        stats_mod.StatsCmd,
        trace_mod.TraceCmd,
        redirect_mod.RedirectCmd,
        prefetch_mod.GlobCmd,
        prefetch_mod.PrefetchCmd,
        prefetch_profile_mod.PrefetchProfileCmd,
    ]

    subcmd_add_list.append(debug_mod.DebugCmd)

    subcmd_mod.add_subcommands(parser, subcmd.commands + subcmd_add_list)

    return parser"
741,"def box_voting(top_dets, all_dets, thresh, scoring_method='ID', beta=1.0):
    """"""Apply bounding-box voting to refine `top_dets` by voting with `all_dets`.
    See: https://arxiv.org/abs/1505.01749. Optional score averaging (not in the
    referenced  paper) can be applied by setting `scoring_method` appropriately.
    """"""","    # top_dets is [N, 5] each row is [x1 y1 x2 y2, sore]
    # all_dets is [N, 5] each row is [x1 y1 x2 y2, sore]
    top_dets_out = top_dets.copy()
    top_boxes = top_dets[:, :4]
    all_boxes = all_dets[:, :4]
    all_scores = all_dets[:, 4]
    top_to_all_overlaps = bbox_overlaps(top_boxes, all_boxes)
    for k in range(top_dets_out.shape[0]):
        inds_to_vote = np.where(top_to_all_overlaps[k] >= thresh)[0]
        boxes_to_vote = all_boxes[inds_to_vote, :]
        ws = all_scores[inds_to_vote]
        top_dets_out[k, :4] = np.average(boxes_to_vote, axis=0, weights=ws)
        if scoring_method == 'ID':
            # Identity, nothing to do
            pass
        elif scoring_method == 'TEMP_AVG':
            # Average probabilities (considered as P(detected class) vs.
            # P(not the detected class)) after smoothing with a temperature
            # hyperparameter.
            P = np.vstack((ws, 1.0 - ws))
            P_max = np.max(P, axis=0)
            X = np.log(P / P_max)
            X_exp = np.exp(X / beta)
            P_temp = X_exp / np.sum(X_exp, axis=0)
            P_avg = P_temp[0].mean()
            top_dets_out[k, 4] = P_avg
        elif scoring_method == 'AVG':
            # Combine new probs from overlapping boxes
            top_dets_out[k, 4] = ws.mean()
        elif scoring_method == 'IOU_AVG':
            P = ws
            ws = top_to_all_overlaps[k, inds_to_vote]
            P_avg = np.average(P, weights=ws)
            top_dets_out[k, 4] = P_avg
        elif scoring_method == 'GENERALIZED_AVG':
            P_avg = np.mean(ws**beta)**(1.0 / beta)
            top_dets_out[k, 4] = P_avg
        elif scoring_method == 'QUASI_SUM':
            top_dets_out[k, 4] = ws.sum() / float(len(ws))**beta
        else:
            raise NotImplementedError(
                'Unknown scoring method {}'.format(scoring_method)
            )

    return top_dets_out"
742,"    def _encode(self, mixed):
        """"""
        Encodes the given ``mixed`` argument with the file encoding if and
        only if it's an unicode string and returns the encoded string.
        """"""","        if type(mixed) == types.UnicodeType:
            return mixed.encode(self.encoding)
        return mixed"
743,"    def updateworkspacearchive(self, reponame, workspace, archived):
        """"""Archive or Restore the given workspace""""""","        self.ui.debug(
            ""sending 'update_workspace_archive' request\n"", component=""commitcloud""
        )
        path = ""/commit_cloud/update_workspace_archive""
        data = {""repo_name"": reponame, ""workspace"": workspace, ""archived"": archived}
        self._timedsend(path, data)"
744,"  def NoExtension(self):
    """"""File has no source file extension.""""""",    return '/'.join(self.Split()[0:2])
745,"def pad_masks(masks, padding):
    """"""
    Args:
        masks (tensor): A tensor of shape (B, M, M) representing B masks.
        padding (int): Number of cells to pad on all sides.

    Returns:
        The padded masks and the scale factor of the padding size / original size.
    """"""","    B = masks.shape[0]
    M = masks.shape[-1]
    pad2 = 2 * padding
    scale = float(M + pad2) / M
    padded_masks = masks.new_zeros((B, M + pad2, M + pad2))
    padded_masks[:, padding:-padding, padding:-padding] = masks
    return padded_masks, scale"
746,"    def write(self, buf):
        """"""write some data""""""",        raise NotImplementedError()
747,"    def untrackfile(self, f, oldstate):
        """"""
        Removes the state marking a file as tracked, but leaves it in the
        treestate for future inspection.
        """"""","        if not self._clock:
            # If watchman clock is not set, watchman is not used, drop
            # untracked files directly. This is also correct if watchman
            # clock is reset to empty, since the next query will do a full
            # crawl.
            return self._tree.remove(f)
        else:
            # If watchman is used, treestate tracks ""untracked"" files before
            # watchman clock. So only remove EXIST_* bits and copy infomation
            # from the file. fsmonitor will do a stat check and drop(real=True)
            # later.
            #
            # Typically, dropfile is used in 2 cases:
            # - ""hg forget"": mark the file as ""untracked"".
            # - ""hg update"": remove files only tracked by old commit.
            entry = self._tree.get(f, None)
            if not entry:
                return False
            else:
                state, mode, size, mtime, copied = entry
                copied = None
                state ^= state & (
                    treestate.EXIST_NEXT
                    | treestate.EXIST_P1
                    | treestate.EXIST_P2
                    | treestate.COPIED
                )
                state |= treestate.NEED_CHECK
                self._tree.insert(f, state, mode, size, mtime, copied)
                return True"
748,"    def commit(self):
        """"""Write current state on disk (if necessary)""""""","        if self._dirty:
            records = self._makerecords()
            self._writerecords(records)
            self._dirty = False"
749,"def get_nq_tokens(simplified_nq_example):
    """"""
    Returns list of blank separated tokens.
    """"""","
    if ""document_text"" not in simplified_nq_example:
        raise ValueError(
            ""`get_nq_tokens` should be called on a simplified NQ""
            ""example that contains the `document_text` field.""
        )

    return simplified_nq_example[""document_text""].split("" "")"
750,"    def get_additional_agent_args(cls) -> ParlaiParser:
        """"""
        Return a parser with arguments sourced from several sub models.
        """"""","        additional_agent_parser = ParlaiParser(add_parlai_args=False)
        BartAgent.add_cmdline_args(additional_agent_parser)
        setup_rag_args(additional_agent_parser)
        GoldDocRetrieverFiDAgent.add_cmdline_args(additional_agent_parser)
        SearchQuerySearchEngineFiDAgent.add_cmdline_args(additional_agent_parser)
        WizIntGoldDocRetrieverFiDAgent.add_cmdline_args(additional_agent_parser)
        ComboFidAgent.add_cmdline_args(additional_agent_parser)
        return additional_agent_parser"
751,"def remotenameforurl(ui, url):
    """"""Convert an URL to a remote name""""""","    return ui.paths.getname(url, forremotenames=True)"
752,"    def copy_codecs_file(self, target_file: str):
        """"""
        Copy the codecs file to a new location.

        Default behavior is to do nothing.

        :param target_file:
            where to copy the codecs.
        """"""",        pass
753,"    def len_episode(self, ep: int) -> int:
        """"""
        Length of an episode.

        Optionally overrideable.
        """"""",        return len(self.raw_data[ep]['dialog'])
754,"    def title_summary(self):
        """"""
        :return: XCUIElement title summary
        :rtype: str | None
        """"""","        if len(self.title_value) == 0:
            return None
        return ""title: '{}'"".format(self.title_value)"
755,"    def _reason_to_disqualify(self, agent: Agent):
        """"""
        Determining if agents had low quality work or had unsafe behaviour.
        """"""","        # Disconncet or timeout
        mephisto_agent = agent.mephisto_agent
        if mephisto_agent.get_status() in (
            AgentState.STATUS_EXPIRED,
            AgentState.STATUS_TIMEOUT,
        ):
            return 'agent was disconnected.'

        # Wizard not using search enough
        if agent.agent_id == 'Wizard' and (
            (self.num_search_queries < self.search_warning_threshold)
            or (self.num_times_search_resutls_selected < self.select_warning_threshold)
        ):
            return (
                'blocked for not enough search activity '
                f'({self.num_search_queries} searches; '
                f'{self.num_times_search_resutls_selected} selected sentecnes).'
            )

        acceptability_checker_results = self.acceptability_checker.check_messages(
            agent.agent_id,
            self.selected_persona,
            messages=self.messages,
            is_worker_0=False,
            violation_types=constants.ACCEPTABILITY_VIOLATIONS,
        )
        if acceptability_checker_results:
            return f'ParlAI acceptability checker found violations: ""{acceptability_checker_results}""'"
756,"    def canannotatedirectly(self, rev):
        """"""(str) -> bool, fctx or node.
        return (True, f) if we can annotate without updating the linelog, pass
        f to annotatedirectly.
        return (False, f) if we need extra calculation. f is the fctx resolved
        from rev.
        """"""","        result = True
        f = None
        if not isinstance(rev, int) and rev is not None:
            hsh = {20: bytes, 40: node.bin}.get(len(rev), lambda x: None)(rev)
            if hsh is not None and (hsh, self.path) in self.revmap:
                f = hsh
        if f is None:
            adjustctx = ""linkrev"" if self._perfhack else True
            f = self._resolvefctx(rev, adjustctx=adjustctx, resolverev=True)
            result = f in self.revmap
            if not result and self._perfhack:
                # redo the resolution without perfhack - as we are going to
                # do write operations, we need a correct fctx.
                f = self._resolvefctx(rev, adjustctx=True, resolverev=True)
        return result, f"
757,"    def __call__(self, val):
        """"""Lookup the pretty-printer for the provided value.""""""","
        # Get the type name.
        typename = gdb.types.get_basic_type(val.type).tag
        if not typename:
            typename = val.type.name
        if not typename:
            return None

        # Iterate over table of type regexps to determine
        # if a printer is registered for that type.
        # Return an instantiation of the printer if found.
        for printer in self.subprinters:
            if printer.enabled and printer.compiled_re.search(typename):
                return printer.gen_printer(val)

        # Cannot find a pretty printer.  Return None.
        return None"
758,"    def task_classes(self):
        """"""Accessor for accessing a copy of registered task classes""""""",        return self._registered[:]
759,"  def CheckEnd(self, filename, clean_lines, linenum, error):
    """"""Run checks that applies to text after the closing brace.

    This is mostly used for checking end of namespace comments.

    Args:
      filename: The name of the current file.
      clean_lines: A CleansedLines instance containing the file.
      linenum: The number of the line to check.
      error: The function to call with any errors found.
    """"""",    pass
760,"    def step(self, closure=None):
        """"""
        Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """"""","        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()  # NOTE: cast to FP32
                if grad.is_sparse:
                    raise RuntimeError('Adafactor does not support sparse gradients.')

                state = self.state[p]
                grad_shape = grad.shape

                factored, use_first_moment = self._get_options(group, grad_shape)
                # State Initialization
                if len(state) == 0:
                    state['step'] = 0

                    if use_first_moment:
                        # Exponential moving average of gradient values
                        state['exp_avg'] = torch.zeros_like(grad)
                    if factored:
                        state['exp_avg_sq_row'] = torch.zeros(grad_shape[:-1]).type_as(
                            grad
                        )
                        state['exp_avg_sq_col'] = torch.zeros(
                            grad_shape[:-2] + grad_shape[-1:]
                        ).type_as(grad)
                    else:
                        state['exp_avg_sq'] = torch.zeros_like(grad)

                    state['RMS'] = 0
                else:
                    if use_first_moment:
                        state['exp_avg'] = state['exp_avg'].type_as(grad)
                    if factored:
                        state['exp_avg_sq_row'] = state['exp_avg_sq_row'].type_as(grad)
                        state['exp_avg_sq_col'] = state['exp_avg_sq_col'].type_as(grad)
                    else:
                        state['exp_avg_sq'] = state['exp_avg_sq'].type_as(grad)

                p_data_fp32 = p.data.float()  # NOTE: cast to FP32

                state['step'] += 1
                state['RMS'] = self._rms(p_data_fp32)
                group['lr'] = self._get_lr(group, state)

                beta2t = 1.0 - math.pow(state['step'], group['decay_rate'])
                update = (grad**2) + group['eps'][0]
                if factored:
                    exp_avg_sq_row = state['exp_avg_sq_row']
                    exp_avg_sq_col = state['exp_avg_sq_col']

                    exp_avg_sq_row.mul_(beta2t).add_(
                        update.mean(dim=-1), alpha=(1.0 - beta2t)
                    )
                    exp_avg_sq_col.mul_(beta2t).add_(
                        update.mean(dim=-2), alpha=(1.0 - beta2t)
                    )

                    # Approximation of exponential moving average of square of gradient
                    self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col, update)
                    update.mul_(grad)
                else:
                    exp_avg_sq = state['exp_avg_sq']

                    exp_avg_sq.mul_(beta2t).add_(update, alpha=(1.0 - beta2t))
                    torch.rsqrt(exp_avg_sq, out=update).mul_(grad)

                update.div_(max(1.0, self._rms(update) / group['clip_threshold']))
                update.mul_(group['lr'])

                if use_first_moment:
                    exp_avg = state['exp_avg']
                    exp_avg.mul_(group['beta1']).add_(
                        update, alpha=(1 - group['beta1'])
                    )
                    update = exp_avg

                if group['weight_decay'] != 0:
                    p_data_fp32.add_(
                        p_data_fp32, alpha=(-group['weight_decay'] * group['lr'])
                    )

                p_data_fp32.add_(-update)

                p.data.copy_(p_data_fp32)

        return loss"
761,"def convert_boxes_to_pooler_format(box_lists: List[Boxes]):
    """"""
    Convert all boxes in `box_lists` to the low-level format used by ROI pooling ops
    (see description under Returns).

    Args:
        box_lists (list[Boxes] | list[RotatedBoxes]):
            A list of N Boxes or N RotatedBoxes, where N is the number of images in the batch.

    Returns:
        When input is list[Boxes]:
            A tensor of shape (M, 5), where M is the total number of boxes aggregated over all
            N batch images.
            The 5 columns are (batch index, x0, y0, x1, y1), where batch index
            is the index in [0, N) identifying which batch image the box with corners at
            (x0, y0, x1, y1) comes from.
        When input is list[RotatedBoxes]:
            A tensor of shape (M, 6), where M is the total number of boxes aggregated over all
            N batch images.
            The 6 columns are (batch index, x_ctr, y_ctr, width, height, angle_degrees),
            where batch index is the index in [0, N) identifying which batch image the
            rotated box (x_ctr, y_ctr, width, height, angle_degrees) comes from.
    """"""","    boxes = torch.cat([x.tensor for x in box_lists], dim=0)
    # __len__ returns Tensor in tracing.
    sizes = shapes_to_tensor([x.__len__() for x in box_lists])
    return _convert_boxes_to_pooler_format(boxes, sizes)"
762,"def embedding_alignment(ref_emb: np.ndarray, hypo_emb: np.ndarray) -> List[float]:
    """"""
    Return embedding matching alignment for each item in hypo_emb
    ref_emb: list of reference embeddings
    hypo_emb: list oh hypothesises embeddings
    """"""","    scores = []
    for he in hypo_emb:
        # some embeddings can be empty. For example, for latex-style equations, or empty string
        if len(he) > 0:
            out = [cosine_similarity_scaled(he, re) for re in ref_emb if len(re) > 0]
            if len(out) > 0:
                scores.append(max(out))
    return scores"
763,"def _avoidambig(path, oldstat):
    """"""Avoid file stat ambiguity forcibly

    This function causes copying ``path`` file, if it is owned by
    another (see issue5418 and issue5584 for detail).
    """"""","
    def checkandavoid():
        newstat = util.filestat.frompath(path)
        # return whether file stat ambiguity is (already) avoided
        return not newstat.isambig(oldstat) or newstat.avoidambig(path, oldstat)

    if not checkandavoid():
        # simply copy to change owner of path to get privilege to
        # advance mtime (see issue5418)
        util.rename(util.mktempcopy(path), path)
        checkandavoid()"
764,"def hashook(ui, htype) -> bool:
    """"""return True if a hook is configured for 'htype'""""""","    if not ui.callhooks:
        return False
    for hname, cmd in _allhooks(ui):
        if hname.split(""."")[0] == htype and cmd:
            return True
    return False"
765,"def CheckAccess(filename, clean_lines, linenum, nesting_state, error):
  """"""Checks for improper use of DISALLOW* macros.

  Args:
    filename: The name of the current file.
    clean_lines: A CleansedLines instance containing the file.
    linenum: The number of the line to check.
    nesting_state: A _NestingState instance which maintains information about
                   the current stack of nested blocks being parsed.
    error: The function to call with any errors found.
  """"""","  line = clean_lines.elided[linenum]  # get rid of comments and strings

  matched = Match((r'\s*(DISALLOW_COPY_AND_ASSIGN|'
                   r'DISALLOW_EVIL_CONSTRUCTORS|'
                   r'DISALLOW_IMPLICIT_CONSTRUCTORS)'), line)
  if not matched:
    return
  if nesting_state.stack and isinstance(nesting_state.stack[-1], _ClassInfo):
    if nesting_state.stack[-1].access != 'private':
      error(filename, linenum, 'readability/constructors', 3,
            '%s must be in the private: section' % matched.group(1))

  else:
    # Found DISALLOW* macro outside a class declaration, or perhaps it
    # was used inside a function when it should have been part of the
    # class declaration.  We could issue a warning here, but it
    # probably resulted in a compiler error already.
    pass"
766,"def buildobsmarkerspart(bundler, markers):
    """"""add an obsmarker part to the bundler with <markers>

    No part is created if markers is empty.
    Raises ValueError if the bundler doesn't support any known obsmarker format.
    """"""","    if not markers:
        return None

    remoteversions = obsmarkersversion(bundler.capabilities)
    version = obsolete.commonversion(remoteversions)
    if version is None:
        raise ValueError(""bundler does not support common obsmarker format"")
    stream = obsolete.encodemarkers(markers, True, version=version)
    return bundler.newpart(""obsmarkers"", data=stream)"
767,"def on_error(ws, error):
    """"""
    Prints an error, if occurs.

    :param ws: WebSocketApp
    :param error: An error
    """"""",    print(error)
768,"def FindPreviousMatchingAngleBracket(clean_lines, linenum, init_prefix):
  """"""Find the corresponding < that started a template.

  Args:
    clean_lines: A CleansedLines instance containing the file.
    linenum: Current line number.
    init_prefix: Part of the current line before the initial >.

  Returns:
    True if a matching bracket exists.
  """"""","  line = init_prefix
  nesting_stack = ['>']
  while True:
    # Find the previous operator
    match = Search(r'^(.*)([<>(),;\[\]])[^<>(),;\[\]]*$', line)
    if match:
      # Found an operator, update nesting stack
      operator = match.group(2)
      line = match.group(1)

      if nesting_stack[-1] == '>':
        # Expecting opening angle bracket
        if operator in ('>', ')', ']'):
          nesting_stack.append(operator)
        elif operator == '<':
          nesting_stack.pop()
          if not nesting_stack:
            # Found matching angle bracket
            return True
        elif operator == ',':
          # Got a comma before a bracket, this is most likely a
          # template argument.  The opening angle bracket is probably
          # there if we look for it, so just return early here.
          return True
        else:
          # Got some other operator.
          return False

      else:
        # Expecting opening parenthesis or opening bracket
        if operator in ('>', ')', ']'):
          nesting_stack.append(operator)
        elif operator in ('(', '['):
          nesting_stack.pop()

    else:
      # Scan the previous line
      linenum -= 1
      if linenum < 0:
        break
      line = clean_lines.elided[linenum]

  # Exhausted all earlier lines and still no matching angle bracket.
  return False"
769,"    def max_depth(self, tree):
        """"""Find the maximum depth of a ttypes.BTree struct""""""","        if tree is None:
            return 0

        if isinstance(tree, ttypes.BTreeBranch):
            tree = tree.child

        child_depths = [self.max_depth(child) for child in tree.children]
        if child_depths:
            max_child_depth = max(child_depths)
        else:
            max_child_depth = 0

        return 1 + max_child_depth"
770,"def make_holidays_df(year_list, country, province=None, state=None):
    """"""Make dataframe of holidays for given years and countries

    Parameters
    ----------
    year_list: a list of years
    country: country name

    Returns
    -------
    Dataframe with 'ds' and 'holiday', which can directly feed
    to 'holidays' params in Prophet
    """"""","    try:
        holidays = getattr(hdays_part2, country)(years=year_list, expand=False)
    except AttributeError:
        try:
            holidays = getattr(hdays_part1, country)(prov=province, state=state, years=year_list, expand=False)
        except AttributeError as e:
            raise AttributeError(f""Holidays in {country} are not currently supported!"") from e

    holidays_df = pd.DataFrame([(date, holidays.get_list(date)) for date in holidays], columns=['ds', 'holiday'])
    holidays_df = holidays_df.explode('holiday')
    holidays_df.reset_index(inplace=True, drop=True)
    holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])
    return (holidays_df)"
771,"def _Filters():
  """"""Returns the module's list of output filters, as a list.""""""",  return _cpplint_state.filters
772,"def _adhocstores(ui, url):
    """"""return local and remote stores for ad-hoc (outside repo) uses""""""","    if url is not None:
        ui.setconfig(""lfs"", ""url"", url)
    return blobstoremod.memlocal(), blobstore.remote(ui)"
773,"    def _pad_tensor(
        self, items: List[Union[List[int], torch.LongTensor]], is_label: bool = False
    ) -> Tuple[torch.LongTensor, List[int]]:
        """"""
        Override to always set fp16friendly to False and left_pad to True.
        """"""","        return padded_tensor(
            items, pad_idx=self.NULL_IDX, left_padded=True, fp16friendly=False
        )"
774,"    def forward(
        self,
        proposals_with_gt: List[Instances],
        densepose_predictor_outputs: Any,
        packed_annotations: PackedCseAnnotations,
        embedder: nn.Module,
    ):
        """"""
        Args:
            proposals_with_gt (list of Instances): detections with associated
                ground truth data; each item corresponds to instances detected
                on 1 image; the number of items corresponds to the number of
                images in a batch
            densepose_predictor_outputs: an object of a dataclass that contains predictor
                outputs with estimated values; assumed to have the following attributes:
                * embedding - embedding estimates, tensor of shape [N, D, S, S], where
                  N = number of instances (= sum N_i, where N_i is the number of
                      instances on image i)
                  D = embedding space dimensionality (MODEL.ROI_DENSEPOSE_HEAD.CSE.EMBED_SIZE)
                  S = output size (width and height)
            packed_annotations (PackedCseAnnotations): contains various data useful
                for loss computation, each data is packed into a single tensor
            embedder (nn.Module): module that computes vertex embeddings for different meshes
        """"""","        pix_embeds = densepose_predictor_outputs.embedding
        if self.pixel_dists.device != pix_embeds.device:
            # should normally be done only once
            self.pixel_dists = self.pixel_dists.to(device=pix_embeds.device)
        with torch.no_grad():
            mask_loss_data = extract_data_for_mask_loss_from_matches(
                proposals_with_gt, densepose_predictor_outputs.coarse_segm
            )
        # GT masks - tensor of shape [N, S, S] of int64
        masks_gt = mask_loss_data.masks_gt.long()  # pyre-ignore[16]
        assert len(pix_embeds) == len(masks_gt), (
            f""Number of instances with embeddings {len(pix_embeds)} != ""
            f""number of instances with GT masks {len(masks_gt)}""
        )
        losses = []
        mesh_names = (
            self.shape_names
            if self.use_all_meshes_not_gt_only
            else [
                MeshCatalog.get_mesh_name(mesh_id.item())
                for mesh_id in packed_annotations.vertex_mesh_ids_gt.unique()
            ]
        )
        for pixel_embeddings, mask_gt in zip(pix_embeds, masks_gt):
            # pixel_embeddings [D, S, S]
            # mask_gt [S, S]
            for mesh_name in mesh_names:
                mesh_vertex_embeddings = embedder(mesh_name)
                # pixel indices [M]
                pixel_indices_flattened = _sample_fg_pixels_randperm(
                    mask_gt, self.num_pixels_to_sample
                )
                # pixel distances [M, M]
                pixel_dists = self.pixel_dists.to(pixel_embeddings.device)[
                    torch.meshgrid(pixel_indices_flattened, pixel_indices_flattened)
                ]
                # pixel embeddings [M, D]
                pixel_embeddings_sampled = normalize_embeddings(
                    pixel_embeddings.reshape((self.embed_size, -1))[:, pixel_indices_flattened].T
                )
                # pixel-vertex similarity [M, K]
                sim_matrix = pixel_embeddings_sampled.mm(mesh_vertex_embeddings.T)
                c_pix_vertex = F.softmax(sim_matrix / self.temperature_pix_to_vertex, dim=1)
                c_vertex_pix = F.softmax(sim_matrix.T / self.temperature_vertex_to_pix, dim=1)
                c_cycle = c_pix_vertex.mm(c_vertex_pix)
                loss_cycle = torch.norm(pixel_dists * c_cycle, p=self.norm_p)
                losses.append(loss_cycle)

        if len(losses) == 0:
            return pix_embeds.sum() * 0
        return torch.stack(losses, dim=0).mean()"
775,"    def test_order(self):
        """"""make sure that output results are sorted""""""","        d = self.xq.shape[1]
        index = faiss.IndexNSGFlat(d, 32)

        index.train(self.xb)
        index.add(self.xb)

        k = 10
        nq = self.xq.shape[0]
        D, _ = index.search(self.xq, k)

        indices = np.argsort(D, axis=1)
        gt = np.arange(0, k)[np.newaxis, :]  # [1, k]
        gt = np.repeat(gt, nq, axis=0)  # [nq, k]
        np.testing.assert_array_equal(indices, gt)"
776,"    def predict_trend(self, df):
        """"""Predict trend using the prophet model.

        Parameters
        ----------
        df: Prediction dataframe.

        Returns
        -------
        Vector with trend on prediction dates.
        """"""","        k = np.nanmean(self.params['k'])
        m = np.nanmean(self.params['m'])
        deltas = np.nanmean(self.params['delta'], axis=0)

        t = np.array(df['t'])
        if self.growth == 'linear':
            trend = self.piecewise_linear(t, deltas, k, m, self.changepoints_t)
        elif self.growth == 'logistic':
            cap = df['cap_scaled']
            trend = self.piecewise_logistic(
                t, cap, deltas, k, m, self.changepoints_t)
        elif self.growth == 'flat':
            # constant trend
            trend = self.flat_trend(t, m)

        return trend * self.y_scale + df['floor']"
777,"def add_decomposed_rel_pos(attn, q, rel_pos_h, rel_pos_w, q_size, k_size):
    """"""
    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.
    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950
    Args:
        attn (Tensor): attention map.
        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).
        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.
        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.
        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).
        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).

    Returns:
        attn (Tensor): attention map with added relative positional embeddings.
    """"""","    q_h, q_w = q_size
    k_h, k_w = k_size
    Rh = get_rel_pos(q_h, k_h, rel_pos_h)
    Rw = get_rel_pos(q_w, k_w, rel_pos_w)

    B, _, dim = q.shape
    r_q = q.reshape(B, q_h, q_w, dim)
    rel_h = torch.einsum(""bhwc,hkc->bhwk"", r_q, Rh)
    rel_w = torch.einsum(""bhwc,wkc->bhwk"", r_q, Rw)

    attn = (
        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]
    ).view(B, q_h * q_w, k_h * k_w)

    return attn"
778,"    def __str__(self):
        """"""
        Returns the string representation of the entry.
        """"""",        return unicode(self).encode(self.encoding)  # noqa
779,"    def test_init_opt(self):
        """"""
        Test --init-opt.
        """"""","
        with testing_utils.tempdir() as temp_dir:

            init_opt_path = os.path.join(temp_dir, 'init_opt.opt')
            test_model_file = '/test_model_path/model'

            # Save a test opt file
            init_opt = Opt({'model_file': test_model_file})
            init_opt.save(init_opt_path)

            # Load the opt back in with --init-opt and make sure it's been set
            # correctly
            opt = ParlaiParser(True, True).parse_kwargs(init_opt=init_opt_path)
            self.assertEqual(opt['model_file'], test_model_file)"
780,"def lazyparents(rev, public, parentfunc):
    """"""lazyparents(rev, public)
    Lazily yield parents of rev in reverse order until all nodes
    in public have been reached or all revs have been exhausted

    10
     | \
     9  8
     |  | \
     7  6  5
     |  | /
     4 *3   First move, 4 -3
     | /
     2 *2   Second move, 4 -1
     | *
     1

    For example:
    >>> parents = { 10:[9, 8], 9:[7], 8:[6,5], 7:[4], 6:[3], 5:[3], 4:[2] }
    >>> parents.update({ 3:[2], 2:[1], 1:[] })
    >>> parentfunc = lambda k: parents[k]
    >>> public = set([1])
    >>> for p in lazyparents(10, public, parentfunc): print p,
    10 9 8 7 6 5 4 3 2 1
    >>> public = set([2,3])
    >>> for p in lazyparents(10, public, parentfunc): print p,
    10 9 8 7 6 5 4 3 2
    >>> parents[4] = [3]
    >>> public = set([3,4,5])
    >>> for p in lazyparents(10, public, parentfunc): print p,
    10 9 8 7 6 5 4 3
    >>> parents[4] = [1]
    >>> public = set([3,5,7])
    >>> for p in lazyparents(10, public, parentfunc): print p,
    10 9 8 7 6 5 4 3 2 1
    """"""","    seen = set()
    heap = [-rev]

    while heap:
        cur = -(heapq.heappop(heap))
        if cur not in seen:
            seen.add(cur)
            yield cur

            published = cur in public
            if published:
                # Down to one public ancestor; end generation
                if len(public) == 1:
                    return
                public.remove(cur)

            for p in parentfunc(cur):
                if p != nullrev:
                    heapq.heappush(heap, -p)
                    if published:
                        public.add(p)"
781,"    def area(self) -> torch.Tensor:
        """"""
        Computes the area of all the boxes.

        Returns:
            torch.Tensor: a vector with areas of each box.
        """"""","        box = self.tensor
        area = (box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1])
        return area"
782,"    def episode_done(self):
        """"""
        Return whether or not this agent believes the conversation to be done.
        """"""",        return self.manager.shutting_down
783,"    def test_train_stream_ordered(self, data_regression):
        """"""
        Test --datatype train:stream:ordered.
        """"""","        return self._regression(data_regression, 'train')"
784,"    def tell(self):
        """"""Return the position within the range.
        This is different from fo.seek in that position 0 is the
        first byte position of the range tuple. For example, if
        this object was created with a range tuple of (500,899),
        tell() will return 0 when at byte position 500 of the file.
        """"""",        return self.realpos - self.firstbyte
785,"def get_item_url(item):
    """"""Take an item dict, return the URL it can be downloaded from.""""""","    return PKGS_URL + '/' + urllib2.quote(item[""installer_item_location""])"
786,"    def forward(self, x):
        """"""
        :param x: (b, c, t, h, w)
        :return:
        """"""","
        batch_size = x.size(0)

        g_x = self.g(x).view(batch_size, self.inter_channels, -1)
        g_x = g_x.permute(0, 2, 1)

        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)
        theta_x = theta_x.permute(0, 2, 1)
        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)
        f = torch.matmul(theta_x, phi_x)
        f_div_C = F.softmax(f, dim=-1)

        y = torch.matmul(f_div_C, g_x)
        y = y.permute(0, 2, 1).contiguous()
        y = y.view(batch_size, self.inter_channels, *x.size()[2:])
        W_y = self.W(y)
        z = W_y + x

        return z"
787,"    def _got_user_exception(self, exc_info, tb_label='traceback'):
        """"""Called when user code raises an exception.

        If 'exc_info' is a `MultipleExceptions`, then we recurse into it
        unpacking the errors that it's made up from.

        :param exc_info: A sys.exc_info() tuple for the user error.
        :param tb_label: An optional string label for the error.  If
            not specified, will default to 'traceback'.
        :return: 'exception_caught' if we catch one of the exceptions that
            have handlers in 'handlers', otherwise raise the error.
        """"""","        if exc_info[0] is MultipleExceptions:
            for sub_exc_info in exc_info[1].args:
                self._got_user_exception(sub_exc_info, tb_label)
            return self.exception_caught
        try:
            e = exc_info[1]
            self.case.onException(exc_info, tb_label=tb_label)
        finally:
            del exc_info
        for exc_class, handler in self.handlers:
            if isinstance(e, exc_class):
                self._exceptions.append(e)
                return self.exception_caught
        raise e"
788,"    def run_script(script_name, namespace):
        """"""Execute the named script in the supplied namespace dictionary""""""",
789,"    def check_dbqp_ports(self):
        """""" Scan the files in /tmp for those files named
            dbqp_port_NNNN.  Existence indicates said port is 'locked'

        """"""","        used_ports = []
        tmp_files = os.listdir('/tmp')
        for tmp_file in tmp_files:
            if tmp_file.startswith('dbqp_port'):
                used_ports.append(int(tmp_file.split('_')[-1]))
        return used_ports"
790,"def build_deduped_split(dpath: str):
    """"""
    Original CMU-DoG has 110 ids that are used in multiple of train/valid/test.

    Get rid of the duplication.
    """"""","    cdir = os.path.join(dpath, ""conversations"")
    data = {}
    for fold in [""test"", ""valid"", ""train""]:
        fpath = os.path.join(cdir, f""{fold}.json"")
        with PathManager.open(fpath) as f:
            data[fold] = json.load(f)

    train_len = len(data[""train""])
    valid_len = len(data[""valid""])
    test_len = len(data[""test""])
    logger.info(
        f""Converation count with duplicates: train-{train_len}, valid-{valid_len}, test-{test_len}""
    )

    train_valid = set(data[""train""].keys()) & set(data[""valid""].keys())
    train_test = set(data[""train""].keys()) & set(data[""test""].keys())
    valid_test = set(data[""valid""].keys()) & set(data[""test""].keys())

    for key in train_valid:
        data[""train""].pop(key)
    for key in train_test:
        data[""train""].pop(key)
    for key in valid_test:
        data[""test""].pop(key)

    train_len = len(data[""train""])
    valid_len = len(data[""valid""])
    test_len = len(data[""test""])
    logger.info(
        f""Converation count without duplicates: train-{train_len}, valid-{valid_len}, test-{test_len}""
    )

    for fold in [""test"", ""valid"", ""train""]:
        fpath = os.path.join(cdir, f""{fold}_deduped.json"")
        with PathManager.open(fpath, ""w+"") as f:
            json.dump(data[fold], f, indent=2)"
791,"def optimize(tree):
    """"""Optimize evaluatable tree

    All pseudo operations should be transformed beforehand.
    """"""","    _weight, newtree = _optimize(tree)
    return newtree"
792,"    def _pad_tensor(self, items, is_label=False):
        """"""
        Override to always set fp16friendly to False and left_pad to True.
        """"""","        return padded_tensor(
            items, pad_idx=self.NULL_IDX, left_padded=True, fp16friendly=False
        )"
793,"def _center_strip_right(text: str, width: int) -> str:
    """"""Returns a string with sufficient leading whitespace such that `text`
    would be centered within the specified `width` plus a trailing newline.""""""","    space = (width - len(text)) // 2
    return space * "" "" + text + ""\n"""
794,"    def delete(self, f: str) -> None:
        """"""Removes a file from the dirstate entirely. This is useful during
        operations like update, to remove files from the dirstate that are known
        to be deleted.""""""","        oldstate = self[f]
        if self._map.deletefile(f, oldstate):
            self._dirty = True
            if not self._istreestate:
                self._updatedfiles.add(f)
                self._map.copymap.pop(f, None)"
795,"    def getbundle(self, node):
        """"""Get the bundleid for a bundle that contains the given node.""""""","        nodepath = os.path.join(self._nodemap, node)
        bundleid = self._read(nodepath)
        return pycompat.decodeutf8(bundleid) if bundleid is not None else None"
796,"    def __init__(self, opt, shared=None):
        """"""
        Initialize this agent.
        """"""","        if not IMPORT_OKAY:
            raise ImportError(
                ""ALICE agent needs python-aiml installed. Please run:\n ""
                ""`pip install git+https://github.com/paulovn/python-aiml.git`.""
            )

        super().__init__(opt)
        self.id = 'Alice'
        self.kern = None
        if shared is None:
            self._load_alice()
        else:
            self.kern = shared['kern']"
797,"    def epoch_done(self):
        """"""
        Return whether all subtasks are completed.
        """"""","        for t in self.tasks:
            if not t.epoch_done():
                return False
        return True"
798,"    def reset_metrics(self):
        """"""
        Reset per-dialogue round metrics.
        """"""","        for v in self.metrics.values():
            v[""hits@1/100""] = 0.0
            v[""loss""] = 0.0
            v[""num_samples""] = 0.0
            if ""med_rank"" in v:
                v[""med_rank""] = []"
799,"    def _get_worker_data(self, worker_id: str) -> Dict[str, List]:
        """"""
        Return worker data if present, else a default dict.
        """"""","        onboarding_todo = list(range(len(self.onboarding_tasks)))
        random.shuffle(onboarding_todo)
        self.worker_data[worker_id] = self.worker_data.get(
            worker_id,
            {
                ""tasks_completed"": [],
                ""conversations_seen"": [],
                ""onboarding_todo"": onboarding_todo,
            },
        )
        return self.worker_data[worker_id]"
800,"  def rel_links(cls, page):
    """"""return rel= links that should be scraped, skipping obviously data links.""""""","    for match in cls.REL_RE.finditer(page):
      href, rel = match.group(0), match.group(1)
      if rel not in cls.REL_TYPES:
        continue
      href_match = cls.HREF_RE.search(href)
      if href_match:
        href = cls.href_match_to_url(href_match)
        parsed_href = urlparse(href)
        if any(parsed_href.path.endswith(ext) for ext in cls.REL_SKIP_EXTENSIONS):
          continue
        yield href"
801,"def wireprotoknownnodes(repo, proto, nodes, others):
    """"""similar to 'known' but also check in infinitepush storage""""""","    nodes = wireproto.decodelist(nodes)
    knownlocally = repo.known(nodes)
    for index, known in enumerate(knownlocally):
        # TODO: make a single query to the bundlestore.index
        if not known and repo.bundlestore.index.getnodebyprefix(
            nodemod.hex(nodes[index])
        ):
            knownlocally[index] = True
    return """".join(b and ""1"" or ""0"" for b in knownlocally)"
802,"def onboarding_mode_toggle_message(onboarding_step):
    """"""
    Formats a message to be sent to front-end to detemine the state of onboarding.
    """"""","    return {
        'id': constants.ONBOARDING_AGENT,
        'text': '',
        'episode_done': False,
        'task_data': {'on_boarding_step': onboarding_step},
    }"
803,"    def shutdown(self):
        """"""
        Defined to shutown the tornado application.
        """"""","        try:
            self.world_runner.shutdown()
            self._expire_all_conversations()
        finally:
            pass
        tornado.ioloop.IOLoop.current().stop()"
804,"def read_rulekeys(keys_file, prefix, delimiter):
    """"""
    Parses lines of the form: '<anything> <prefix> <key> <delimiter> <tokens>',
    where <tokens> structure is described in `tokenize_rulekey_line`.
    """"""","    t0 = time.clock()
    keys = {}
    line_cnt = 0
    for line in keys_file:
        line = line.strip()
        line_cnt += 1
        pos_p = line.find(prefix)
        if pos_p < 0:
            continue
        pos_p += len(prefix)
        pos_t = line.find(delimiter, pos_p)
        if pos_t < 0:
            continue
        key = line[pos_p:pos_t]
        pos_t += len(delimiter)
        keys[key] = tokenize_rulekey_line(line[pos_t:])
    dt = time.clock() - t0
    eprint(""parsed lines: "", line_cnt, "", keys: "", len(keys), "", time: "", dt, sep="""")
    return keys"
805,"def get_assign_roles_fn(world_module, world_name):
    """"""
    Get assign roles function for a world.

    :param world_module:
        module. a python module encompassing the worlds
    :param world_name:
        string. the name of the world in the module

    :return:
        the assign roles function if available, else None
    """"""","    return get_world_fn_attr(
        world_module, world_name, 'assign_roles', raise_if_missing=False
    )"
806,"def get_logger():
    """"""
  get_logger():

  Creates logger object to output messages to stdout
  """"""","
    log_dir = os.path.join(utils.get_paths()[""logs""])
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    log_path = os.path.join(log_dir, ""pantri.log"")
    logger = logging.getLogger(__name__)
    if not len(logger.handlers):
        format_str = ""[%(asctime)s] %(levelname)s: %(message)s""

        # Define stdout handler
        console = logging.StreamHandler()
        console.setFormatter(logging.Formatter(format_str))

        # Define log handler
        log_file = logging.handlers.TimedRotatingFileHandler(
            log_path, when=""midnight"", interval=1, backupCount=14
        )
        # log_file = logging.FileHandler(log_path)
        log_file.setFormatter(logging.Formatter(format_str))

        # Add handlers
        logger.addHandler(console)
        logger.addHandler(log_file)
        logger.setLevel(logging.DEBUG)

    return logger"
807,"    def get_ctxt_rep(self, batch: Batch) -> Tuple[torch.Tensor, torch.BoolTensor]:
        """"""
        Encode context representation.
        """"""","        ctxt_rep, ctxt_rep_mask, _ = self.model(**self._model_context_input(batch))
        return ctxt_rep, ctxt_rep_mask"
808,"def Error(filename, linenum, category, confidence, message):
  """"""Logs the fact we've found a lint error.

  We log where the error was found, and also our confidence in the error,
  that is, how certain we are this is a legitimate style regression, and
  not a misidentification or a use that's sometimes justified.

  False positives can be suppressed by the use of
  ""cpplint(category)""  comments on the offending line.  These are
  parsed into _error_suppressions.

  Args:
    filename: The name of the file containing the error.
    linenum: The number of the line containing the error.
    category: A string used to describe the ""category"" this bug
      falls under: ""whitespace"", say, or ""runtime"".  Categories
      may have a hierarchy separated by slashes: ""whitespace/indent"".
    confidence: A number from 1-5 representing a confidence score for
      the error, with 5 meaning that we are certain of the problem,
      and 1 meaning that it could be a legitimate construct.
    message: The error message.
  """"""","  if _ShouldPrintError(category, confidence, linenum):
    _cpplint_state.IncrementErrorCount(category)
    if _cpplint_state.output_format == 'vs7':
      sys.stderr.write('%s(%s):  %s  [%s] [%d]\n' % (
          filename, linenum, message, category, confidence))
    elif _cpplint_state.output_format == 'eclipse':
      sys.stderr.write('%s:%s: warning: %s  [%s] [%d]\n' % (
          filename, linenum, message, category, confidence))
    else:
      sys.stderr.write('%s:%s:  %s  [%s] [%d]\n' % (
          filename, linenum, message, category, confidence))"
809,"def recover(ui, repo):
    """"""roll back an interrupted transaction

    Recover from an interrupted commit or pull.

    This command tries to fix the repository status after an
    interrupted operation. It should only be necessary when @Product@
    suggests it.

    Returns 0 if successful, 1 if nothing to recover.
    """"""","    if repo.recover():
        return 0
    return 1"
810,"def _makefpartparamsizes(nbparams):
    """"""return a struct format to read part parameter sizes

    The number parameters is variable so we need to build that format
    dynamically.
    """"""","    return "">"" + (""BB"" * nbparams)"
811,"    def finalize_message(self, msg: Dict[str, Any]) -> Message:
        """"""
        Return message as is.
        """"""",        return Message(msg)
812,"    def load_state_dict(self, state_dict):
        """"""
        Load the state dict into model.

        This is easily overridable to facilitate transfer of state dicts.
        """"""","        if self.is_finetune and self.opt['load_from_pretrained_ranker']:
            self.base_model.load_state_dict(state_dict, strict=False)
        else:
            self.model.load_state_dict(state_dict)"
813,"    def add_cmdline_args(
        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None
    ) -> ParlaiParser:
        """"""
        Add cmd line args.
        """"""","        AddLabelFixedCandsTRA.add_cmdline_args(parser, partial_opt=partial_opt)
        PolyencoderAgent.add_cmdline_args(parser, partial_opt=partial_opt)
        return parser"
814,"    def writefile(self, path, contents):
        """"""Writes `contents` to the `path` relative to this directory""""""","        return writefile(self.join(path), contents)"
815,"    def _addfileindir(self, filename, status):
        """"""Add a file in this directory as a direct child.""""""","        self.files.append((filename, status))"
816,"    def read(self, oid):
        """"""Read blob from local blobstore.""""""","        if self.cachevfs and not self.vfs.exists(oid):
            self.cachevfs.linktovfs(oid, self.vfs)
        return self.vfs.read(oid)"
